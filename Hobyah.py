#! python3
#
# Copyright 2020-2024, Ewan Bennett
#
# All rights reserved.
#
# Released under the BSD 2-clause licence (SPDX identifier: BSD-2-Clause)
#
# email: ewanbennett@fastmail.com
#
# A program for:
#  * doing tunnel ventilation engineering calculations and plotting the
#    results.
#  * plotting SES output (the SES output is processed by a separate
#    program in the suite, SESconv.py).
#  * plotting data from .csv files.
#
# Hobyah.py opens one or more text input files with tunnel ventilation
# system definitions, runs each through a syntax checker to ensure
# that all begin...end blocks match.  Then it reads the following blocks:
#  * settings (mandatory)
#  * constants (optional)
#  * data <name> (optional)
#  * csv (optional)
#  * fan <name> (optional)
#  * sectypes (mandatory in files that calculate)
#  * tunnel <name> (mandatory in files that calculate)
#  * tunnelclones <name> (optional)
#  * route <name> (optional)
#  * jetfantypes (optional)
#  * traffictypes (optional)
#  * trafficsteady (optional)
#  * plotcontrol (optional)
#  * files (optional)
#  * testblock (optional, only used in test files)
#  * SESdata (optional)
#  * plots (mandatory)
#
# If a calculation is to be run it divides the tunnels into segments
# of constant area and carries out a transient compressible flow
# calculation.
#
# The calculation handles homentropic flow by the method of characteristics
# (MoC), calculating values of speed of sound and air velocity at
# gridpoints in a 1D network.  The theoretical basis of this is given
# in the file "MoC.pdf" in the documentation folder.
# The program writes the calculated values to a binary file for a set of
# plotter routines to read.
# The program's actions are recorded in a log file in a subfolder.
#
#
#
# The program can also be told to take its geometry and turn it into
# the skeleton of an SES input file.
# Hobyah's definitions of tunnels are split into segments of constant
# area bounded by its nodes and joins.  The program contains route
# definitions.  This makes it relatively easy to build the bulk of an
# SES input file (forms 1, 2, 3, 5, 6 and 8) from the geometry.
# The 'SESdata' block contains all the entries required to fill in
# some of the blanks like the definitions of thermal data, fans, jet fans,
# fires, trains etc. (temperatures in forms 3 and 5, forms 4, 7
#
# The program can be used just as a plotter, taking data from the input
# file's calculations, data in the input file, data in .csv files, data
# from other Hobyah runs' binary files and data from binary files created
# from SES output files.
#
# The plot function generates multi-page pdf files (using gnuplot).  It
# can also be told to call ImageMagick's "convert" utility to generate
# one .png file for each page in the pdf file.
#
# See "Hobyah-User-Guide.pdf" in the documentation folder for more details.
#
# Verification and validation of the programs are covered in the document
# "Verification+validation.pdf" in the documentation folder.
#
import sys
import os
import math
import re                 # regular expressions
import argparse           # processing command-line arguments
import generics as gen    # general routines
import syntax             # syntax check routines
import UScustomary as USc # imperial to metric conversion
import subprocess
import pickle
import classSES as clSES # class that holds SES binary files (.sbn files)
import classHobyah as clHobyah # class that holds Hobyah binaries (.hbn files)
import pathlib
import datetime
import copy
import multiprocessing
import operator         # sort lists according to values in a sub-list.

try:
    # Try to get numpy, pandas, scipy and some Fortran in at the top level.
    import numpy as np
    import pandas as pd
    import scipy.optimize
    import compressible as ftn # Fortran compressible flow routines inside f2py
except (ModuleNotFoundError, ImportError):
    # We let this pass.  We complain about it if we need it later, after
    # we've opened the first logfile and can write the complaint to it.
    # Also, we only need scipy and compressible if we are running a
    # method of characteristics calculation; they may not be needed.
    pass

# Create a counter for the gnuplot polygon and label numbers at the top level.
# It's incremented in different routines that use it as a global, which is
# lazy.  If you are an expert user of gnuplot and want to set numbers for
# labels, arrows and polygons in your gnuplot verbatim blocks, keep your
# numbers below 2 billion, as that will avoid clashes with the label, arrow
# and polygon numbers auto-generated by Hobyah.py.
poly_num = 2000000000


def ProcessCurves(curve_triples, settings_dict, last_doodad,
                  sources_list, files_dict, log, plt):
    '''Read all the rest of the lines in a graph definition.  They
    should all begin with the curve type and define one type of curve
    that we want plotted.  The routine returns after a line with
    "end graph" is read.

        Parameters:
            curve_triples [(int, str, str)] List of lines in the graph
                                            definition that set curves.
            settings_dict   {}              Dictionary of the run settings.
            last_doodad     bool            True if this is in the last graph
                                            or image on the page, False
                                            otherwise.  Used to add gnuplot
                                            labels for header and/or footer.
            sources_list    []              List of the names of data sources,
                                            used to create the footer.
            files_dict      {}              The dictionary of file names
                                            and their nicknames.
            log             handle          The handle of the logfile.
            plt             handle          The handle of the gnuplot file.

        Returns:
            tr_index        int             Pointer to the line containing
                                            "end graph".
            sources_list    []              An updated list of the names of
                                            data sources, with new sources
                                            added.

        Errors:
            Aborts with 6041 if a keyword (first word on the line)
            was a valid plot command in the wrong place.
            Aborts with 6042 if a keyword was not a valid entry
            in the valid_settings dictionary.
            Aborts with 6043 if the keyword was "userdata" and the
            user did not define any blocks of data or names of .csv
            files.
            Aborts with 6044 if the name of a block of "userdata" was
            not a valid name.
            Aborts with 6045 if a new type of curve was allowed to be
            used but was not handled here.
            Aborts with 6046 if a file in the "ancillaries" subfolder
            could not be written to.
    '''
    # Get the units we will be plotting these curves in.
    graph_units = settings_dict["graphunits"]

    # Break out the various settings we will need here.
    file_name = settings_dict["file_name"]
    file_stem = settings_dict["file_stem"]
    debug1 = settings_dict["debug1"]
    reserved = settings_dict["reserved"]
    QAlist1 = settings_dict["QAlist1"]
    page_num = settings_dict["page_num"]
    graph_num = settings_dict["graph_num"]
    ancill_path = settings_dict["ancill_path"]
    plotnames = settings_dict["plotnames"]

    # Now process the words on the line.  These are generally
    # one of the following forms, all of which have four
    # mandatory words (one keyword and three identifiers):
    #  "userdata"  nickname   2         5    # One line, X on column 2, Y on column 5
    #  "userdata"  nickname time      P_tot  # One line, X on a column headed by
    #                                          the word "time" and Y on a column
    #                                          headed by the word "p_tot".
    #  "transient" property nickname entity@chainage       Hobyah or SES route
    #  "transient" property nickname train@offset          Tunnel property seen by a passing train
    #  "transient" property nickname train                 Property of a train
    #  'transient" property nickname 101-106-5m            SES section-segment-subseg-locn
    #  "transient" property nickname 101-106-5             SES section-segment-subseg (defaults
    #                                                      to the midpoint)
    #  "transient" property nickname 123-5m                SES segment-subseg-locn
    #  "transient" property nickname 123-5                 SES segment-subseg (midpoint)
    #  "transient" property nickname segment@distance      SES segment (distance 0-length)
    #  "transient" property nickname zone4                 SES controlled zone HVAC heat flow
    #  "profile" property nickname entity@time             Hobyah or SES route.  Time may be
    #                                                      absent if it's a fixed property,
    #                                                      e.g. elevation.
    #  "waterfall" nickname routename "distance"        train path diagram, distance on X
    #  "waterfall" nickname routename "time"            train path diagram, time on X
    #  "property" property nickname entity
    #
    # Note that if we add more types of curve we will need to
    # add them to a list in ProcessGraph.  The list is called
    # "curve_keys"
    #
    #    elif words_low[0] in ("transient", "profile", "waterfall",
    #
    # If you add anything to this dictionary of valid settings, search
    # for the line of text above and add the key of the new curve to that
    # logic test.
    #
    valid_settings = {"transient": ("#name", "#name", "#name", "QAstr"),
                      "profile":   ("#name", "#name", "#name", "QAstr"),
                      "waterfall": ("#name", "#name", ("time", "distance"), "QAstr"),
                      "property":  ("#name", "#name", "#name", "QAstr"),
                      "userdata":  ("#name", "#name", "#name", "QAstr"),
                      "icons":     ("#name", "#name", "#name", "QAstr"),
                      "fandata":   ("#name", "#name", "#name", "QAstr"),
                     }
    # We include all the keywords we already processed, so that we can
    # give a better informative error message (6041) later.
    for word in reserved:
        valid_settings.__setitem__(word, ("QAstr",))


    # Make a list of the valid curve definition words, excluding "#skip".
    # We will need this later.
    valid_keys = list(valid_settings.keys())[-1]

    requireds = []
    # We make a dictionary of the optional keywords that accepts
    # an optional value.  There are some standard options for
    # offsetting the curve data, limiting the extent plotted and
    # setting which axes to plot on and what colour and type of
    # curve to.  Some show (lines are the default but some people
    # want points.  Curves that are distances on the X-axis can
    # be told to plot as chainage (the default) or as co-ordinates.

    # First define the optional entries that all plot types
    # have.  These relate to controlling the axis selection,
    # the curve offsets & multipliers, and clipping the X-axis
    # extents.
    # In some circumstances setting a divisor is more convenient
    # instead of a multiplier. For example, if test data in a .csv
    # file is in Pascals and you want to convert it to inches of
    # water gauge, a divisor of 249.089 is more recognisable than a
    # multiplier of 0.0041464.  But maybe that's just me.
    default_opts = {
                    "axes": ("x", "y", "x1", "y1", "xy", "xy1", "x1y", "x1y1",
                             "y2", "xy2", "x1y2",
                             "x2", "x2y", "x2y1",
                             "x2y2"),               # default is x1y1
                    "lt": "int 0+ null a line type",
                    "lw": "int 0+ null a line width",
                    "xoffset": "float any null  an offset in X", # default 0
                    "yoffset": "float any null  an offset in Y", # default 0
                    "xmult":   "float any null  a multiplier on X", # default 1
                    "ymult":   "float any null  a multiplier on Y", # default 1
                    "xdiv":    "float any null  a divisor on X", # default 1
                    "ydiv":    "float any null  a divisor on Y", # default 1
                    "xstart":  "float any null  an X start value", # default -inf
                    "xstop":   "float any null  an X stop value", # default +inf
                   }
    # Define dictionaries of optional entries appropriate to each
    # curve type, then add the default optional entries to each.
    # This approach saves us having to add each new default to all
    # the dictionaries manually.
    trans_opt = {"xaxis": ("time", "chainage", "coords"),}
    prof_opt = {"xaxis": ("time", "chainage", "coords"),
                "distance": ("chainage", "coords"),
                # "datum" is for pressures profiles along smoke ducts.
                # The pressure across this section will be added to
                # the pressures along the routes.  It is only used in
                # the 'DP' plot type in classSES.
                "datum":   "int + null  a section number"}
    wfall_opt = {"xaxis": ("time", "chainage", "coords"),
                 "distance": ("chainage", "coords"),}
    prop_opt = {}
    user_opt = {}
    icons_opt = {"profile": ("route", "stack", "flat"),
                 "float":   "float any null  a base uplift",
                 "height":  "float any null  an object height",
                 "colour":  "#name",
                 "color":  "#name",
                 # These next two are only used by jet fan icons, they
                 # set the length.
                 "aspect":   "float any null  an aspect ratio",
                 "length":   "float + null  an object length",}
    fans_opt = {}

    # Now set a new entry for the profile options (prof_opt).  It applies
    # only to one keyword (stacks).  It is a factor to multiply the tunnel
    # area by before adding it to the stack height.  This is useful because
    # it means that the profile changes slightly at each change of area.
    # This means that we can tell on a profile where the cut and cover
    # ends, where the stations are etc.  Such profiles are best plotted
    # a few metres above the true stack profile so that the two together
    # look like the tunnel floor and roof.  The default value is 0.05.
    # so that if you have a 25 m^2 tunnel it adds 1.25 m to the elevation
    # that is plotted.  It is processed in the binary file's class, not
    # in Hobyah.
    prof_opt.__setitem__("areamult", "float 0+ null  a multiplier on area")

    for dictionary in (trans_opt, prof_opt, wfall_opt,
                       prop_opt, user_opt, icons_opt,
                       fans_opt,
                      ):
        dictionary.update(default_opts)
    optionals = {"transient": trans_opt,
                 "profile":   prof_opt,
                 "waterfall": wfall_opt,
                 "property":  prop_opt,
                 "userdata":  user_opt,
                 "icons":  icons_opt,
                 "fandata": fans_opt,
                }
    # We allow any and all duplicates.
    duplicates = ("transient", "profile", "waterfall", "property",
                  "userdata", "icons", "fandata",
                 )
    settings = (valid_settings, requireds, optionals, duplicates)
    # Now call ProcessBlock.  We spoof a "begin graph" line so that
    # we don't have to use an extra "begin curves...end curves" block.
    #
    spoof_triples = [(-1, "begin graph", "begin graph # Spoofed begin")] + \
                    curve_triples
    result = ProcessBlock(spoof_triples, 0 , settings_dict, "graph",
                          {}, settings, log)
    if result is None:
        return(None)
    # If we get to here we know that the lines defining the curves
    # were correctly formed.


    if debug1:
        print("Building curves.")
    # Create an empty string to hold the lines to hold the lines of the
    # plot command.
    plot_string = ''
    tr_index = 0
    # Create something to let us track the plotting of the first curve.
    first_curve = True
    while True:
        (line_number, line_data, line_text) = curve_triples[tr_index]
        if debug1:
            print("Line2", str(line_number) + ":", line_data)
        # Get the optional entries on the line into a dictionary.
        result = gen.GetOptionals(line_number, line_data, line_text,
                 file_name, debug1, log)
        if result is None:
            return(None)
        else:
            (line_data, optionals_dict) = result
        words = line_data.split(maxsplit = 4)
        words_low = line_data.lower().split(maxsplit = 4)
        # Check for the end of the graph definition.
        if words_low[:2] == ["end", "graph"]:
            # We've finished this block of curves.  Break out.
            break
        else:
            # Get the keyword and nickname.  We know these exist because
            # the lines passed ProcessBlock's checks.
            keyword =  words_low[0]
            if keyword == "userdata":
                # In userdata curves the four entries are keyword,
                # nickname, column for X, column for Y.
                nickname = words[1]
            else:
                # In other curves the four entries are keyword,
                # property, nickname, locator.
                nickname = words[2]

        # Now process the curve definitions.
        if keyword in reserved:
            # The user has used a graph definition command after
            # starting the curves, which is not allowed.
            # Figure out what the line number of the first curve
            # is.
            first_curve = curve_triples[0][0]
            err = ('> Found a valid keyword in a graph after\n'
                   '> the start of the curve definitions in\n'
                   '> "' + file_name + '".\n'
                   '> Unfortunately this is not allowed, as\n'
                   '> it is too complex to process (the curves\n'
                   '> are all lumped into one gnuplot "plot"\n'
                   '> command).  Please edit the file to either\n'
                   '> remove it or move it before the '
                     + gen.Enth(first_curve) + ' line\n'
                   '> of the input file.'
                   )
            gen.WriteError(6041, err, log)
            gen.ErrorOnLine(line_number, line_text, log, False)
            return(None)


        # If we get to here, we have a valid curve keyword.  If it is not
        # a "userdata" curve we read the binary file that we want to plot
        # from.
        # Make the string at the end of the line the key, if there is one.
        if len(words) > 4:
            key_text = words[4].strip()
            # Now check if the user put the word *time in the key text
            # and it can be replaced by a time stored in settings_dict.
            # Having a time stored in settings_dict means this is a
            # time loop.
            # Note that this just replaces the first instance, and
            # only for time loops.
            if "#*time" in settings_dict:
                keytext_lc = key_text.lower()
                if "*time" in keytext_lc:
                    time = gen.FloatText(settings_dict["#*time"][0])
                    place = keytext_lc.find("*time")
                    key_text = key_text[:place] + time + key_text[place + 5:]
        else:
            # Make a note that we need to use the autokey.  The leading
            # "#" means that the user cannot spoof it.
            key_text = "#auto_key"


        # Check if the line had any commands to offset the values, multiply the
        # values, clip to a given window of X values or plot against time
        # instead of against distance.  If we are converting to US units
        # then the multipliers, offsets and clipping are applied AFTER
        # converting to US units.  We don't have to check for whether the
        # entries in the optionals dictionary are numbers because we've
        # already checked that in PROC GetOptionals.
        if "xmult" in optionals_dict:
            xmult1 = float(CheckForConstant(optionals_dict["xmult"],
                                             False, settings_dict))
        else:
            # The optional entry was not set so need to multiply by anything.
            xmult1 = 1.0
        if "xdiv" in optionals_dict:
            xdiv1 = float(CheckForConstant(optionals_dict["xdiv"],
                                             False, settings_dict))
        else:
            xdiv1 = 1.0
        if "xoffset" in optionals_dict:
            xoffset = float(CheckForConstant(optionals_dict["xoffset"],
                                             False, settings_dict))
        else:
            # No need to add anything after multiplying.
            xoffset = 0.0
        if "ymult" in optionals_dict:
            ymult1 = float(CheckForConstant(optionals_dict["ymult"],
                                             False, settings_dict))
        else:
            ymult1 = 1.0
        if "ydiv" in optionals_dict:
            ydiv1 = float(CheckForConstant(optionals_dict["ydiv"],
                                             False, settings_dict))
        else:
            ydiv1 = 1.0
        if "yoffset" in optionals_dict:
            yoffset = float(CheckForConstant(optionals_dict["yoffset"],
                                             False, settings_dict))
        else:
            yoffset = 0.0
        if "xstart" in optionals_dict:
            # xstart is a number that can be used to prevent the curve
            # from being plotted if the X values are below a certain
            # value.  This is most useful on moving trains.
            xstart = float(CheckForConstant(optionals_dict["xstart"],
                                             False, settings_dict))
        else:
            # Plot everything higher than minus infinity.
            xstart = -math.inf
        if "xstop" in optionals_dict:
            xstop = float(CheckForConstant(optionals_dict["xstop"],
                                             False, settings_dict))
        else:
            xstop = +math.inf

        if "xaxis" in optionals_dict and optionals_dict["xaxis"] != "time":
            versustime = False
        else:
            versustime = True

        # Figure out which pair of axes to plot on.
        if "axes" in optionals_dict:
            axes_candidate = optionals_dict["axes"]
            if axes_candidate in ("x", "y", "x1", "y1", "xy", "xy1", "x1y", "x1y1",):
                axes = "x1y1"
            elif axes_candidate in ("y2", "xy2", "x1y2"):
                axes = "x1y2"
            elif axes_candidate in ("x2", "x2y", "x2y1"):
                axes = "x2y1"
            elif axes_candidate == "x2y2":
                axes = "x2y2"
            else:
                print("Fouled up in the axes settings!  It was:", axes_candidate)
                return(None)
        else:
            # Nothing was set in the input, so use the default.
            axes = "x1y1"



        if keyword == "userdata":
            # Get the user data and a list of nicknames associated with them.
            # This is all the data in "begin data...end data" blocks and the
            # data in .csv files.
            user_data_dict = settings_dict["user_data"]
            user_keywords = user_data_dict.keys()

            if len(user_keywords) == 0:
                # There are no user-defined curves.
                err = ('> There was a curve definition for user-defined\n'
                       '> data in "' + file_name + '", but there\n'
                       '> were no blocks of user-defined data and not\n'
                       '> a block of .csv filenames in the file.'
                       )
                gen.WriteError(6043, err, log)
                gen.ErrorOnLine(line_number, line_text, log, False)
                return(None)

            elif (nickname.lower() not in user_keywords and
                "csv_" + nickname.lower() not in user_keywords):
                # There was userdata defined in the file but the keyword
                # is not valid.
                # Build a suitable error message depending on what user
                # data is present in the file.
                user_list = [entry for entry in user_keywords if entry[:4] != "csv_"]
                csv_list = [entry[4:] for entry in user_keywords if entry[:4] == "csv_"]

                err = ('> Found an invalid nickname ("' + nickname + '") for a\n'
                       '> user-defined curve in "' + file_name + '".\n'
                      )
                if user_list != [] and csv_list != []:
                    err = (err +
                           '> Please edit the file to correct it.  For what\n'
                           '> it is worth these are the names of the blocks\n'
                           '> of user data in the file:\n'
                           + gen.FormatOnLines(user_list) +
                           '\n> And these are the nicknames of the .csv files\n'
                           '> mentioned in the file:\n'
                           + gen.FormatOnLines(csv_list)
                          )
                elif user_list != []:
                    err = (err +
                           '> Please edit the file to correct it.  For what it\n'
                           '> is worth these are the nicknames of the blocks\n'
                           '> of user data in the file:\n'
                           + gen.FormatOnLines(user_list)
                          )
                else:
                    err = (err +
                           '> Please edit the file to correct it.  For what it\n'
                           '> is worth these are the nicknames of the .csv files\n'
                           '> mentioned in the file:\n'
                           + gen.FormatOnLines(csv_list)
                          )
                gen.WriteError(6044, err, log)
                gen.ErrorOnLine(line_number, line_text, log, False)
                return(None)
            # We have a valid nickname for a .csv file or a block of user data.
            # Get the columns of data to plot.
            try:
                # Check if it is a data block inside the file.  We know it
                # must be one or the other or we'd have raised error 6044.
                data = user_data_dict[nickname.lower()]
                csv = False
            except KeyError:
                # Nope, it's from a .csv file.
                data = user_data_dict["csv_" + nickname.lower()]
                csv = True

            # Get the column numbers (they start at 1) or the column nicknames.
            x_col = words[2]
            y_col = words[3]

            result = PickUserData(data, x_col, "X axis", nickname,
                                  settings_dict, line_number, line_text, log)
            if result is None:
                return(None)
            else:
                # Get the X values and the third block of QA data (the source
                # of the data and the name of the column of X data, if any).
                (x_data, QAlist3a, auto_key_x, col_header1,
                 xcol_num, data_name) = result
                # Set the US-SI conversion factor.
                x_USconv = "null"

            result = PickUserData(data, y_col, "Y axis", nickname,
                                  settings_dict, line_number, line_text, log)
            if result is None:
                return(None)
            else:
                # Get the Y values and the fourth set of QA data (the name
                # of the column of Y data, if any).
                (y_data, QAlist3b, auto_key_y, col_header2,
                                ycol_num, data_name) = result
                y_USconv = "null"
                # Concatenate the two lists of QA data into one.
                QAlist3 = QAlist3a + QAlist3b
                QAlist3.extend(['#']*7) # Pad out to align with SES QA lists.
                QAlist3.extend(['# X conversion factor, 1.0',
                                '# Y conversion factor, 1.0'])
                # Concatenate the two autokeys
                auto_key = auto_key_x + auto_key_y
                # Let later code know that there is no QA data
                # for each line of the results.
                QAdetails = None

                col_headers = [col_header1 + col_header2
                                + ["Plot X", "Plot Y"],
                               ["(-)"] * 4]

            if debug1:
                print("User data QA:", QAlist3)
                print("User data plotted:", tuple(zip(x_data, y_data))[:5])
            # Add the name of the source to the list of file names.  This
            # will either be the name of a .csv file or the word "block"
            # and the name of the block inside the source file.
            if data_name not in sources_list:
                sources_list.append(data_name)

            # Set a flag that identifies that this dataset cannot choose
            # between time or distance on the X axis, those concepts don't
            # mean anything in the context of user data.
            TX_axes = False

        else:
            # Check the nickname of the file we are using (the second word on
            # the line).
            if debug1:
                print("checking .hbn/.sbn file ", nickname)
            result = ReadBinData(line_number, line_text, nickname, settings_dict,
                                 files_dict, log)
            if result is None:
                return(None)
            else:
                # The result is all the fixed and transient data from a type
                # of file that we know Hobyah can process.
                bdat = result

            # Add the name of the source file to the list of plot files if
            # it is not already in there.
            if bdat.runfile_name not in sources_list:
                sources_list.append(bdat.runfile_name)

            # Now build a string that has the line text with the line
            # number appended to it.
            line_QA = line_text + "# from line " + str(line_number)




            # We know - because there were no faults in ProcessBlock -
            # that we have four words here.  Get property we want to plot
            # and where/when we want to plot it.
            prop = words[1]
            where = words[3]

            if keyword in ("transient", "profile", "icons", "fandata"):
                # transients and profiles have a common structure of
                # four words.  The last word does most of the heavy
                # lifting.  Example structures are:
                #  "transient" property nickname route_num@chainage
                #  "transient" property nickname train_num@offset
                #  "transient" property nickname train_num@<ignored number>
                #  'transient" property nickname 101-106-5m
                #  "transient" property nickname 101-106-5
                #  "transient" property nickname 123-5m
                #  "transient" property nickname 123-5
                #  "transient" property nickname segment@distance
                #  "transient" property nickname zone3
                #  "profile"   property nickname route@time
                #  "property"  property nickname entity
                #  "icons"     property nickname entity
                #
                # Check if this property is valid for this file.
                # Some properties, such as pressure) are only available
                # in SES runs with certain supplementary output options.
                # We send over the contents of the binary file, the property
                # and the type of curve it is in (for errors) as well
                # about which line is being read, for the error messages.
                result = bdat.CheckProperty(prop, keyword, file_name,
                                            line_number, line_text)
                if result is None:
                    return(None)
                else:
                    # This is a Boolean, True if it is transient, False
                    # if it is fixed.
                    trans_curve = result

                # We now know that the property is a valid one and is
                # appropriate to this type of plot.  We now process
                # the fourth word.  In a transient curve this will
                # be something giving an entity like a route and a
                # chainage along it, or a train number and a distance
                # from the train's down end, in the form of "route4@1200"
                # or "train54@-130".  In an SES file it may
                # be an SES-specific location, e.g. 106-2m (the midpoint
                # of the 2nd subsegment of segment 106).
                # Note that some of the functions of "SubstituteAt" are
                # duplicated in bdat.CheckAt.   This is unavoidable,
                # as we need to call CheckAts here so that if the number
                # after the @ is the name of a constant then the value of
                # the constant can be substituted in before calling this.
                result = SubstituteAt(where, bdat.prog_type,
                                      trans_curve, settings_dict,
                                      line_number, line_text, log)
                if result is None:
                    return(None)
                else:
                    # The variable "where2" may be an exact copy of
                    # "where".  The only time it is not is when a user
                    # has set up an entry in the constants block (say
                    # by assigning the value 900 to the name "sensor3")
                    # then feeding the location "route1@sensor3" to
                    # PROC CheckAts.  In that case it will have returned
                    # "route1@900".
                    where2 = result
                # Now we diverge a bit and process transient and profile
                # separately.
                if keyword == "transient":
                    # Transient curves need one pandas dataframe key and
                    # plot all the data at that point over all times in
                    # the file.

                    # Check if this location exists in the file and get
                    # the key to access it.
                    result = bdat.TransientAt(where2, prop, keyword,
                                              graph_units, file_name,
                                              line_number, line_text)
                    if result is None:
                        return(None)
                    else:
                        where3 = result
                        # The variable "where3" may be an exact copy of
                        # "where2", or it may be modified from it to be the
                        # correct key for the property.  For example, if
                        # "where2" was "route1@900" in an SES run, "where3"
                        # would have been turned into the section, segment,
                        # subsegment or subpoint closest to that location
                        # (depending on what the property being plotted
                        # was).  If the location is outside the tunnel then
                        # "where3" will be the string "open_air".
                        # If the location was a train, "where3" will be
                        # "where2" but converted to lower case.
                        # In Hobyah runs it may be a string referring to
                        # a train or two-element list giving the index of
                        # a segment in the list of pandas databases and a
                        # gridpoint distance that can be used as a index
                        # in a pandas database for that gridpoint.


                    if where3[:5] == "train":
                        # The data is either the property of a train as it
                        # passes through the model (e.g. tractive effort)
                        # or a set of properties seen by a train as it passes
                        # through the tunnel (e.g. the air temperature of
                        # a thermometer on the train next to the intake of
                        # an air conditioning unit on the rear carriage).
                        result = bdat.GetTrainData(prop, where, where3,
                                                   line_QA, graph_units)
                        if result is None:
                            return(None)
                        elif result[-1] is True:
                            (QAlist3, x_times, x_dists, x_USconv, y_data,
                             y_USconv, auto_key, col_headers, TX_axes) = result
                        else:
                            # The property being plotted was either
                            # distance vs time or time vs distance,
                            # so treat it as if it was a transient.
                            (QAlist3, x_data, x_USconv, y_data, y_USconv,
                             auto_key, col_headers, TX_axes) = result
                        QAdetails = None
                    else:
                        # At this point we know that the location exists at
                        # a fixed location in the file and either have the
                        # key to access it in a pandas database or the string
                        # "open_air".  Get the data.
                        result = bdat.GetTransientData(prop, where, where3,
                                                       line_text, graph_units)
                        if result is None:
                            return(None)
                        else:
                            (QAlist3, x_data, x_USconv, y_data, y_USconv,
                             auto_key, col_headers, TX_axes) = result
                            # Let later code know that there is no QA data
                            # attached to each line of the results (unlike
                            # in profile plots)
                            QAdetails = None
                elif keyword == "profile":
                    # It's a profile curve.
                    #
                    # Profile curves need a route number and a time.  They
                    # either plot all the data along that route at one time
                    # or generate a set of icons for trains, fires, jet
                    # fans and such.  Even if the profile is fixed (like a
                    # vertical profile) a time is needed.

                    # Check if this route exists in the file and get the
                    # time to plot it at.  If the time given in where2
                    # does not match a time in the file, it is rounded
                    # down to the next valid time.
                    result = bdat.RouteAt(where2, prop, keyword,
                                          graph_units, file_name,
                                          optionals_dict,
                                          line_number, line_text)
                    if result is None:
                        return(None)
                    else:
                        (transient, distances, keys, time, result_time) = result
                        # What we get back depends on if this curve is
                        # a property that does not change with time (such
                        # as track height versus distance) or a transient
                        # property (such as air velocity versus distance).
                        # If it is fixed, the first entry in 'result'
                        # is a False value Boolean called 'transient'.
                        if transient:
                            # Check if it is a traffic property.
                            if bdat.IsTrafficProperty(prop.lower()):
                                result = bdat.GetProfileTraffic(distances,
                                                prop, where, where2,
                                                graph_units, keys,
                                                line_number, line_text,
                                                result_time, time)
                            else:
                                # Get the transient profile from a
                                # routine that extracts info from the
                                # pandas databases.
                                result = bdat.GetProfileTransient(distances,
                                                prop, where, where2,
                                                graph_units, keys,
                                                line_text, optionals_dict,
                                                result_time, time)
                        else:
                            # Get the fixed profile from a routine that
                            # extracts info from the lists in the route
                            # definition.  We ignore the distances and
                            # the time.
                            result = bdat.GetProfileFixed(prop, where,
                                            where2, optionals_dict,
                                            graph_units, line_text)
                        if result is None:
                            # Something went wrong.
                            return(None)
                        else:
                            # It is for a curve.
                            (QAlist3, x_data, x_USconv, y_data, y_USconv,
                             auto_key, col_headers, QAdetails, TX_axes) = result

                elif keyword == "icons":
                    # It's a set of icons.  These are similar to profiles
                    # but build a set of "set object" commands instead of
                    # plotting a curve.  The icons are of trains, fires,
                    # jet fans, adits

                    # Check if this route exists in the binary file and get
                    # the time to plot it at.  If the time given in where2
                    # does not match a time in the file, it is rounded down
                    # to the next valid time.
                    result = bdat.RouteAt(where2, prop, keyword, graph_units,
                                          file_name, optionals_dict,
                                          line_number, line_text)
                    if result is None:
                        return(None)
                    else:
                        (transient, distances, keys, time, result_time) = result
                        if prop == "trains":
                            # The result is a list of lists that can be
                            # used to construct gnuplot's "set object
                            # polygon" commands.
                            results = bdat.TrainIcons(where2, line_number,
                                                 optionals_dict, result_time)
                        elif prop == "fires":
                            # The result is a list of lists that can be
                            # used to construct polygon commands and an
                            # air velocity.
                            results = bdat.FireIcons(where2,
                                                 optionals_dict, result_time)
                        elif prop == "jetfans":
                            # The result is a list of lists that can be
                            # used to construct polygon commands and an
                            # arrow.
                            results = bdat.JetFanIcons(where2, line_number,
                                                 optionals_dict, result_time)
                        for entry in results:
                            result = WriteGnuPolygon(entry,optionals_dict,
                                                     xmult1, xdiv1, xoffset,
                                                     ymult1, ydiv1, yoffset,
                                                     tr_index, settings_dict,
                                                     plt)
                elif keyword == "fandata":
                    # We want to plot something on the flow-pressure plane.  A
                    # fan characteristic, a system characteristic curve or a
                    # duty point.
                    # First check if the property is correct and figure out if
                    # it is a fixed property (like the input fan characteristic)
                    # or a transient property (like the fan characteristic at
                    # an instant in time as the fan runs up or down).
                    result = bdat.FanAt(where2, prop, keyword, graph_units,
                                            file_name, line_number, line_text)
                    if result is None:
                        return(None)
                    else:
                        (transient, fan_name, time, result_time) = result
                        # What we get back depends on if this curve is
                        # a property that does not change with time (such
                        # as the fan characteristic the user set) or a
                        # transient property (such as the fan characteristic
                        # when it is running at 75% speed at an instant
                        # in time).
                        # If it is fixed, the first entry in 'result'
                        # is a False value Boolean called 'transient'.
                        if transient:
                            # Get the transient profile from a routine that
                            # extracts info from the fan definition and the
                            # pandas databases.
                            result = bdat.GetFanDataTransient(fan_name,
                                            prop, where, where2, graph_units,
                                            line_text, result_time, time)
                        else:
                            # Get the fixed profile from a routine that
                            # extracts info from the fan definition.
                            result = bdat.GetFanDataFixed(fan_name,
                                            prop, where, where2, graph_units,
                                            line_text)
                        if result is None:
                            # Something went wrong.
                            return(None)
                        else:
                            # It is for a curve.
                            (QAlist3, x_data, x_USconv, y_data, y_USconv,
                             auto_key, col_headers) = result
                            # Set some variables that are needed later.
                            QAdetails = None
                            TX_axes = False
            else:
                # We have added code to accept a curve type that we
                # have not added code to handle (at the time of writing
                # it is "waterfall" or "property").
                err = ('> Found a type of curve in "' + file_name + '"\n'
                       '> that cannot be processed yet ("'
                          + keyword + '"). Please raise a bug report.\n'
                       )
                gen.WriteError(6045, err, log)
                gen.ErrorOnLine(line_number, line_text, log, False)
                gen.OopsIDidItAgain(log, file_name)
                return(None)

        if keyword not in "icons":
            # If we get to here we have two lists of data and some lines of QA data.
            # Create lists to hold the data.  We create eight lists
            # but may only use some of them.  These are the original
            # X data (which may be time, distance or both), the original
            # source of the values on the Y axis (in SI Units), the distance
            # in US units (if used) and the original source of the values
            # on theY axis (in US units).
            # Next are the modified X and Y values (multiplied and offset)
            # in the plot units - these are the columns that gnuplot uses
            # when plotting the curve.  Finally there is a list of QA data
            # which are things like the subpoint locator in plots along
            # a route.
            orig_time = []
            orig_dist = []  # Distance along a route in SI units
            orig_Y = []     # Value in the binary file
            US_dist = [] # Distance along a route in US units
            US_Y = [] # copy of 'orig_Y' but in US units.
            x_modded1 = [] # The X value to plot
            y_modded1 = [] # The Y value to plot, multiplied, offset and trimmed
            QA_list = []

            if not TX_axes:
                # We do not have both time and distance on the X axis.
                # Spoof the distances for the enumerate below.
                x_times = x_data

            # Check if we need to add QA data to the end of each line.
            if QAdetails is None:
                has_QA = False
            else:
                has_QA = True

            # Now iterate over our lists of values and build lists of the
            # entries to write to the curve data file.
            for (index, (x_time, y_value)) in enumerate(zip(x_times, y_data)):
                if (x_time == "     ") or (y_value == "     "):
                    # It's a break in the curve.  Tag that we need a blank entry
                    # unless the last entry was also a blank.
                    if x_modded1 != [] and x_modded1[-1] != "blank":
                        orig_time.append("")
                        orig_Y.append("")
                        if TX_axes:
                            orig_dist.append("")
                        US_dist.append("")
                        US_Y.append("")
                        x_modded1.append("blank")
                        y_modded1.append("")
                        if has_QA:
                            QA_list.append("")
                        if debug1:
                            print("Added a break in a curve")
                else:
                    # Store the X and Y values from the binary file in the first
                    # and third columns.
                    orig_time.append(x_time)
                    orig_Y.append(y_value)
                    if TX_axes:
                        # Process the second set of X-axis values (which are
                        # distances in a time-based dataset tied to a moving
                        # train) and store them in the second column (SI value)
                        # and the fourth column (US value).
                        x_dist = x_dists[index]
                        orig_dist.append(x_dist)
                        # Convert the distance value to feet.
                        (x_US, discard) = USc.ConvertToUS(x_USconv, x_dist,
                                                          debug1, log)
                        US_dist.append(x_US)
                    elif x_USconv != "null":
                        # The variable we have tagged as time is not actually time
                        # in this curve, it is something else, like fan volume flow.
                        # We use its conversion factor to give us the values in
                        # 'x_time' in US units.
                        (x_US, (x_SIunits, x_USunits)) = USc.ConvertToUS(
                                                             x_USconv, x_time,
                                                             debug1, log)
                        US_dist.append(x_US)
                    else:
                        # The X variable has a null conversion factor, so its
                        # value in US units is the same as in SI units.  We make
                        # a copy.
                        x_US = x_time
                        US_dist.append(x_time)

                    (USy_value, (SI_text, US_text)) = USc.ConvertToUS(
                                                          y_USconv, y_value,
                                                          debug1, log)
                    US_Y.append(USy_value)

                    if has_QA:
                        try:
                            QA_list.append(QAdetails[index])
                        except:
                            print("A QA details exception occurred", prop,
                                  index, QAdetails, len(QAdetails))
                            print(x_data, len(x_data))
                            sys.exit()

                    # Now we generate the columns that we tell gnuplot to use
                    # when it plots.  We apply the transformations and clipping:
                    # Multiply by its multiplier (default 1.0), divide by its
                    # divisor (default 1.0) then add its offset (default 0.0).

                    if graph_units == "si":
                        x_to_use = x_time
                    else:
                        x_to_use = x_US
                    if TX_axes and not versustime:
                        # There was an optional argument choosing to plot
                        # against distance.
                        if graph_units == "si":
                            x_to_use = x_dist
                        else:
                            x_to_use = x_US

                    x_modval = x_to_use * xmult1 / xdiv1 + xoffset

                    # Check if the value lies between xstart and xstop (inclusive) and
                    # add a datapoint if it does.
                    if xstart <= x_modval <= xstop:
                        # x_modded1.append(x_modval)

                        # Add the data to the first pair of lists.
                        if keyword in ("transient", "waterfall"):
                            # The X value is time or distance, so round it to one
                            # microsecond or micrometre.
                            x_modded1.append(round(x_modval,6))
                        else:
                            x_modded1.append(x_modval)

                        # Now translate and scale the Y value and add it to the
                        # list to be plotted.
                        if graph_units == "us":
                            y_modval = USy_value * ymult1 / ydiv1 + yoffset
                        else:
                            y_modval = y_value * ymult1 / ydiv1 + yoffset
                        y_modded1.append(y_modval)
                    else:
                        # We are not plotting this particular entry because
                        # it lies outside the specified range, even though
                        # data exists for it.  We need to spoof blank entries
                        # so that the rows of data line up.
                        x_modded1.append("      ")
                        y_modded1.append("      ")
            # Now figure out how many columns we need to write to the file.
            # There could be anywhere between four and eight.
            if TX_axes:
                if has_QA:
                    # We need to zip together all eight lists: times, distances (SI),
                    # Y values (SI), distances (US), Y value (US), X values to plot,
                    # Y values to plot, QA locations.
                    # This type of curve is used for segment data as seen
                    # by moving trains, where the user can choose to plot
                    # the data against time or chainage.  Things like
                    # air velocity in the open tunnel ahead of the train.
                    curve_lists = zip(orig_time, orig_dist, orig_Y,
                                      US_dist, US_Y,
                                      x_modded1, y_modded1,
                                      QA_list)
                    col_headers[1].append("Location")
                else:
                    # We need to zip together seven lists: times, distances (SI),
                    # Y values (SI), distances (US), Y value (US), X values to plot,
                    # Y values to plot.
                    # This type of curve is used for train data on
                    # moving trains, where the user can choose to plot
                    # the data against time or chainage.  Things like
                    # train speed vs time or train speed vs chainage.
                    curve_lists = zip(orig_time, orig_dist, orig_Y,
                                      US_dist, US_Y,
                                      x_modded1, y_modded1)
                if versustime:
                    # We need to replace the X plot column header (the default
                    # header is "Plot distance").
                    col_headers[0][5] = "Plot time"
                    col_headers[1][5] = "   (s)   "
                # Set which column to use to plot the X value.  This is one
                # lower than the actual column becaue we use it as a Python
                # list index.
                xcol = 5
            elif keyword == "userdata":
                # Userdata has no concept of SI or US units, so only has
                # four columns: the original X and Y values and the
                # modified X and Y values.
                curve_lists = zip(orig_time, orig_Y,
                                  x_modded1, y_modded1)
                # Set which columns to use to plot.
                xcol = 2
            elif x_USconv != "seconds":
                if has_QA:
                    # We need to zip together seven lists: X values in SI
                    # units (such as metres), X values in US units (ft),
                    # Y values in SI units (such as m/s), Y values in
                    # US units (fpm), X values to plot, Y values to plot,
                    # QA locations.
                    # This type of curve is used for data plotted along
                    # routes, such as air velocity vs distance.
                    curve_lists = zip(orig_time, orig_Y,
                                      US_dist, US_Y,
                                      x_modded1, y_modded1,
                                      QA_list)
                    col_headers[1].append("Location")
                else:
                    # We need to zip together six lists: X values in SI
                    # units (such as m^3/s), X values in US units (CFM),
                    # Y values in SI units (such as Pa), Y values in
                    # US units (IWG), X values to plot, Y values to plot.
                    # This type of curve is used for things like fan
                    # characteristics in the flow-pressure plane.
                    curve_lists = zip(orig_time, orig_Y,
                                      US_dist, US_Y,
                                      x_modded1, y_modded1)
                # Set which columns to use to plot.
                xcol = 4
            else:
                if has_QA:
                    # We need to zip together six lists: times,
                    # Y values (SI), Y value (US), X values to plot,
                    # Y values to plot, QA locations.
                    # This type of curve hasn't been used yet, but it's
                    # better to write it now than later.
                    curve_lists = zip(orig_time, orig_Y,
                                      US_Y,
                                      x_modded1, y_modded1,
                                      QA_list)
                    col_headers[1].append("Location")
                else:
                    # We need to zip together five lists: times,
                    # Y values (SI), Y value (US), X values to plot,
                    # Y values to plot.
                    # This is typically used for transient plots at
                    # entities, such as damper area vs. time.
                    curve_lists = zip(orig_time, orig_Y,
                                      US_Y,
                                      x_modded1, y_modded1)
                # Set which columns to use to plot.
                xcol = 3


            # Make up a name for the datafile.  We have the base file name, a
            # page number, graph number and curve number.  The default extension
            # is .txt but the user can tell it to use .csv by putting
            # "plotnames .csv" (without double quotes) in the settings block.
            curve_file_name = (file_stem + '-p' + str(page_num) + '-g'+ str(graph_num)
                               + '-c' + str(tr_index + 1) + plotnames)
            # Now open a file in the "ancillaries" folder to hold the data.
            try:
                handle = open(ancill_path + curve_file_name, 'w', encoding='utf-8')
            except PermissionError:
                # This is unlikely to happen but it is still worth catching it.
                err = ('> Skipping "' + file_name + '", because\n'
                       "> you do not have permission to write to\n"
                       '> one of its data files, "' + curve_file_name + '" in\n'
                       '> the "ancillaries" folder.')
                gen.WriteError(6046, err, log)
                return(None)



            # Put various bits of QA data at the top of the curve data file.
            QAlist2 = ['# Data file name, "' + curve_file_name + '"',
                       '#',
                      ]


            if settings_dict["autokeys"] == "on" or key_text == "#auto_key":
                # The user did not set a curve key, so use the auto_key.  Print
                # it in the % size text defined in "settings_dict" (default
                # is 65% size).
                mult = gen.FloatText(settings_dict["keytextscale"])
                key_text = '"{/*' + mult + ' ' + SanitiseEnhancedString(auto_key) + '}"'
            elif key_text.lower() in  ("ignore", "no key", "nokey",
                                       "nolabel", "notitle"):
                # This is a special to show no key entry for this line.  It
                # is typically used when vertical profiles are wanted on the
                # graph but their description is not wanted in the key.
                key_text = ''

            # Add the text on the line of data to the first QA list.  Include
            # the line number in the Hobyah input file that it came from.
            if '"' not in line_text:
                # The line text doesn't include double quotes so add them
                # in case it has commas.
                source_line = ['# ' + gen.Enth(line_number) + ' line, "'
                                + line_text.strip() + '"']
            else:
                source_line = ['# ' + gen.Enth(line_number) + ' line, '
                                + line_text.strip()]

            if key_text == '':
                delim = '"'
            elif (key_text[0] == '"' and key_text[-1] == '"' and   \
                '"' in key_text[1:-1]) or   \
               (key_text[0] == "'" and key_text[-1] == "'" and   \
                "'" in key_text[1:-1]):
                # There are quote marks, but they are delimiters set but
                # the user.
                delim = ''
            elif '"' in key_text and "'" in key_text:
                delim = "'''"
            elif '"' in key_text:
                delim = "'"
            else:
                delim = '"'
            QAlist4 = ['# Auto key, ' + auto_key,
                       '# Key used, ' + delim + key_text + delim,
                       '# Multiplier on the X axis (applied first), ' + gen.FloatText(xmult1/xdiv1),
                       '# Offset added to the X axis (applied second), ' + gen.FloatText(xoffset),
                       '# Multiplier on the Y axis (applied first), ' + gen.FloatText(ymult1/ydiv1),
                       '# Offset added to the Y axis (applied second), ' + gen.FloatText(yoffset),
                       '# X value at which data starts being plotted, ' + gen.FloatText(xstart),
                       '# X value at which data stops being plotted, ' + gen.FloatText(xstop),
                       ]

            for QAlist in (QAlist1, source_line, QAlist2, QAlist3, QAlist4):
                for QA_line in QAlist:
                    gen.WriteOut(QA_line, handle)

            # Add the lines of column headers.  First line has the
            # description, second line has the units.
            for entries in col_headers:
                # Generate a suitable list.  We don't enclose it in
                # double quotes as we probably won't have commas.
                line = ',  '.join([text for text in entries])
                gen.WriteOut(line, handle)

            # for (index,line) in enumerate(data):
            for line in list(curve_lists):
                if line[xcol] == "blank":
                    # Put in two blank lines so that gnuplot breaks the
                    # curve here.
                    gen.WriteOut("\n", handle)
                else:
                    # Check to see if the last entries in the line are
                    # blank entries, not numbers.  Check twice, because
                    # if an entry exists in the file and is out of the
                    # plot range there will be one string of spaces for
                    # the X value and a second string of values for the
                    # Y value.  This check cleans up entries that are
                    # not plotted due to  restrictions on the range of
                    # X values being plotted (the 'xstart' and 'xstop'
                    # optional entries).  It doesn't affect plot types
                    # that have QA data at the end of the line because
                    # the QA data is never "      ".
                    for index in range(2):
                        if line[-1] == "      ":
                            line = line[:-1]
                    # Put in the numbers separated by commas.
                    gen.WriteOut(",    ".join(gen.FloatText(entry) for entry in line), handle)
            handle.close()


            # Figure out which columns to tell gnuplot to use.
            using = " using " + str(xcol + 1) + ":" + str(xcol + 2)

            # Check if a linetype was set
            if "lt" in optionals_dict:
                linetype = " linetype " + optionals_dict["lt"]
            else:
                linetype = ""

            if "lw" in optionals_dict:
                linewidth = " linewidth " + optionals_dict["lw"]
            else:
                linewidth = ""

            # Now generate the gnuplot plot command.  This varies depending
            # on whether this is the first line or not, and also on whether
            # the plot is in US units (columns 3 & 4) or SI units (columns
            # 1 & 2).
            if first_curve is True:
                line = 'plot "'
                first_curve = False
            else:
                line = ', \\\n     "' # \\ is one backslash, gnuplot's continuation char.
            line = (line + curve_file_name + '"' + using + ' title '
                    + ChooseGnuplotString(key_text)
                    + ' axes ' + axes + ' with lines' + linetype + linewidth )
            plot_string = plot_string + line

        # When we get to here we've processed a line of input, generated
        # a file of curve data in the "ancillaries" subfolder and written
        # a line to plot the curve data to the gnuplot command file.
        tr_index += 1

    # Check if we need to write the QA data before plotting.  We do
    # this only on the last graph or the last image.
    if last_doodad:
        PageQA1(settings_dict, sources_list, plt)

    # Write the lines for the plot to the file.
    gen.WriteOut(plot_string, plt)

    return(tr_index, sources_list)


def WriteGnuPolygon(entry, optionals_dict, xmult, xdiv, xoffset,
                    ymult, ydiv, yoffset, tr_index, settings_dict, plt):
    '''Take values defining a polygon, a dictionary of optional entries
    setting things like a polygon's colour, a set of offsets and multipliers
    and turn them into a gnuplot "set object polygon" command that draws
    the polygon.  The polygon may be accompanied by arrows and labels.

    Figure out the polygons nodes and write the gnuplot commands defining
    the polygons to the .plt file.

        Parameters:
            entry           {}              A tuple of data defining a polygon,
                                            its comments and instructions on
                                            how to generate it.
            optionals_dict  {}              Dictionary of the optional entries
                                            on the line that defined the
                                            polygons.
            xmult           float           Multiplier on the X values.
            xdiv            float           Divisor on the X values.
            xoffset         float           Value to add to the X values.
            ymult           float           Multiplier on the Y values.
            ydiv            float           Divisor on the Y values.
            yoffset         float           Value to add to the Y values.
            tr_index        int             Pointer to where the line is in
                                            line_triples.
            settings_dict   {}              Dictionary of the run settings.
            plt             handle          The handle of the .plt file.

        Returns:
            None.
    '''
    units = settings_dict["graphunits"]
    debug1 =  settings_dict["debug1"]

    # Set the default axes text.  By default X values are on the "first"
    # axis.  Y values are on the "first" axis unless the optional profile
    # setting is "flat", in which case the Y values are on the graph
    # frame.
    xtext = "first "
    if "profile" in optionals_dict and optionals_dict["profile"] == "flat":
        # This is the flat icons which we want to put at the base of
        # the graph frame.
        ytext = "graph "
        flat = True
    else:
        # The co-ordinates include a height value.  Default to plotting them
        # on the first Y-axis.
        ytext = "first "
        flat = False


    # Now check to see if the user set custom axes.  No need to check
    # for "x" or "x1" because we default to the first x axis.
    if "axes" in optionals_dict:
        axes_candidate = optionals_dict["axes"]
        if axes_candidate == "x2":
            # Change to the second X-axis.
            xtext = "second "
        elif axes_candidate in ("y", "y1", "xy", "xy1", "x1y", "x1y1"):
            # The user wants to plot on the first X- and Y-axes.  This
            # overwrite the default of plotting on the graph frame for
            # flat icons.
            ytext = "first "
        elif axes_candidate in ("y2", "xy2", "x1y2"):
            ytext = "second "
        elif axes_candidate in ("x2y", "x2y1"):
            xtext = "second "
            ytext = "first "
        elif axes_candidate == "x2y2":
            xtext = "second "
            ytext = "second "

    # Check if an icon height has been set.
    if "height" in optionals_dict:
        height = float(optionals_dict["height"])
    elif flat:
        # In flat profiles the icon is 3.5% of the graph frame height.
        height = 0.035
    elif units == "si":
         # In elevation and stack profiles 6.0 m seems to be a comfortable
         # default
        height = 6.0
    else:
        height = 19.7 # 6 m, but in feet (we already converted the elevations).

    if "float" in optionals_dict:
        base = float(optionals_dict["float"])
    elif flat:
        # In flat profile set the base of the icon to be 2% of the graph
        # frame's height above the base of the frame.  This clears the
        # major ticmarks.
        base = 0.02
    elif units == "si":
        # In elevation and stack profiles icons appear 0.75 m above the
        # line of the vertical profile.  This generally keeps them clear
        # of the vertical profile (if plotted).
        base = 0.75
    else:
        base = 2.5 # 0.75 m, but in feet.

    # Get the comment associated with the icon.  This tells
    # us whether we are plotting train icons, fires jet fans etc.
    comment = entry[2]

    # Figure out if the user set any optional entries defining the
    # object's colour or the height.  We accept the keywords "colour"
    # and "color".  If we have both we use the one in "colour", because
    # I can't be bothered to complain if someone sets both.  This
    # option does not apply to fires.
    if "colour" in optionals_dict:
        colour = optionals_dict["colour"]
        custom = True
    elif "color" in optionals_dict:
        colour = optionals_dict["color"]
        custom = True
    elif "train" in comment:
        # Set a light blue colour as the default.
        colour = '#00BFFF'
        custom = False
    else:
        # Set black as the default.
        colour = 'black'
        custom = False
    # If a custom colour was set, check if it is a six-digit
    # hexadecimal number (an RGB number).
    if custom and len(colour) == 6:
        try:
            # If this doesn't raise an exception, the colour is a
            # valid hex number.
            int(colour, 16)
        except ValueError:
            # It's not.  Hopefully it is the name of one of gnuplot's
            # predefined colours.  We'll let gnuplot complain if it
            # is not.
            pass
        else:
            # Prepend a "#" character to it for gnuplot.
            colour = '#' + colour

    if flat and ytext in ("first ", "second "):
        # Issue a warning to the user that they have chosen to plot
        # flat icons (which are intended to be set to the graph frame)
        # but have put them onto a Y axis.  Get the line number from
        # the comments.
        message = ("> Warning: You have chosen to plot flat train icons\n"
                   "> but have set custom axes.  This is unusual and you\n"
                   "> may want to remove the custom axis definition from\n"
                   "> line " + comment.split()[-1] + ".")
        print(message)



    if "train" in comment:
        # Unpack the entries we need for the polygons and scale them.
        # Note that we can still use this when plotting flat icons
        # because the elevations are all zero.
        (chs, elevs, comment, downclip, upclip) = entry

        chs = TranslateChs(entry[0], xmult, xdiv, xoffset, units)
        elevs = TranslateChs(entry[1], ymult, ydiv, yoffset, units)

        # Call a routine that turns the chainages and elevations
        # into sets of vertices that draw each train.  These include
        # all the offsets and multipliers but does not do clipping.
        polygons = BuildTrainVertices(chs, elevs, downclip, upclip, base,
                                      height, colour, comment,
                                      units, debug1)
    elif "fire" in comment:
        (chs, elevs, comment, reverse, u_ann, u_open, operating) = entry

        chs = TranslateChs(entry[0], xmult, xdiv, xoffset, units)
        elevs = TranslateChs(entry[1], ymult, ydiv, yoffset, units)

        if "length" in optionals_dict:
            # The user set a custom length for a fire or jet fan icon.
            # This doesn't affect trains, which are always the length
            # of the train type.
            length = float(optionals_dict["length"])
        else:
            # Make the fire 90 m long by default.  Fully ablaze!  This
            # works well when plotting fires on profiles that are 5 km
            # long but gets a bit wide on profiles 1 km long.
            length = 90

        if units == "us":
            # Convert the length to feet.
            length = length/0.3048

        # Send the whole of 'entry' to the routine so we can figure out
        # the direction of airflow and point the flames in the correct
        # direction.
        subst_entry = (chs, elevs, comment, reverse, u_ann, u_open, operating)
        (polygons, labels) = BuildFireVertices(subst_entry, base, height,
                                               length, flat, units, debug1)
    elif "jet fan" in comment:
        (chs, elevs, comment) = entry

        chs = TranslateChs(entry[0], xmult, xdiv, xoffset, units)
        elevs = TranslateChs(entry[1], ymult, ydiv, yoffset, units)

        # Set an aspect ratio for how stretched the fan is.  We expect to be
        # showing jet fans on an exaggerated vertical profile, so we want
        # the corrections for rotation to include this factor so that the
        # fans end up looking correct when rotated.
        if "aspect" in optionals_dict:
            aspect = float(optionals_dict["aspect"])
        else:
            # Set a default aspect ratio of 30, which means that when jet
            # fans are rotated, the ends of the silencers are a bit squarer
            # than they would otherwise be.
            aspect = 30
        # Set the default horizontal distance, which we'll take as one-tenth
        # of the length of the fan (I do this because it'll be convenient
        # to have a fan that has two 2D silencers, and a 1D fan unit).
        if "length" in optionals_dict:
            length = float(optionals_dict["length"])
        else:
            length = 40

        if units == "us":
            # Convert the aspect and length to feet.  Height is already in feet.
            aspect = aspect/0.3048
            length = length/0.3048

        # Move the jet fans up so that trains pass under them.
        if flat:
            base = base + 0.055
        elif units == "us":
            base = base + 23
        else:
            base = base + 7 # Move the jet fans 7 m up




        # Send the whole of 'entry' to the routine so we can figure out
        # the direction of jet fan operation/off.
        subst_entry = (chs, elevs, comment)
        (polygons, lines) = BuildJetFanVertices(subst_entry, base, height,
                                                length, aspect, colour,
                                                flat, debug1)
        # Write the polygons to the file.
        for arrowdef in lines:
            WriteOneArrow(arrowdef, xtext, ytext, plt)

    # Write the polygons to the file.
    WriteManyPolygons(polygons, tr_index, xtext, ytext, plt)
    return()


def WriteManyPolygons(polygons, tr_index, xtext, ytext, plt):
    '''Take set of polygon definitions, xtext and ytext and the handle of
    the .plt file.
    Build a unique number for each polygon and write them all to the .plt
    file.

        Parameters:
            polygons        ()              A tuple of data defining polygons.
            tr_index        int             Pointer to where the line is in
                                            line_triples.
            xtext           str             Identifies which X axis to plot on.
                                            Could be "first ", "second " or
                                            "graph ".
            ytext           str             Identifies which Y axis to plot on.
            plt             handle          The handle of the .plt file.

        Returns:
            None.
    '''
    for polygon in polygons:
        (xvals, yvals, colour, comment) = polygon
        # Make the polygon number unique by appending the line number
        # to it, then call a routine that writes the polygon definition
        # to the .plt file.
        WriteOnePolygon(xvals, yvals, xtext, ytext, colour, comment, plt)
    return()


def WriteOneArrow(arrowdef, xtext, ytext, plt):
    '''Take an object number, pairs of Y and Y values, arrowhead text,
    xtext and ytext (which could be "first ", "second " or "graph ", a
    colour name and a comment.  Write an arrow to the .plt file.
    At some point we will need to include the ability to adjust the
    arrowheads and turn them off, but at the moment there is an arrowhead
    at the right end (for jet fan icons).

        Parameters:
            arrowdef        (,)             A tuple of file entries:
              xvals         []               A pair of X axis values.  These
                                             have already been translated
                                             and adjusted by offsets and
                                             multipliers.
              yvals         []               A pair of Y axis values.
              arrow         str              One of the words " head",
                                             " nohead", "backhead" or
                                             "nohead".  Controls the
                                             arrowheads put on the lines.
              colour        str              String identifying a colour.
                                             This could be a name that gnuplot
                                             recognizes or a hex RGB value.
              comment       str              A comment to add to the arrow
                                             to identify it to anyone trying
                                             to debug the .plt file.
            xtext           str             Identifies which X axis to plot on.
                                            Could be "first ", "second " or
                                            "graph ".
            ytext           str             Identifies which Y axis to plot on.
            plt             handle          The handle of the .plt file.

        Returns:
            None.
    '''
    global poly_num
    poly_num += 1

    (xvals, yvals, arrow, colour, comment) = arrowdef
    line = ('  set arrow ' + str(poly_num) + ' from'
                + Write2Vertices(xvals[0], yvals[0], xtext, ytext, False)
                + '  \\\n' + ' '* 25 + "to"
                + Write2Vertices(xvals[1], yvals[1], xtext, ytext, False)
                + arrow + '  \\\n' + ' '* 28 + 'linecolor rgb "' + colour + '"')

    if comment != "":
        gen.WriteOut("  # Object below is " + comment, plt)
    gen.WriteOut(line, plt)
    return()


def WriteOnePolygon(xvals, yvals, xtext, ytext, colour, comment, plt):
    '''Take an object number, lists of Y and Y values, xtext and ytext
    (which could be "first ", "second " or "graph ", a colour name
    and a comment.  Write a polygon to the .plt file.

        Parameters:
            xvals           []              List of the nodes on the X axis.
                                            These have been translated and
                                            adjusted by offsets and multipliers.
            yvals           []              List of the nodes on the Y axis.
            xtext           str             Identifies which X axis to plot on.
                                            Could be "first ", "second " or
                                            "graph ".
            ytext           str             Identifies which Y axis to plot on.
            colour          str             String identifying a colour.  This
                                            could be a name that gnuplot
                                            recognizes or a hex RGB value.
            comment         str             A comment to add to the polygon to
                                            identify it to anyone trying to
                                            debug the .plt file.
            plt             handle          The handle of the .plt file.

        Returns:
            None.
    '''
    global poly_num
    poly_num += 1

    line = ('  set object ' + str(poly_num) + ' polygon from'
                + Write2Vertices(xvals[0], yvals[0], xtext, ytext, False) + '  \\\n')

    for x, y in zip(xvals[1:], yvals[1:]):
            line = line + Write2Vertices(x, y, xtext, ytext) + '  \\\n'
    # Last entries are the settings and the comment.  We put the comment
    # above the entry, if there is one (the comment may have already been
    # written above an earlier object).
    if comment != "":
        gen.WriteOut("  # Object below is " + comment, plt)
    line = (line  + ' '* 33 + 'fillcolor rgb "' + colour
             + '" fillstyle solid noborder \\\n'
           )
    # Write out everything except the last "\\\n".
    gen.WriteOut(line[:-2], plt)
    return()


def TranslateChs(chs, mult, div, offset, units):
    '''Take a list of distance values and three things to scale/translate
    them by, converting them from metres to feet if appropriate.  This
    also works for things like elevations in train icons.

        Parameters:
            chs             [float]         A list of distances.
            mult            float           A multiplier to apply to the
                                            distances.
            div             float           A divisor to apply to the distances.
            offset          float           An offset to add to the distances.
            units           str             "si" or "us"  If "us, the distances
                                            are converted into feet before
                                            applying the multipliers etc.

        Returns:
            chs             [float]         A modified list of distances.
    '''
    if units == "us":
        # Convert the distances into feet and apply the offsets and multipliers.
        chs = [ch / 0.3048  * mult/div + offset for ch in chs]
    else:
        # Just apply all the offsets and multipliers.
        chs = [ch * mult/div + offset for ch in chs]
    return(chs)


def BuildJetFanVertices(details, base, height, length, aspect, colour,
                        flat, debug1):
    ''' Take a pair of chainages and elevations, a base uplift and a height
    and turn them into a set of vertices for a polygon that represents
    a jet fan and an arrow that represents a flow direction.

        Parameters:
            details         []              A list of things to define the
                                            location of the jet fan, any
                                            labels attached to it and comments
                                            in the gnuplot file.
            base            float           An offset to uplift the jet fan by.
            height          float           A distance that controls the height
                                            or diameter of the jet fan icon.
            length          float           The distance that controls the
                                            length of the jet fan icon.
            aspect          float           An aspect ratio that controls how
                                            the jet fan is rotated.  The user
                                            can change so that rotated jet fans
                                            have square ends.
            colour          str             The name or RGB number of the
                                            colour to use for the jet fan icon.
            flat            bool            If True, the jet fans won't be
                                            rotated.
            debug1          bool            The debug Boolean set by the user.

        Returns:
            points          []              A list of polygon coordinates,
                                            polygon colours, polygon numbers
                                            and gnuplot command comments.
            ""              str             An empty string, will be a comment
                                            later.
    '''
    (chs, elevs, comment) = details

    # Figure out where to place the jet fan at (the back end of the
    # subsegment).  xbase and ybase are the setting-out point for the
    # jet fan.  This is the half-way along the length of the jet fan
    # and at the soffit that the jet fan hangs from.
    # print("details", chs, elevs, comment)

    xbase = chs[0]
    ybase = elevs[0]
    gradient = (elevs[1] - ybase) / (chs[1] - xbase)

    # Figure out how far up to push the jet fan up.
    ybase += base

    # Figure out if we need to draw an arrow to represent the direction of
    # air flow through the jet fan.
    if "N jet fan blowing" in comment:
        draw_arrow = True
        if "blowing up" in comment:
            direction  = -1.
        else:
            direction  = +1.
    else:
        draw_arrow = False


    # Define the shape of a jet fan.  The setting-out point is the
    # half-way along the jet fan on the underside.  When we rotate the
    # jet fan we rotate it around that point.
    # We divide the length by eight so that the jet fans match the
    # actual distance.
    length = length / 8.

    # This polygon is the filled-in shape of the jet fan.  The values
    # used here
    x1_case = [-4 * length, -length, -length, +length, +length, +4* length,
               +4 * length, +length, +length, -length, -length, -4* length,
               -4* length]
    y1_case = [-0.02 * height, -0.02 * height, -0.1 * height,
               -0.1 * height,  -0.02 * height, -0.02 * height,
               -0.46 * height, -0.46 * height, -0.38 * height,
               -0.38 * height, -0.46 * height, -0.46 * height,
               -0.02 * height]

    # Rotate the jet fan to the angle of the segment.  Figure out the
    # cosine and sine of the rotation angle.  We are rotating around
    # the origin so no need to include offsets.

    (x_rot, y_rot) = RotatePoints(x1_case, y1_case, gradient, aspect, flat)

    # Now offset the points to the correct location on the vertical profile.
    x1_case = [x + xbase for x in x_rot]
    y1_case = [y + ybase for y in y_rot]

    # Store the points that outline the jet fan with a colour and a comment.
    points = ((x1_case, y1_case, colour, comment), )

    # If the jet fan is on, draw a suitable arrow.
    if draw_arrow is True:
        x2_arrow = (0.0, 9 * direction * length)
        y2_arrow = (-0.24 * height, -0.23 * height)
        (x_rot, y_rot) = RotatePoints(x2_arrow, y2_arrow, gradient, aspect, flat)
        x2_arrow = [x + xbase for x in x_rot]
        y2_arrow = [y + ybase for y in y_rot]
        lines = [(x2_arrow, y2_arrow, " head", colour, "")]
    else:
        lines = []
    return(points, lines)


def RotatePoints(x_orig, y_orig, gradient, aspect, flat):
    '''Take a pair of lists of X and Y values, a gradient and an aspect
    ratio and rotate the points around the origin.  The aspect ratio is
    useful when building shapes like jet fans, where we want the rotated
    points to build correct shapes in stretched co-ordinate systems.  We
    can change the aspect ratio by an optional argument to make it look
    like the ends of jet fan silencers are not angled after rotating
    them.

        Parameters:
            x_orig          []              A list of X coordinates.
            y_orig          []              A list of Y coordinates.
            gradient        float           The incline to twist to, from
                                            horizontal (fraction, 0 to 1).
            aspect          float           An aspect ratio that controls how
                                            the icon is rotated.  The user
                                            can change so that rotated jet fans
                                            have square ends.
            flat            bool            If True, the jet fans won't be
                                            rotated.

        Returns:
            x_rot           []              A list of X coordinates.
            y_rot           []              A list of Y coordinates.
    '''
    if flat:
        x_rot, y_rot = x_orig, y_orig
    else:
        angle = 2 * math.sin(gradient / 2)
        cos_fac = math.cos(angle)
        x_sin_fac = math.sin(angle)
        y_sin_fac = x_sin_fac * aspect

        # This 'for' loop is clear, but slow.
        # x_rot = []
        # y_rot = []
        # for index, x in enumerate(x_orig):
        #     y = y_orig[index]
        #     x_rot.append(   cos_fac * x + y_sin_fac * y)
        #     y_rot.append(-x_sin_fac * x +   cos_fac * y)

        # This list comprehension is fast, but gobbledegook.
        x_rot, y_rot = zip(*[ (  cos_fac * x + y_sin_fac * y,
                               x_sin_fac * x -   cos_fac * y) for
                             x, y in zip(x_orig, y_orig)]
                          )
    return(x_rot, y_rot)


def BuildFireVertices(details, base, height, length, flat, units, debug1):
    ''' Take a pair of chainages and elevations, a base uplift and a height
    and turn them into a set of vertices for two polygons that represent
    a fire.  Use the air velocity to decide whether to show the flames
    burning up or bent over to one side or the other.  Optionally include
    a label showing the cold air velocity upwind of the fire.

        Parameters:
            details         []              A list of things to define the
                                            location of the fire, any labels
                                            attached to it and comments in the
                                            gnuplot file.
            base            float           An offset to uplift the fire by.
            height          float           The height of the fire icon.
            units           str             "si" or "us"  If "us, the distances
                                            are converted into feet before
                                            applying the multipliers etc.
            debug1          bool            The debug Boolean set by the user.

        Returns:
            points          []              A list of polygon coordinates,
                                            polygon colours, polygon numbers
                                            and gnuplot command comments.
            ""              str             An empty string, will be a comment
                                            later.
    '''
    (chs, elevs, comment, reverse, u_ann, u_open, operating) = details
    # Figure out where to start the fire from.  This is the midpoint
    # of the fire's extents.
    xbase = sum(chs)/2
    if flat:
        ybase = base
    else:
        ybase = sum(elevs)/2 + base
    # Set a suitable width.  Set a positive or negative value to orient
    # which way the tips of the flames.
    width = 0.3 * length / 90

    # We turn the input height (which is in metres) into something that scales
    # for the lists of numbers below.
    if operating:
        # Use the full height
        height = 0.007 * height
    else:
        # The fire is not burning, make an icon one-tenth the size.
        height = 0.0007 * height

    # Create lists to hold the data.  The first icon is a yellow polygon
    # drawing four flames.  The second is a smaller red polygon that sits
    # inside the yellow one.  There are two different pairs of polygons:
    # flames pointing to one side, and flames pointing up.

    if abs(u_ann) <= 0.5:
        # The air velocity is so slow that we'll show the flames going
        # straight up.
        # Co-ordinates for a large yellow fire polygon with flame tips
        # pointing up.
        x2_up1 = (xbase,                xbase - width  *  70,
                  xbase - width  * 110, xbase - width  * 140,
                  xbase - width  * 150, xbase - width  * 160,
                  xbase - width  * 130, xbase - width  * 100,
                  xbase - width  *  80, xbase - width  *  85,
                  xbase - width  *  75, xbase - width  *  30,
                  xbase - width  *  15, xbase - width  *   0,
                  xbase + width  *  40, xbase + width  *  60,
                  xbase + width  *  70, xbase + width  *  60,
                  xbase + width  *  40, xbase + width  *  80,
                  xbase + width  * 120, xbase + width  * 135,
                  xbase + width  * 145, xbase + width  * 130,
                  xbase + width  * 100, xbase + width  *  70,
                  xbase)
        y2_up1 = (ybase,                ybase + height * 30,
                  ybase + height *  60, ybase + height * 100,
                  ybase + height * 130, ybase + height * 200,
                  ybase + height * 160, ybase + height * 110,
                  ybase + height * 200, ybase + height * 260,
                  ybase + height * 240, ybase + height * 130,
                  ybase + height * 100, ybase + height * 130,
                  ybase + height * 200, ybase + height * 255,
                  ybase + height * 210, ybase + height * 160,
                  ybase + height * 100, ybase + height * 130,
                  ybase + height * 180, ybase + height * 220,
                  ybase + height * 180, ybase + height * 120,
                  ybase + height *  70, ybase + height *  30,
                  ybase)

        # Co-ordinates for a small red fire polygon with flame tips
        # pointing up.
        x2_up2 = (xbase - width  *  10, xbase - width  *  40,
                  xbase - width  *  90, xbase - width  * 120,
                  xbase - width  * 130, xbase - width  * 145,
                  xbase - width  * 105, xbase - width  *  60,
                  xbase - width  *  65, xbase - width  *  70,
                  xbase - width  *  20, xbase + width  *  10,
                  xbase + width  *  50, xbase + width  *  20,
                  xbase + width  *   5, xbase + width  *  50,
                  xbase + width  *  80, xbase + width  * 115,
                  xbase + width  *  90, xbase + width  *  40,
                  xbase + width  *  10, xbase - width  *  10)
        y2_up2 = (ybase, ybase + height * 15,
                  ybase + height *  70, ybase + height * 100,
                  ybase + height * 120, ybase + height * 160,
                  ybase + height * 100, ybase + height *  70,
                  ybase + height * 110, ybase + height * 170,
                  ybase + height *  70, ybase + height * 110,
                  ybase + height * 190, ybase + height * 110,
                  ybase + height *  60, ybase + height *  80,
                  ybase + height * 100, ybase + height * 150,
                  ybase + height * 100, ybase + height *  20,
                  ybase, ybase)
        points = ((x2_up1, y2_up1, "#FFD700", comment),
                  (x2_up2, y2_up2, "#FF4000", comment))
    else:
        # The flames should be pointed downwind, whichever direction
        # that is.
        if (not reverse and u_ann > 0.5) or (reverse and u_ann < -0.5):
            # The flames should be pointed in the up direction, not
            # the down direction.
            width = -width

        # Co-ordinates for a large yellow fire polygon with flame tips
        # pointing to one side.
        x1_side1 = (xbase,                xbase - width  *  70,
                    xbase - width  * 110, xbase - width  * 140,
                    xbase - width  * 150, xbase - width  * 220,
                    xbase - width  * 120, xbase - width  *  80,
                    xbase - width  *  80, xbase - width  * 130,
                    xbase - width  *  55, xbase - width  *  30,
                    xbase - width  *  15, xbase - width  *   5,
                    xbase - width  *  45, xbase + width  *  10,
                    xbase + width  *   5, xbase + width  *  30,
                    xbase + width  *  75, xbase + width  *  85,
                    xbase + width  *  85, xbase + width  *  70,
                    xbase + width  *  50, xbase)
        y1_side1 = (ybase,                ybase + height *  30,
                    ybase + height *  60, ybase + height * 100,
                    ybase + height * 130, ybase + height * 160,
                    ybase + height * 150, ybase + height * 110,
                    ybase + height * 150, ybase + height * 165,
                    ybase + height * 160, ybase + height * 130,
                    ybase + height * 100, ybase + height * 140,
                    ybase + height * 155, ybase + height * 150,
                    ybase + height * 155, ybase + height * 155,
                    ybase + height * 150, ybase + height * 140,
                    ybase + height * 120, ybase + height *  70,
                    ybase + height *  20, ybase)

        # Co-ordinates for a small red fire polygon with flame tips
        # pointing to one side.
        x1_side2 = (xbase - width  *  10, xbase - width  *  40,
                    xbase - width  *  90, xbase - width  * 110,
                    xbase - width  * 120, xbase - width  * 145,
                    xbase - width  * 185, xbase - width  * 135,
                    xbase - width  *  85, xbase - width  *  60,
                    xbase - width  *  65, xbase - width  *  70,
                    xbase - width  * 100, xbase - width  *  60,
                    xbase - width  *  20, xbase + width  *   5,
                    xbase + width  *  10, xbase - width  *  20,
                    xbase + width  *  20, xbase + width  *  25,
                    xbase + width  *  20, xbase + width  *  20,
                    xbase + width  *  30, xbase + width  *  60,
                    xbase + width  *  70, xbase + width  *  20,
                    xbase + width  *  75, xbase + width  *  80,
                    xbase + width  *  70, xbase + width  *  40,
                    xbase + width  *  10, xbase - width  *  10)
        y1_side2 = (ybase,                ybase + height *  15,
                    ybase + height *  70, ybase + height * 100,
                    ybase + height * 120, ybase + height * 140,
                    ybase + height * 150, ybase + height * 145,
                    ybase + height * 100, ybase + height *  70,
                    ybase + height * 110, ybase + height * 150,
                    ybase + height * 160, ybase + height * 155,
                    ybase + height *  70, ybase + height * 110,
                    ybase + height * 140, ybase + height * 150,
                    ybase + height * 145, ybase + height * 135,
                    ybase + height * 110, ybase + height *  70,
                    ybase + height *  80, ybase + height * 100,
                    ybase + height * 130, ybase + height * 150,
                    ybase + height * 140, ybase + height * 130,
                    ybase + height * 100, ybase + height *  20,
                    ybase, ybase)
        points = ((x1_side1, y1_side1, "#FFD700", comment),
                  (x1_side2, y1_side2, "#FF4000", comment))
    return((points, ""))


def BuildTrainVertices(chs, elevs, downclip, upclip, base, height, colour,
                       comment, units, debug1):
    ''' Take a list of chainages and elevations, a base uplift and a height
    and turn them into a set of vertices that represent a train.  If downclip
    is True, chamfer the down end of the train to crudely represent a nose
    If upclip is True, chamfer the down end of the train to crudely represent
    a tail.

        Parameters:
            chs             []              A list of chainages.  It may
                                            have two points (back end of the
                                            train and forward end of it), or
                                            it may have points between there
                                            at which a change of gradient
                                            occurs.
            elevs           []              A list of elevations, one for
                                            each chainage in 'chs'.
            downclip        bool            If true, don't chamfer the down
                                            end of the train polygon.
            upclip          bool            If true, don't chamfer the up
                                            end of the train polygon.
            base            float           An offset to uplift the train by.
            height          float           The height of the train icon.
            colour          str             The name or RGB number of the
                                            colour to use for the train icon.
            comment         str             Comment to append to gnuplot's "set
                                            object polygon" command.  It gives
                                            the train number that this polygon
                                            represents, for debugging .plt
                                            files.
            units           str             "si" or "us"  If "us", the distances
                                            are converted into feet before
                                            applying the multipliers etc.
            debug1          bool            The debug Boolean set by the user.

        Returns:
            points          []              A list of polygon coordinates,
                                            polygon colours, polygon numbers
                                            and gnuplot command comments.
    '''
    if debug1:
        print("train vertex chainages", chs)

    # Create lists to hold the data.
    xvals = []
    yvals = []

    # Draw the base of the train and figure out what indices to use on the
    # roof (some points may be so close to the nose or tail that we don't
    # want to draw them when we do the angled ends at the nose and tail).

    # We chamfer the train end back within 6 m of the nose, unless it looks
    # weird on very short trains.
    ch_nose =  chs[-1]
    ch_tail = chs[0]
    if units == "si":
        back = min(6.0, abs(ch_tail - ch_nose)/8)
    else:
        back = min(19.7, abs(ch_tail - ch_nose)/8)

    # Build the chainages we need for the chamfered train ends.  Variable
    # names ending in '2' are on the roof and those ending in '3' are on the
    # undercarriage.
    if ch_tail < ch_nose:
        ch_tail2 = ch_tail + back
        ch_nose2 = ch_nose - back
        ch_tail3 = ch_tail + 0.15 * back
        ch_nose3 = ch_nose - 0.15 * back
        increasing = True
    else:
        ch_tail2 = ch_tail - back
        ch_nose2 = ch_nose + back
        ch_tail3 = ch_tail - 0.15 * back
        ch_nose3 = ch_nose + 0.15 * back
        increasing = False
    if debug1:
        print("nose, top to base", ch_nose2, ch_nose, ch_nose3)
        print("tail, top to base", ch_tail2, ch_tail, ch_tail3, increasing)
    floor_start = 0
    floor_end = len(chs) - 1
    roof_start = 0
    roof_end = len(chs) - 1

    # Set the shape of the endcaps.  Small angle to a quarter of the way up
    # then a larger angle to the roof level.
    top = base + height
    mid = base + 0.25 * height


    # Figure out which points in "chs" need to be plotted.  Exclude those
    # that are so close to the nose or tail that they are within the chamfer.
    for index, ch in enumerate(chs):
        if increasing:
            if ch < ch_tail2:
                # Once we go above ch_tail2 we stop updating this.
                roof_start = index
            if ch < ch_nose2:
                roof_end = index + 1
            if ch < ch_tail3:
                floor_start = index
            if ch < ch_nose3:
                floor_end = index + 1
        else:
            # The trains are going up the route instead of down it.
            if ch > ch_tail2:
                roof_start = index
            if ch > ch_nose2:
                roof_end = index + 1
            if ch > ch_tail3:
                floor_start = index
            if ch > ch_nose3:
                floor_end = index + 1

    if debug1:
        print("indices", roof_start, roof_end, floor_start, floor_end)
        if not(chs[roof_start] <= ch_tail2 <= chs[roof_start+1]):
            print("dud roof_start, ", chs[roof_start], ch_tail2, chs[roof_start+1])
        if not(chs[roof_end-1] <= ch_nose2 <= chs[roof_end]):
            print("dud roof_end, ", chs[roof_end-1], ch_nose2,  chs[roof_end])
        if not(chs[floor_start] <= ch_tail3 <= chs[floor_start+1]):
            print("dud floor_start, ", chs[floor_start], ch_tail3, chs[floor_start+1])
        if not(chs[floor_end-1] <= ch_nose3 <= chs[floor_end]):
            print("dud floor_end, ", chs[floor_end-1], ch_nose3,  chs[floor_end])
        # raise()

    # Build the points in the base of the train, starting near the tail.
    for index in range(floor_start + 1, floor_end):
        ch = chs[index]
        elev = elevs[index]
        xvals.append(ch)
        yvals.append(elev + base)

    # Now build the points at the down end (the nose).
    if downclip:
        # The train is partway out of the tunnel on the train stacks,
        # so the nose is not shown.  Just put in a vertical line from
        # floor to roof at the exit portal.
        xvals.append(ch_nose)
        yvals.append(elevs[-1] + base)
        xvals.append(ch_nose)
        yvals.append(elevs[-1] + top)
    else:
        # Find the Y-value at the base of the chamfered endcap.  We
        # use gen.Interp instead of np.interp because Interp has better
        # error messages when we foul up and accidentally extrapolate.
        # Once we're sure the code is solid, we can switch.
        if debug1:
            print("interp 1")
        yval3 = gen.Interpolate(chs[floor_end - 1], chs[floor_end],
                               elevs[floor_end - 1], elevs[floor_end],
                               ch_nose3, False)
        xvals.append(ch_nose3)
        yvals.append(yval3 + base)
        xvals.append(ch_nose)
        yvals.append(elevs[-1] + mid)
        # Find the Y-value at the top of the chamfered endcap.
        if debug1:
            print("interp 2", ch_nose2, roof_end)
        yval2 = gen.Interpolate(chs[roof_end - 1], chs[roof_end],
                               elevs[roof_end - 1], elevs[roof_end],
                               ch_nose2, False)
        xvals.append(ch_nose2)
        yvals.append(yval2 + top)

    # Build the points in the roof of the train, if any.
    if len(chs) > 2:
        for index in range(roof_end - 1, roof_start, -1):
            if debug1:
                print("index down", index, chs[index])
            xval = chs[index]
            xvals.append(chs[index])
            yvals.append(elevs[index] + top)


    # Build the endcap at the tail.
    if upclip:
        # The train is partway into the tunnel on the train stacks,
        # so the tail is not shown.  Just put in a vertical line from
        # roof to floor at the entry portal.
        xvals.append(ch_tail)
        yvals.append(elevs[0] + top)
        xvals.append(ch_tail)
        yvals.append(elevs[0] + base)
    else:
        xvals.append(ch_tail2)
        # Find the Y-value at the top of the chamfered endcap.
        if debug1:
            print("interp 3")
        yval2 = gen.Interpolate(chs[roof_start], chs[roof_start + 1],
                               elevs[roof_start], elevs[roof_start + 1],
                               xvals[-1], False)
        yvals.append(yval2 + top)
        xvals.append(ch_tail)
        yvals.append(elevs[0] + mid)
        xvals.append(ch_tail3)
        # Find the Y-value at the base of the chamfered endcap.
        if debug1:
            print("interp 4", ch_tail3, chs[floor_start], chs[floor_start + 1])
        yval3 = gen.Interpolate(chs[floor_start], chs[floor_start + 1],
                               elevs[floor_start], elevs[floor_start + 1],
                               xvals[-1], False)
        yvals.append(yval3 + base)

    # Close the curve so we don't get complaints from gnuplot.
    xvals.append(xvals[0])
    yvals.append(yvals[0])

    # We only have one polygon to represent a train, so we put it in
    # a tuple with one value (the routine calling this one can also
    # call routines that generate multiple polygons, like fires.
    polygon = ((xvals, yvals, colour, comment),)
    return(polygon)


def Write2Vertices(x1, y1, xtext, ytext, include_to = True):
    '''Take two values on the X Y plane and build gnuplot text that
    draws a line to that point.  If the Boolean "include_to" is True
    then the text has "to " prepended to it.

        Parameters:
            x1              float           X co-ordinate to print to the
                                            gnuplot input file.
            y1              float           Y co-ordinate to print to the
                                            gnuplot input file.
            xtext           str             Padding text to format the line
                                            in the gnuplot file and tell gnuplot
                                            which reference frame to plot on
                                            ("first", "second" or "screen").
            ytext           str             Text defining which reference frame
                                            to plot on ("first", "second" or
                                            "screen").
            include_to      bool            If False, this is the first vertex.
                                            If True, this is a later vertex and
                                            a "to " needs to be prepended to it.

        Returns:
            text            str             A line of text to add to the "set
                                            object polygon" command.
    '''
    if include_to:
        text = ' ' * 33 + 'to '
    else:
        text = ' '

    text = (text + xtext + gen.FloatText(x1) + ', '
                 + ytext + gen.FloatText(y1))
    return(text)


def PageQA1(settings_dict, sources_list, plt):
    '''Take the settings dictionary and a list of data sources and prepare
    a list of lines that generate gnuplot labels that are QA for a page of
    data.  These labels are added when the last graph is plotted, then
    removed.  The lines are written to the .plt file in this routine.

        Parameters:
            settings_dict   {}              Dictionary of the run settings.
            sources_list    []              List of the names of data sources
            plt             handle          The handle of the gnuplot file.

        Returns:
            None
    '''
    # Break out the various settings we will need here.
    file_name = settings_dict["file_name"]
    file_stem = settings_dict["file_stem"]
    page_num = settings_dict["page_num"]
    when_who = settings_dict["when_who"]
    qa1 = settings_dict["qa1"]
    qa2 = settings_dict["qa2"]
    qa3 = settings_dict["qa3"]
    header = settings_dict["header"]
    footer = settings_dict["footer"]

    # Set the default locations of the four strings.  These are fractions
    # of page width and height.  Eventually we will write code to cram
    # these next to the graphs and set them in the input files.
    # We set two locations (baseleft and baseright) for the Y-fraction
    # of the footer because the left footer could be long enough to
    # overlap the right footer.  If they are slightly staggered there
    # is no clash.  Note that these are set to work well for A4 landscape
    # pages.
    left = 0.08
    right = 0.92
    baseleft = 0.05
    baseright = 0.06
    if header == "underfoot":
        # Put the header line below the footer.  This is useful when
        # building timeloops that can be used as flipbook presentations,
        # because the viewer's attention isn't drawn to the small text
        # at the top of the page.  It also lets you put a big header line
        # at the top of the page to drive home what the flipbook is about.
        top = 0.025
    else:
        # Put the header at the top of the page.
        top = 0.94

    # Generate QA data in for headers and footers on the page.  The list
    # of source files was created by scanning the page to see which files
    # were used.


    # Do the headers.  A note on the icon numbers: gnuplot seems to have
    # a limit on label/icon numbers of 2^31-1 = 2,147,483,647 (MAX INT for
    # a four byte integer).  We use 2,000,000,000 to 2,147,483,642 for
    # internal icon/arrow/label numbering and leave everything below two
    # billion for the user in their verbatim blocks, if they want to number
    # things.
    if header in ("on", "underfoot"):
        # Remove the '#' that is in the default entry.  If the default entry
        # was modified, this replace command does nothing because there is
        # no way for a modified entry to contain '#'.
        # The user set a project name (qa2)
        mod_label1 = qa1.replace('#', '')
        mod_label2 = qa2.replace('#', '')
        QA_lt = ('  set label 2147483643 "{/*0.5 Project no: '
                 + SanitiseEnhancedString(mod_label1) + '    Project name: '
                 + SanitiseEnhancedString(mod_label2) + '}" at screen '
                 + str(left) + ', ' + str(top) + ' left')
        gen.WriteOut(QA_lt, plt)

        QA_rt = ('  set label 2147483644 "{/*0.5 Created by Hobyah.py from '
                 + SanitiseEnhancedString(file_name) + ', ' + SanitiseEnhancedString(when_who)
                 + '}" at screen ' + str(right) + ', ' + str(top) + ' right')
        gen.WriteOut(QA_rt, plt)


    if footer == "on":
        if sources_list == []:
            # This page had no graphs on it and no images that were tagged
            # by optional arguments that their filenames were added to the
            # list of sources.
            QA_lb = ('  set label 2147483645 "{/*0.5 No source files}" at screen '
                      + str(left) + ', ' + str(baseleft) + ' left')
        else:
            sources_list.sort()
            san_sources = [SanitiseEnhancedString(source) for source in sources_list]
            file_names = gen.FormatOnLines(san_sources).replace('\n', '').replace('>   ', '')
            add = gen.Plural(len(sources_list))

            QA_lb = ('  set label 2147483646 "{/*0.5 Source file' + add + ': '
                       +  file_names + '}" at screen ' + str(left) + ', ' + str(baseleft) + ' left')
        gen.WriteOut(QA_lb, plt)

        QA_rb = ('  set label 2147483647 "{/*0.5 Page ' + str(page_num) + '}" '
                    'at screen ' + str(right) + ', ' + str(baseright) + ' right')
        gen.WriteOut(QA_rb, plt)
    return()


def PickUserData(data, col_name, axis, nickname, settings_dict,
                 line_number, line_text, log):
    '''Take a block of data and a column name or number and get the data
    in that column.  Return the column of data and a list of QA lines to
    be written to the text file that holds the data.

        Parameters:
            data            {}              A dictionary of columns of data
                                            indexed by a key.
            col_name        str             The column number (starting at 1,
                                            not zero) or the name at the top
                                            of the column.
            axis            str             "X axis or "Y axis".  Used to
                                            decide which QA data to return.
            nickname        str             The nickname given to this block
                                            of data in the input file, for QA
                                            purposes.
            settings_dict   {}              Dictionary of the run settings.
            line_number     int             The line number being processed.
            line_text       str             The text of the line being processed.
            log             handle          The handle of the logfile.

        Returns:
            A list of data in the column, an autokey, a list of lines of
            QA text (printed to the curve data file), the column number
            and a string naming the data for use in the footer.

        Errors:
            Aborts with 6141 if a column number was given but it is below 1
            Aborts with 6142 if a column number was given but it is higher than
            the count of columns in the block of data.
            Aborts with 6143 if a column name was given and it was not
            a valid column name in that block of data.
    '''
    file_name = settings_dict["file_name"]
    # Get the titles of the columns to use.
    titles = list(data.keys())
    if "#name" in titles:
        # Take the name off and make a note that this block of data is from a
        # .csv block.
        titles.pop(titles.index("#name"))
        csv_file = True
    else:
        csv_file = False

    # Get the name of the source of the data.
    if csv_file:
        data_name = data["#name"]
    else:
        data_name = "datasource " + nickname

    # Now generate a list of QA data about the curve.  If we are processing
    # the column of X axis data we put in a lot of header data.  If we
    # are processing the Y axis data we skip this, because we already
    # have it.
    col_header = []
    if axis == "X axis":
        if csv_file:
            QAlist = ['# Hobyah .csv data QA',
                      '# Name of source .csv file, ' + data_name,
                      '# Nickname of source .csv file: ' + nickname,]
            auto_key = data_name + ": "
        else:
            QAlist = ['# Hobyah begin data...end data QA',
                      '#',
                      '# Nickname of source datasource: ' + nickname,]
            auto_key = "datasource " + nickname + ": "
    else:
        # Spoof an empty list when we process the Y axis (we append to it
        # below).
        QAlist = []
        auto_key = "and "

    # First check if there is a number for the column instead of
    # a nickname.
    try:
        col_num = int(col_name)
        # The user has set a number for the column instead of a name.
        # Check if the number is above zero
        # Check if the number is small enough to match one of the titles.
        if col_num < 1:
            maxcols = str(len(titles))
            if nickname[:4] == "csv_":
                # Take off the prefix.
                nickname = nickname[4:]
            err = ('> A curve definition for the user-defined data\n'
                   '> "' + nickname + '" in "' + file_name + '" was \n'
                   '> given a number (' + col_name + ') for the column to use\n'
                   '> for the ' + axis + ' but the number is below 1.\n'
                   '> The block of data has ' + str(len(titles)) + ' columns of data,\n'
                   '> so a number between 1 and ' + str(len(titles)) + ' is needed.\n'
                   '> Please edit the file to correct it.\n'
                   )
            gen.WriteError(6141, err, log)
            gen.ErrorOnLine(line_number, line_text, log, False)
            return(None)
        elif col_num > (len(titles) - 1):
            if nickname[:4] == "csv_":
                # Take off the prefix.
                nickname = nickname[4:]
            err = ('> A curve definition for the user-defined data\n'
                   '> "' + nickname + '" in "' + file_name + '" was \n'
                   '> given a number (' + col_name + ') for the column to use\n'
                   '> for the ' + axis + ' but it is too high as there are\n'
                   '> only ' + str(len(titles) - 1) + ' columns in that block.\n'
                   '> Please edit the file to correct it.\n'
                   )
            gen.WriteError(6142, err, log)
            gen.ErrorOnLine(line_number, line_text, log, False)
            return(None)
        else:
            # Replace the number with the name of the column (this works
            # even if there were no header lines).
            column = titles[col_num - 1]
        if str(col_num) == column:
            # The user did not set any nicknames in the block of data.
            QAlist.append('# Column used for the ' + axis + ' was the '
                           + gen.Enth(col_num) + '.  It had no nickname.')
            auto_key = auto_key + gen.Enth(col_num) + " column "
            col_header.append(gen.Enth(col_num) + " column")
        else:
            QAlist.append('# Column used for the ' + axis + ' was the '
                           + gen.Enth(col_num) + ', nicknamed "'
                           + column + '".')
            auto_key = auto_key + gen.Enth(col_num) + ' column (' + column + ') '
            col_header.append(gen.Enth(col_num) + ' column (' + column + ')')
    except ValueError:
        # It's not an integer, it's a nickname.
        if col_name.lower() not in titles:
            # It's not recognized.
            err = ('> A curve definition for the user-defined data\n'
                   '> "' + nickname + '" in "' + file_name + '" was \n'
                   '> given the column name "' + col_name + '" to use for\n'
                   '> the ' + axis + ' but no column with that name was in\n'
                   '> that particular block.  Please edit the file to\n'
                   '> correct it.  For what it is worth the data block\n'
                   '> contained the following column names:\n'
                    + gen.FormatOnLines(titles)
                    )
            gen.WriteError(6143, err, log)
            gen.ErrorOnLine(line_number, line_text, log, False)
            return(None)
        else:
            column = col_name.lower()
            # Figure out which column number this is.
            col_num = titles.index(col_name.lower()) + 1
            QAlist.append('# Column used for the ' + axis + ' was the '
                          + gen.Enth(col_num) + ', nicknamed "'
                          + col_name.lower() + '".')
            auto_key = auto_key + gen.Enth(col_num) + ' column (' + column + ') '
            col_header.append(gen.Enth(col_num) + ' column (' + column + ')')

    return(data[column], QAlist, auto_key, col_header, col_num, data_name)


def SubstituteAt(where, prog_type, trans_curve, settings_dict,
                 line_number, line_text, log):
    '''Check if a word has an "@" symbol in the middle of it.  Return
    the word before the "@" and the number after it (constants may be
    used for the latter and are replaced in this routine).  If there is
    no "@", fault if the program type is Hobyah.

        Parameters:
            where           str             The string to process into a
                                            location.
            prog_type       str             "Hobyah", "SES 4.10" etc.
            trans_curve     bool            True if the property is transient,
                                            False if it is not.
            settings_dict   {}              Dictionary of the run settings.
            line_number     int             The line number being processed.
            line_text       str             The text of the line being processed.
            log             handle          The handle of the logfile.

        Returns:
            A copy of "where" with any constants replaced by the appropriate
            number from the constants block.

        Errors:
            Aborts with 6121 if the location starts with an "@".
            Aborts with 6122 if the location ends with an "@".
            Aborts with 6123 if the location contained more than one "@".
            Aborts with 6124 if the file being used was transient, came from
            Hobyah and the location did not contain an "@" symbol.
    '''
    file_name = settings_dict["file_name"]

    if where[0] == "@":
        err = ('> Tried to plot a curve in "' + file_name + '",\n'
               '> but found that the location identifier "' + where + '"\n'
               '> started with the symbol "@", meaning that no entity\n'
               '> to plot at (tunnel, route, train etc.) was given.\n'
               '> Please edit the file to correct it.\n'
               )
        gen.WriteError(6121, err, log)
        gen.ErrorOnLine(line_number, line_text, log, False)
        return(None)
    elif where[-1] == "@":
        err = ('> Tried to plot a curve in "' + file_name + '",\n'
               '> but found that the location identifier "' + where + '"\n'
               '> ended with the symbol "@", meaning that the second\n'
               '> part of the identifier (distance along an entity or\n'
               '> time to plot at) was not given.\n'
               '> Please edit the file to correct it.\n'
               )
        gen.WriteError(6122, err, log)
        gen.ErrorOnLine(line_number, line_text, log, False)
        return(None)

    parts = where.split(sep = "@")
    if len(parts) >= 3:
        # There was more than one instance of "@" in the word.
        err = ('> Tried to plot a curve in "' + file_name + '",\n'
               '> but found that the location identifier "' + where + '"\n'
               '> contained more than one "@" symbol.\n'
               '> Please edit the file to correct it.\n'
               )
        gen.WriteError(6123, err, log)
        gen.ErrorOnLine(line_number, line_text, log, False)
        return(None)
    elif len(parts) == 1:
        # There was no "@" in the word.
        if not trans_curve:
            # Fixed properties don't need to have a time entry in either
            # program, return it unchanged.
            location = where
        elif prog_type[:6] == "Hobyah":
            # Transient location identifiers in Hobyah must have the "@",
            # so we fault.
            err = ('> Tried to plot a curve in "' + file_name + '",\n'
                   '> but found that the location identifier "' + where + '"\n'
                   '> did not contain the symbol "@" to divide the\n'
                   '> entity from the location or time.\n'
                   '> Please edit the file to correct it.\n'
                   )
            gen.WriteError(6124, err, log)
            gen.ErrorOnLine(line_number, line_text, log, False)
            return(None)
        else:
            # Transient location identifiers in SES do not need to have
            # the "@" because they could be section, segment, subsegment
            # or subpoint locations so we let it through unchanged.
            location = where
    else:
        # There are words on either side of the "@".
        (place, dist_time) = parts
        # The first entry can be any one-word name so we don't check it here.
        # The second entry (a distance or a time) can be any number or the
        # name of any constant.  We check it in PROC CheckRangeAndSI and
        # if necessary replace the name of the constant with its number.
        expected = "float any null location identifier"
        # err_lines = ('> Tried to plot a curve in "' + file_name + '",\n'
        #              '> but found that the location identifier "' + dist_time + '"\n'
        #              '> was not a valid number or constant.  Please edit the\n'
        #              '> file to give a valid SES location term.\n'\n'
        result = CheckRangeAndSI(dist_time, expected, True, "",
                                 line_number, line_text,
                                 settings_dict, log)

        if result is None:
            return(None)
        else:
            location = place + "@" + str(result)
    return(location)


def ReadBinData(line_number, line_text, nickname, settings_dict,
                files_dict, log):
    '''Take a nickname, find its binary file and load all its data.
    Check if the binary file is well structured.  Check its version
    is not too old or newer than the program can handle.  Load its data
    and check it is well-structured.
    We do these checks here because this is where we actually use the
    data.  It is loaded elsewhere.

        Parameters:
            line_number     int             The line number being processed.
            line_text       str             The text of the line being processed.
            nickname        str             Nickname of the file we want
                                            to read
            settings_dict   {}              Dictionary of the run settings.
            files_dict      {}              The dictionary of file names
                                            and their nicknames.
            log             handle          The handle of the logfile.

        Returns:
            A class instance holding all the file's data if successful,
            None if not successful.

        Errors:
            Aborts with 6061 if the nickname "calc" is not valid because
            the associated binary file does not exist.
            Aborts with 6062 if the nickname of a different file is not
            valid because that binary file does not exist.
            Aborts with 6063 if the binary file was not of a suitable
            type for plotting from.
            Aborts with 6064 if the nickname was "*name" and a
            suitable replacement nickname was not found in settings_dict.
    '''
    # Break out the various settings we will need here.
    debug1 = settings_dict["debug1"]
    file_name = settings_dict["file_name"]


    if nickname == "*name":
        # This is an entry that the user uses inside a "filesloop"
        # block.  Check for an entry in settings_dict that tells
        # us which nickname to substitute for it.
        try:
            # Try to overwrite it.
            nickname = settings_dict["#*name"][0]
        except KeyError:
            # This is not a line of plot data inside a "filesloop"
            # block (if it was, a KeyError would not have been raised).
            # This can happen when graph definitions are copied out of
            # a "filesloop" block into a "page" or "timeloop" block.
            # Give a helpful error message.
            err = ('> Came across a file nickname that was not valid in\n'
                   '> in the plots block of "' + file_name + '".\n'
                   '> "*name" is a special nickname that can only be\n'
                   '> used inside "filesloops" blocks.  You probably\n'
                   '> copied and pasted the graph from a loop definition\n'
                   '> into a page definition.  Please change all the\n'
                   '> instances of "*name" outside of "filesloop"\n'
                   '> blocks to valid nicknames.  The following are\n'
                   '> the valid nickname(s) in this file:\n'
                  )
            err = err + gen.FormatOnLines(tuple(files_dict.keys()))
            gen.WriteError(6064, err, log)
            gen.ErrorOnLine(line_number, line_text, log, False)
            return(None)
    try:
        pf_full_name = files_dict[nickname][0]
    except KeyError:
        if nickname == "calc":
            # Figure out what caused the binary file to not
            # be recreated.
            runtype = settings_dict["runtype"]
            if runtype == "calc":
                # We must have used the command-line option "-nocalc"
                # for a new binary not to have been created.
                adv = '> Please turn off the "-nocalc" command-line\n' \
                      '> optional argument if you want to plot from\n' \
                      '> it.'
            else:
                adv = '> Please change the "runtype plot" option\n' \
                      '> in the settings block to "runtype calc"\n' \
                      '> to regenerate the binary file.'
            err = ('> Found the invalid nickname "calc" in\n'
                   '> a curve in "' + file_name + '".\n'
                   '> You may have deleted the binary file and\n'
                   '> be running in plot mode instead of in\n'
                   '> calc mode.\n'
                     + adv
                   )
            gen.WriteError(6061, err, log)
            gen.ErrorOnLine(line_number, line_text, log, False)
        else:
            err = ('> Found the invalid nickname "' + nickname + '" in\n'
                   '> a curve in "' + file_name + '".\n'
                   '> Please edit the file to correct it.\n'
                   '> Valid nicknames in this file are as\n'
                   '> follows:\n'
                   )
            err = err + gen.FormatOnLines(tuple(files_dict.keys()))
            gen.WriteError(6062, err, log)
            gen.ErrorOnLine(line_number, line_text, log, False)
        return(None)
    else:
        # Take off the directory string so we can use the file name in
        # error messages.
        (pf_name, pf_dir_name, pf_stem,
               pf_ext) = gen.GetFileData(pf_full_name, "", debug1)
    # If we get to here we have a valid nickname.  We know the file
    # exists and that we have permission to read it because we have
    # already checked in ProcessPlotFiles (errors 2149 & 2150).

    contents = files_dict[nickname][1]
    # Now check if it is a type of binary that can be plotted from.  This
    # is expected to be triggered when new versions of programs are being
    # written and Hobyah has yet to be updated to handle them.
    allowed_progs = ("Hobyah 1", "SES 4.10", "SES 4.2", "SES 4.3",
                    "SES 204.2", "SES 204.3", "SES 204.4", "SES 204.5")

    if contents.prog_type not in allowed_progs:
        err = ('> Found an invalid binary file in "' + file_name + '".\n'
               '> The binary file "' + pf_name + '" is\n'
               '> referenced by the nickname "' + nickname + '" but is\n'
               '> not of a suitable type; it is of type "'
                 + contents.prog_type + '".\n'
               '> The only valid types come from these programs:\n'
                 + gen.FormatOnLines(allowed_progs) + '\n'
               '> Please remove the line or edit it to use a valid\n'
               '> type of binary file.'
               )
        gen.WriteError(6063, err, log)
        gen.ErrorOnLine(line_number, line_text, log, False)
        return(None)
    return(contents)


def WriteVerbatim(line_text, sp_count, plt):
    '''Write a line of data to the .plt file with a given count of
    spaces before it.  This is used to indent lines of verbatim data
    to make it easier to read when debugging the .plt file.

        Parameters:
            line_text       str             The string to be written.
            sp_count        int             The count of spaces to prepend.
            plt             handle          The handle of the gnuplot file.

        Returns:
            None
    '''
    v_words = ' '* sp_count + line_text.lstrip()
    gen.WriteOut(v_words, plt)
    return()


def ProcessGraph(graph_triples, settings_dict, last_doodad,
                 sources_list, files_dict, log, plt):
    '''Read all the data defining a graph.  Once the first plot file
    nickname is encountered, call the curve creation routine to
    generate the lines of gnuplot data and the gnuplot plot command.

        Parameters:
            graph_triples [(int, str, str)] List of lines in the graph.
            settings_dict   {}              Dictionary of the run settings.
            last_doodad     bool            True if this is in the last
                                            graph or image on the page,
                                            False otherwise.
                                            Used to add gnuplot labels for
                                            and/or footer.
            sources_list    []              List of the names of data
                                            sources, used to create the
                                            footer.
            files_dict      {}              The dictionary of file names
                                            and their nicknames.
            log             handle          The handle of the logfile.
            plt             handle          The handle of the gnuplot
                                            file.

        Returns:
            sources_list    []              An updated list of the names of
                                            data sources, with new sources
                                            added.

        Errors:
            Aborts with 6021 if a keyword (first word on the line)
            was not valid.  If the keyword was "set" add the advice
            that they probably forgot to prepend the "verbatim" keyword.
    '''
    # Next we assume that we are plotting in whatever set of units
    # the page is in.  We can change it in the graphs, however.
    page_units = settings_dict["pageunits"]

    # Break out the various settings we will need here.
    file_name = settings_dict["file_name"]
    debug1 = settings_dict["debug1"]
    reserved = settings_dict["reserved"]

    # Set the default system of units to plot in (same as the page units).
    # We do this every time we start processing a new graph, so the setting
    # for "graphunits" does not persist from graph to graph.
    settings_dict.__setitem__("graphunits", page_units)

    # Get the graph commands that we need to read before we read any
    # other commands.
    valid_settings = {"graphunits": (("si", "us"),),
                      "#skip": "discard"  # This catches all other lines
                     }
    # We make a list of entries that we must have (none).
    requireds = []
    # We make a dictionary of the optional keywords that accepts
    # any optional value.
    optionals = {}
    # We don't allow any duplicates.
    duplicates = ()
    settings = (valid_settings, requireds, optionals, duplicates)
    # Now call ProcessBlock.  It returns None if an error occurred.  It
    # updates the settings_block dictionary with the values of any
    # entries in "valid_settings" (if any are set).  This why the
    # argument "settings_dict" appears twice.
    result = ProcessBlock(graph_triples, 0, settings_dict, "graph",
                          settings_dict, settings, log)
    if result is None:
        return(None)
    else:
        (discard, settings_dict) = result
    # Flatten any new settings from tuples (as returned by ProcessBlock)
    # to their values.
    settings_dict = FlattenSettings(settings_dict)

    # Write a message about which units we are plotting in to the
    # dictionary if the graph units and the page units are different.
    graph_units = settings_dict["graphunits"]
    if graph_units != page_units:
        # This graph is being plotted in different units to the ones
        # we plot on this page.  Add a message about it to the .plt
        # file to remind of it if things get messy.
        message = ("Plotting this graph in " + graph_units.upper() +
                   " units, which differ from the page units.")
    else:
        message = ("Plotting this graph in " + graph_units.upper() +
                   ' units.')
    gen.WriteOut("#    " + message, plt)
    if debug1:
        print(message)

    # Now we run through the lines in the graph definition.
    #   margins left_margin right_margin bottom_margin top_margin
    #   title  Air velocity vs time
    #   xlabel  Time (sec)
    #   ylabel  Velocity (m/s)
    #   x2label  Time (sec)
    #   y2label  Velocity (m/s)
    #   xrange      0   140    20   # min, max and interval
    #   yrange     -5    11     1
    #   x2range  -900 13000  1000
    #   y2range     0   120    20
    # The numbers in the axis definition have a special definition:
    # they can be a true number or a number with an asterisk before
    # it.  In gnuplot, the extents of axes are set by commands like
    # "set xrange [0:10]", but gnuplot can be told to autoscale by
    # using an asterisk in place of one of the arguments, e.g.
    # "set xrange [*:10]".  Same goes for the interval value, which
    # can be "set xtics autofreq" or a more complex definition.
    # Prepending an asterisk to a number means "ignore the number
    # and tell gnuplot to autoscale this entry".
    # An example:
    #             xrange      0   140    20
    # is turned into the gnuplot commands
    #             set xrange [0:140]
    #             set xtics 20
    # but
    #             xrange      0   *140    *20
    # is turned into
    #             set xrange [0:*]
    #             set xtics autofreq
    #
    # This way of setting is handy because we don't need to delete
    # the number we already have in order to get gnuplot to use
    # its autoscale.  Gnuplot's axis setting commands are more flexible
    # than this simple input permits but the more flexible uses are
    # not parsed here (too complex). Their full capabilities can
    # be accessed by the use of the "verbatim" keyword or by a
    # "begin verbatim...end verbatim" block in the graph definition:
    #
    #             verbatim set xrange [*<10:50<*]
    #
    # This tells gnuplot to autoscale the axis but constrains the
    # autoscale to keep the lower limit under 10 and the upper limit
    # above 50.

    # Define the keywords and the acceptable entries that follow them.
    valid_settings = {"margins": ("float any null graph left edge",
                                  "float any null graph right edge",
                                  "float any null graph bottom edge",
                                  "float any null graph top edge",),
                      "lmargin": ("float any null graph left edge",),
                      "rmargin": ("float any null graph right edge",),
                      "bmargin": ("float any null graph bottom edge",),
                      "tmargin": ("float any null graph top edge",),
                      "lrmargins": ("float any null graph left edge",
                                    "float any null graph right edge",),
                      "btmargins": ("float any null graph bottom edge",
                                    "float any null graph top edge",),
                      "xlabel": ("QAstr",),
                      "ylabel": ("QAstr",),
                      "x2label": ("QAstr",),
                      "y2label": ("QAstr",),
                      "xrange": ("*float any null X-axis min. value",
                                 "*float any null X-axis max. value",
                                 "*float any null X-axis interval",),
                      "yrange": ("*float any null Y-axis min. value",
                                 "*float any null Y-axis max. value",
                                 "*float any null Y-axis interval",),
                      "x2range": ("*float any null X2-axis min. value",
                                  "*float any null X2-axis max. value",
                                  "*float any null X2-axis interval",),
                      "y2range": ("*float any null Y2-axis min. value",
                                  "*float any null Y2-axis max. value",
                                  "*float any null Y2-axis interval",),
                      "verbatim": ("QAstr",),
                      # "begin verbatim" is valid, "begin sub_testblock"
                      # is a test of recursive calls in the Syntax module
                      # that is convenient to have here.
                      "begin":   (("verbatim", "sub_testblock"),),
                      "#skip": "discard",  # This catches all other lines
                     }
    requireds = []
    # We make a dictionary of the optional keywords that accepts
    # any optional value.
    optionals = {}
    # We allow all duplicates except "sub_testblock".
    duplicates = ("margins", "lmargin", "rmargin", "bmargin", "tmargin",
                  "lrmargins", "btmargins",
                  "xlabel", "ylabel", "x2label", "y2label",
                  "xrange", "yrange", "x2range", "y2range",
                  "begin", "verbatim",
                 )
    settings = (valid_settings, requireds, optionals, duplicates)
    # Now call ProcessBlock.  It returns None if an error occurred.
    result = ProcessBlock(graph_triples, 0 , settings_dict, "graph",
                          {}, settings, log)
    if result is None:
        return(None)
    # If we get to here we know that the lines defining the graph
    # are correctly formed.  We can process them in series without
    # having to (for example) check that the margins command has four
    # numbers after it.  We throw away what ProcessBlock returned
    # since we don't need it.  Note that we didn't check the syntax
    # of things sent directly to gnuplot, such as whatever is after
    # the word "xlabel": gnuplot can check those directly and raise
    # its own error.

    # Create a list of keywords that signal the start of the curve
    # definitions.
    curve_keys = ["transient", "profile", "waterfall",
                  "property", "userdata", "icons", "fandata"]

    # Set a flag that we use for verbatim blocks.
    verbatim_on = False
    if debug1:
        print("Processing graph lines")
    # Set the index directly, because many of the lines will be
    # processed in a subroutine.  Skip the line with "begin graph".
    tr_index = 1
    while tr_index < len(graph_triples):
        (line_number, line_data, line_text) = graph_triples[tr_index]
        if debug1:
            print("Line1",str(line_number) + ":", line_data)
        # Get the optional entries on the line into a dictionary.
        result = gen.GetOptionals(line_number, line_data, line_text,
                 file_name, debug1, log)
        if result is None:
            return(None)
        else:
            (line_data, optionals_dict) = result
        words = line_data.split()
        words_low = line_data.lower().split()

        if words_low[:2] == ["end", "graph"]:
            # We've finished this graph. Return.
            return(sources_list)
        elif words_low[0] in ("title", "xlabel", "ylabel", "x2label", "y2label"):
            # The user has set a graph label.
            # If the first character of the title/label is not ' or " we
            # send it to gnuplot after enclosing it in double quotes and
            # prepending 'set ' to it.
            # If the first character of the title/label is ' or " we
            # assume that it is in correct gnuplot syntax and rely on
            # gnuplot's error messages to let the user know where they
            # made a mistake.
            QAstring = line_data.split(maxsplit = 1)
            if len(QAstring) == 1:
                # The user just had the keyword, meaning we want to unset
                # the entry.
                plt_line = "unset " + QAstring[0] + '       # From line ' \
                           + str(line_number)
            else:
                title = ChooseGnuplotString(QAstring[1])
                plt_line = '  set ' + QAstring[0] + ' ' + title + \
                           '       # From line ' + str(line_number)
            gen.WriteOut(plt_line, plt)
        elif words_low[0] in ("xrange", "yrange", "x2range", "y2range"):
            # These are commands that map to two lines of entry in
            # the gnuplot file.  We have already checked that there
            # are three numbers (or three instance of "*XXX" after
            # the keyword).
            (minval, maxval, stepval) = words[1:]

            # Check for autoscale instructions (first letter is "*"), an
            # allusion to the autoscale character in gnuplot).
            if minval[0] == "*":
                # The minimum value is to be autoscaled
                minval = "*"
            else:
                # Check if the minimum value is the name of a constant.
                minval = CheckForConstant(minval, False, settings_dict)
            if maxval[0] == "*":
                # Ditto
                maxval = "*"
            else:
                maxval = CheckForConstant(maxval, False, settings_dict)
            if stepval[0] == "*":
                # Ditto
                stepval = "autofreq"
            else:
                stepval = CheckForConstant(stepval, False, settings_dict)

            # Build the line for the axis extents
            plt_line = ('  set ' + words[0] + ' [' + minval + ':' + maxval
                        + ']       # From line ' + str(line_number))
            gen.WriteOut(plt_line, plt)

            # Build the line for the tic spacing.
            tics_name = '  set ' + words[0].split(sep = "range")[0] + 'tics '
            plt_line = tics_name + stepval
            gen.WriteOut(plt_line, plt)
        elif words_low[0] == "margins":
            # Build four margin commands from the numbers on the line.
            for index, prefix in enumerate("lrbt", start = 1):
                # First replace any constants with their values.
                word = words_low[index]
                this_margin = CheckForConstant(word, False, settings_dict)
                # Build the words in the command ( (e.g. "set lmargin...").
                title = "  set " + prefix + "margin at screen " + str(this_margin)
                if index == 1:
                    title = title + '       # From line ' + str(line_number)
                gen.WriteOut(title, plt)
        elif words_low[0] in  ("lmargin", "rmargin", "bmargin", "tmargin"):
            # Build one margin command from the line.
            title = "  set " + words_low[0] + " at screen "

            # Now replace any constants.
            word = words_low[1]
            this_margin = CheckForConstant(word, False, settings_dict)
            title = title + str(this_margin) + '       # From line ' + str(line_number)
            gen.WriteOut(title, plt)
        elif words_low[0] in ("lrmargins", "btmargins"):
            # Build two margin commands from the numbers on the line.
            for index, prefix in enumerate(words_low[0][:2], start = 1):
                # Build the words in the command ( (e.g. "set lmargin...").
                title = "  set " + prefix + "margin at screen "

                # Now replace any constants.
                word = words_low[index]
                this_margin = CheckForConstant(word, False, settings_dict)
                title = title + str(this_margin) + '  # ' + str(line_number)
                gen.WriteOut(title, plt)
        elif words_low[0] == "verbatim":
            # Write one line of verbatim data to the file.  First remove
            # the word verbatim from the start:
            rest_text = line_text.split(maxsplit = 1)[1] + '       # From line ' + str(line_number)
            # Now write it out, changing "centre" to "center" if required.
            WriteVerbatim(SwitchCentre(rest_text), 2, plt)
        elif words_low[:2] == ["begin", "verbatim"]:
            # Call a routine that puts a block of verbatim text into the
            # .plt file.  We do not check its contents, we assume it is
            # valid gnuplot commands.  When the routine encounters
            # "end verbatim" it returns with tr_index pointing to after
            # the "end verbatim" line.
            tr_index = ProcessVerbatim(graph_triples, tr_index, debug1, plt)
        elif words_low[0] in curve_keys:
            # If we get to here, we have put in all the graph definition
            # commands and are on to the curve definitions.  We expect
            # all the rest of the line to be definitions of curves to
            # plot.  We call a routine that processes the rest of the
            # graph block and generates the curve data.  If it finds
            # a graph definition command after this first curve definition
            # command it will barf out an error message and stop.
            curve_triples = graph_triples[tr_index:]
            result = ProcessCurves(curve_triples, settings_dict, last_doodad,
                                   sources_list, files_dict, log, plt)
            if result is None:
                return(None)
            else:
                # Get the count of lines we've read and the list of
                # sources of data.
                (count, sources_list) = result
                # We get the value of tr_index for the line
                # before "end graph".  Set tr_index so that this
                # routine reads the line "end graph" next.
                tr_index = tr_index + count - 1
        elif words_low[0] == "graphunits":
            # We've already processed this line and can ignore it here.
            pass
        else:
            # It isn't a recognised keyword.  Get the names of all the
            # recognised keywords.
            keywords = list(valid_settings.keys()) + curve_keys
            keywords.remove("#skip")
            keywords.sort()
            #
            err = ('> Found an invalid keyword in a graph\n'
                   '> block in "' + file_name + '".\n'
                   '> The keyword is "' + words[0] + '".  Please edit\n'
                   '> the file to correct it.  For what it\n'
                   '> is worth, the valid keywords are\n'
                    + gen.FormatOnLines(keywords)
                   )
            gen.WriteError(6021, err, log)
            gen.ErrorOnLine(line_number, line_text, log, False)
            if words_low[0] in ("set", "unset"):
                # They probably forgot a verbatim keyword, add a sentence
                # about that.
                print('> Maybe you forgot a "verbatim" keyword?')
            return(None)


        tr_index += 1

    return(sources_list)


def ProcessVerbatim(graph_triples, tr_index, debug1, plt):
    '''Read all the data defining a verbatim block and send the lines out
    to the .plt file.  Make one change: each time a word "centre" is encountered
    outside a string, change it to "center".

        Parameters:
            graph_triples [(int, str, str)] List of lines in the graph.
            tr_index        int             Pointer to the line with
                                            "begin verbatim" on it.
            plt             handle          The handle of the gnuplot file.

        Returns:
            tr_index        int             Updated pointer to the line after
                                            the "end verbatim" block.
    '''
    # Figure out what line to reference in the comment at the start of
    # the verbatim block.
    line_number = graph_triples[tr_index][0]
    gen.WriteOut('  # Start of a verbatim block, at line '
                  + str(line_number), plt)
    tr_index += 1

    while tr_index < len(graph_triples):
        (line_number, line_data, line_text) = graph_triples[tr_index]
        if debug1:
            print("Line1",str(line_number) + ":", line_data)
        words = line_data.split()
        words_low = line_data.lower().split()

        if words_low[:2] == ["end", "verbatim"]:
            # Stop dumping lines.
            gen.WriteOut('  # End of a verbatim block, at line '
                          + str(line_number), plt)
            break
        else:
            # Write the string, changing "centre" to "center"
            WriteVerbatim(SwitchCentre(line_text), 4, plt)
        tr_index += 1
    return(tr_index)


def ProcessImage(img_triples, settings_dict, last_doodad, sources_list, log, plt):
    '''Read all the data defining an image (file name to use, location
    on the page in one of a variety of forms).  Process any verbatim lines
    or blocks that appear before the "filename" line and complain about
    any that appear after it.

        Parameters:
            img_triples  [(int, str, str)]  List of lines in the image.
            settings_dict   {}              Dictionary of the run settings.
            last_doodad     bool            True if this is in the last graph
                                            or image on the page, False otherwise.
                                            Used to add gnuplot labels for header
                                            and/or footer.
            sources_list    []              List of the names of data sources,
                                            used to create the footer.
            log             handle          The handle of the logfile.
            plt             handle          The handle of the gnuplot file.

        Returns:
            string          str             Text indicating success (not None).

        Errors:
            Aborts with 6181 if the keywords "width" and "height" both appeared.
            Aborts with 6182 if neither of the keywords "width" and "height"
            appeared.
            Aborts with 6183 if there was a "filename" keyword not followed by
            a file name
            Aborts with 6184 if there was more than one of "leftbase",
            "midbase", "rightbase", "leftmid" or "lefttop".
            Aborts with 6185 if there was a "midbase" or "rightbase" keyword
            and a "height" keyword.
            Aborts with 6186 if there was a "leftmid" or "lefttop" keyword
            and a "width" keyword.
            Aborts with 6187 if the image file name did not end in a file
            type extension like ".jpg" or ".png".
            Aborts with 6188 if the image file existed but could not be read.
            Aborts with 6189 if the image file does not exist.
    '''

    # Break out the various settings we will need here.
    file_name = settings_dict["file_name"]
    dir_name = settings_dict["dir_name"]
    debug1 = settings_dict["debug1"]
    image_num = settings_dict["image_num"]
    reserved = settings_dict["reserved"]

    # Define the keywords and the acceptable entries that follow them.
    # Some of these are mutually exclusive, because gnuplot needs to
    # be told where the bottom left corner of the image is.
    # The rules are;
    #  * you can only set one of "leftbase", "midbase", "rightbase", "leftmid"
    #    and "lefttop".
    #  * If you set "leftbase" you can set either "width" or "height", as
    #    we already know where the bottom left corner is.
    #  * If you set "midbase" or "rightbase" you must set "width" and
    #    not set "height".
    #  * If you set "leftmid" or "lefttop" you must set "height" and
    #    not set "width".
    #
    valid_settings = {"leftbase":    ("float any null   an image's left edge location",
                                      "float any null   an image's bottom edge location",),
                      "midbase":     ("float any null   an image's midpoint on the X axis",
                                      "float any null   an image's bottom edge location",),
                      "rightbase":   ("float any null   an image's right edge location",
                                      "float any null   an image's bottom edge location",),
                      "leftmid":     ("float any null   an image's left edge location",
                                      "float any null   an image's midpoint on the Y axis",),
                      "lefttop":     ("float any null   an image's left edge location",
                                      "float any null   an image's top edge location",),
                      "width":       ("float any null   an image's width",),
                      "height":      ("float any null   an image's height",),
                      "filename":    ("QAstr",),
                      "verbatim": ("QAstr",),
                      "begin":   (("verbatim",),),
                      "#skip": "discard",  # This catches all other lines
                     }


    requireds = ["filename"]
    # We make a dictionary of the optional keywords that accepts an
    # optional value.  By default images are kept at their natural
    # aspect ratio.  +1 forces the image to be a square, <1 makes
    # it wide and short, >1 makes it narrow and tall).
    # The second turns on a border around the image (it is off by
    # default).  The "off" entry is only there so that I can change
    # "border := on" to  "border := off" instead of deleting the whole
    # optional argument.
    # The third entry lets the user tell Hobyah that the name of an
    # image file should be added to the list of sources and be shown
    # in the page footer.  This is off by default, because most images
    # will be things like company logos, which should not be shown in
    # the list of sources.  But in a few cases (such as when comparing
    # scans of full-scale test data to digitized data or calculated
    # data) having the name of the image file in the list of sources
    # is useful have.
    # The fourth entry lets the user rotate the image.  Values are in
    # degrees.  Negative values cause clockwise rotation, positive values
    # cause widdershins rotation.
    options = {"ratio": ("float  +  null  an image's aspect ratio"),
               "border": ("on", "off"),
               "namecheck": ("on", "off"),
               "rotate": ("float  any  null  an angle of rotation (deg)"),
              }
    optionals = {"leftbase": options,
                 "midbase": options,
                 "rightbase": options,
                 "leftmid": options,
                 "lefttop": options,
                }

    # We allow one duplicate.
    duplicates = ("verbatim",)

    settings = (valid_settings, requireds, optionals, duplicates)
    # Now call ProcessBlock.  It returns None if an error occurred.
    result = ProcessBlock(img_triples, 0 , settings_dict, "image",
                          {}, settings, log)
    if result is None:
        return(None)
    else:
        (discard, image_dict) = result
    # If we get to here we know that the lines defining the image
    # are correctly formed.  Now we check that all the things we
    # need to plot the image are present and that we have only
    # one of "height" and "width".
    if "height" in image_dict and "width" in image_dict:
        (hline_num, discard, hline_text) = img_triples[image_dict["height"][2]]
        (wline_num, discard, wline_text) = img_triples[image_dict["width"][2]]
        err = ('> Found conflicting lines of entry in an image\n'
               '> block in "' + file_name + '".\n'
               '> There was a "width" keyword setting the width\n'
               '> of an image and a "height" keyword setting the\n'
               '> height.  Only one of these is permitted, due\n'
               '> to the way gnuplot handles image.  Please\n'
               '> edit the file to remove one or the other.'
               )
        gen.WriteError(6181, err, log)
        gen.ErrorOnTwoLines(hline_num, hline_text, wline_num, wline_text, log)
        return(None)
    elif "height" not in image_dict and "width" not in image_dict:
        # This is an unusual message in that we usually check for
        # missing entries in ProcessBlock.  But because the user may
        # set either height or width, we have to make them optional
        # entries and check here.
        (line_number, discard, line_text) = img_triples[0]
        err = ('> Found an invalid image block "' + file_name + '".\n'
               '> There was no entry defining the width or height\n'
               '> of an image.  Please edit the file to add one\n'
               '> or the other, something along the lines of\n'
               '>    width 0.1     # fraction of page width'
               '> or\n'
               '>    height 0.14   # fraction of page height\n'
               '> The image block began at the ' + gen.Enth(line_number)
                   + ' line of\n'
               '> the file:\n'
               '> ' + line_text.strip()
               )
        gen.WriteError(6182, err, log)
        return(None)
    elif image_dict["filename"][0] == "":
        (line_num, discard, line_text) = img_triples[image_dict["filename"][2]]
        err = ('> Found an invalid line of entry in an image\n'
               '> block in "' + file_name + '".\n'
               '> There was an "filename" keyword that was not\n'
               '> followed by an image file name.  Please\n'
               '> edit the file to correct this.'
               )
        gen.WriteError(6183, err, log)
        gen.ErrorOnLine(line_num, line_text, log, False)
        return(None)

    # Check for more than one base setting-out location.  The users must
    # supply one of five keywords but there is nothing in ProcessBlock
    # to prevent them supplying more of them.
    entries = [key for key in image_dict.keys() if (("left" in key) or
                                                          ("base" in key))]
    if len(entries) > 1:
        # Complain about the first two entries.  Don't complain about
        # any more (it is too complex to construct an error message for
        #  2, 3, 4 or 5 conflicting keywords).
        (line1_num, discard, line1_text) = img_triples[image_dict[entries[0]][-1]]
        (line2_num, discard, line2_text) = img_triples[image_dict[entries[1]][-1]]
        err = ('> Found conflicting lines of entry in a graph\n'
               '> block in "' + file_name + '".\n'
               '> There was a "' + entries[0] + '" keyword defining where to set\n'
               '> out an image from and a "' + entries[1] + '" keyword to set it\n'
               '> a second time.  Only one of these is permitted, due to\n'
               '> the way that gnuplot handles images.\n'
               '> Please edit the file to remove one or the other.'
               )
        gen.WriteError(6184, err, log)
        gen.ErrorOnTwoLines(line1_num, line1_text, line2_num, line2_text, log)
        return(None)

    if entries[0] in ("midbase", "rightbase") and "width" not in image_dict:
        # These two keywords must be accompanied by the "width" keyword.
        # Complain.
        (line1_num, discard, line1_text) = img_triples[image_dict[entries[0]][-1]]
        (line2_num, discard, line2_text) = img_triples[image_dict["height"][-1]]
        err = ('> Found conflicting lines of entry in a graph\n'
               '> block in "' + file_name + '".\n'
               '> There was a "' + entries[0] + '" keyword defining where to set\n'
               '> out an image from and a "height" keyword.  Unfortunately,\n'
               '> due to the way that gnuplot handles images you can only\n'
               '> pair a "' + entries[0] + '" keyword with the "width" keyword.\n'
               '> Please edit the file to either use the "width" keyword\n'
               '> or use a "leftbase", "leftmid" or "lefttop" keyword\n'
               '> instead of "' + entries[0] + '".'
               )
        gen.WriteError(6185, err, log)
        gen.ErrorOnTwoLines(line1_num, line1_text, line2_num, line2_text, log)
        return(None)
    elif entries[0] in ("leftmid", "lefttop") and "height" not in image_dict:
        # These two keywords must be accompanied by the "height" keyword.
        # Complain.
        (line1_num, discard, line1_text) = img_triples[image_dict[entries[0]][-1]]
        (line2_num, discard, line2_text) = img_triples[image_dict["width"][-1]]
        err = ('> Found conflicting lines of entry in a graph\n'
               '> block in "' + file_name + '".\n'
               '> There was a "' + entries[0] + '" keyword to set where to set out\n'
               '> an image from and a "width" keyword.  Unfortunately,\n'
               '> due to the way that gnuplot handles images you can only\n'
               '> pair a "' + entries[0] + '" keyword with the "height" keyword.\n'
               '> Please edit the file to either use the "height" keyword\n'
               '> or use a "leftbase", "midbase" or "rightbase" keyword\n'
               '> instead of "' + entries[0] + '".'
               )
        gen.WriteError(6186, err, log)
        gen.ErrorOnTwoLines(line1_num, line1_text, line2_num, line2_text, log)
        return(None)
    # If we get to here we have all the information we need to set out the
    # image on the page.  Get the line number and text for the file name in
    # case we need it for an error message.
    (line_number, discard, line_text) = img_triples[image_dict["filename"][2]]




    file_data = image_dict["filename"][0].strip()
    # Turn the file data into a path, file name, extension and file stem.
    (image_file_name, image_dir_name, discard,
            image_file_ext) = gen.GetFileData(file_data, "", debug1, dir_name)
    # If we get to here we have three valid lines of entry.  Check
    # that there is a file name after the "filename" keyword and that
    # if the filename ends with a filename extension (.png, .jpg etc).
    if image_file_ext == "":
        # There was not a '.' character in the last word of the
        # line, meaning that there was no file extension.  The
        # line should have ended with '.png' or '.jpg' or some
        # such image extension and we need that to pass to gnuplot
        # to tell it what type of image file to expect.
        err = ('> Found an invalid line of entry in a graph\n'
               '> block in "' + file_name + '".\n'
               '> The name of the file after an "image" keyword\n'
               '> did not end in a filename extension (".png",\n'
               '> ".jpg", "tif" or some such).  Please edit\n'
               '> the file to correct this.'
               )
        gen.WriteError(6187, err, log)
        gen.ErrorOnLine(line_number, line_text, log, False)
        return(None)
    else:
        # Check that the directory and file name can be enclosed
        # in either single quotes or double quotes.  If it can,
        # the routine returns the string enclosed in double
        # quotes (the preferred option) or single (if there are
        # already double quotes present in the name).
        result = CheckGnuplotName(image_file_name, image_dir_name,
                        settings_dict, line_number, line_text, log)
        if result is None:
            return(None)
        else:
            gnuplot_path = result
            noimage = False


    # When we get to here we have a suitable file locator.
    if os.access(image_dir_name + image_file_name, os.F_OK):
        # The file exists.
        try:
            inp = open(image_dir_name + image_file_name, 'r')
        except PermissionError:
            err = ('> Found an invalid line of entry in a graph\n'
                   '> block in "' + file_name + '".\n'
                   '> The image file "' + image_file_name + '"\n'
                   '> exists but cannot be read.  Please either\n'
                   "> change the file's permissions or remove the\n"
                   '> block that tries to plot the image.'
                   )
            gen.WriteError(6188, err, log)
            gen.ErrorOnLine(line_number, line_text, log, False)
            noimage = True
    else:
        err = ('> Found an invalid line of entry in a graph\n'
               '> block in "' + file_name + '".\n'
               '> The image file "' + image_file_name + '"\n'
               '> does not exist in the directory named\n'
               '> "' + image_dir_name + '".\n'
               '> Please edit the line of entry to point to\n'
               '> an image that exists.'
               )
        gen.WriteError(6189, err, log)
        gen.ErrorOnLine(line_number, line_text, log, False)
        noimage = True
    if noimage and settings_dict["images"] == "required":
        # We have raised errors 6188 or 6189 and there was an entry in
        # the settings block that required us to fail if we could not
        # read the images.
        return(None)
    elif settings_dict["images"] == "hidden":
        # The user has told the file to not process the images (choosing
        # this can speed up the processing of the files while they are
        # being developed, especially if images are being rotated).
        # Forcible set the noimage setting.
        noimage = True


    # Get the data out of the line that contains the setting out point.
    # It could be one of five.
    setout_point = image_dict[entries[0]]
    # Get any optional entries on the line with the setting-out point.
    optionals_dict = setout_point[2]
    locn_text = "     # From line " + str(img_triples[setout_point[-1]][0])

    # Set a value to add to the one dimension of the graph frame that
    # we don't know.  It should be high so that the height/width and
    # aspect ratio dominate the placement of the image.  We use 100,
    # which in this context is 100 page widths wide or 100 page heights
    # high.
    to_add = 100

    # Figure out where the bottom left corner is.
    if entries[0] == "leftbase":
        left = setout_point[0]
        base = setout_point[1]
        if "width" in image_dict:
            right = left + image_dict["width"][0]
            top = base + to_add
        else:
            right = left + to_add
            top = base + image_dict["height"][0]
    elif entries[0] == "midbase":
        mid = setout_point[0]
        base = setout_point[1]
        # There must be a width setting or we wouldn't have arrived here.
        width = image_dict["width"][0]
        left = mid - width / 2
        right = left + width
        top = base + to_add
    elif entries[0] == "rightbase":
        right = setout_point[0]
        base = setout_point[1]
        left = right - image_dict["width"][0]
        top = base + to_add
    elif entries[0] == "leftmid":
        left = setout_point[0]
        mid = setout_point[1]
        # There must be a height setting or we wouldn't have arrived here.
        height = image_dict["height"][0]
        base = mid - height / 2
        right = left + to_add
        top = base + height
    elif entries[0] == "lefttop":
        left = setout_point[0]
        top = setout_point[1]
        base = top - image_dict["height"][0]
        right = left + to_add
    else:
        # We will get here one of these days during development.
        print("You need to add more 'if' statements in PROC ProcessImage.")
        gen.OopsIDidItAgain(log, file_name)
        return(None)

    # Check if the user defined an aspect ratio.  If they did then it can
    # be used even if the image file is not available.
    if "ratio" in optionals_dict:
        aspect_ratio = float(optionals_dict["ratio"])
    elif noimage:
        # We have to give something and we have no idea what aspect ratio
        # the image actually has.  Make it a square, it's as good as guess
        # as any.
        aspect_ratio = +1
    else:
        # Let the aspect ratio of the image dictate the aspect ratio used
        # by gnuplot.
        aspect_ratio = -1

    if "namecheck" in optionals_dict and optionals_dict["namecheck"] == "on":
        # This image is important enough that the user wants its name to be
        # added to the list of sources in the footer.  This is useful for
        # scans of test data from technical papers.  It isn't needed for
        # company logos.  It is off by default and has to be turned on for
        # each image individually by the "namecheck" optional argument.
        if image_file_name not in sources_list:
            sources_list.append(image_file_name)

    if noimage:
        # We are allowed to keep going despite not being able to show
        # the image.  Show a placeholder image, a rectangle with a
        # cross inside it.
        setup = ["  unset key; unset title; unset tics",
                 "  unset xlabel; unset ylabel; unset x2label; unset y2label",
                 "  set xrange [0:1]; set yrange [0:1]",
                 '  set lmargin at screen ' + gen.FloatText(left) + '; set '
                       'rmargin at screen ' + gen.FloatText(right) + locn_text,
                 '  set bmargin at screen ' + gen.FloatText(base) + '; set '
                       'tmargin at screen ' + gen.FloatText(top),
                 "  set border; set size ratio " + str(aspect_ratio),
                ]
        image = ["#  The following image file is unavailable:",
                 "#     " + image_dir_name + image_file_name,
                 "#  Showing two crossed lines in a box instead.",
                 "plot '-' with lines linestyle 1",
                 "   0  0",
                 "   1  1",
                 "",
                 "   0  1",
                 "   1  0",
                 "  e",
                ]
        # Write the name of the absent image to the log file so we can
        # see the whole path to it in there.
        for line in image[:2]:
            gen.WriteOut(line, log)
    else:
        # If we get to here the image file exists and we can read it.
        # We already have a full path to the image file, enclosed in
        # suitable quote marks in the variable "gnuplot_path".
        # Turn off the graph annotations so that nothing is put on the
        # page except the image.  Set the graph extents to be autosized
        # on the X and Y axes and plot the image.
        setup = ["  unset key; unset title; unset tics",
                 "  unset xlabel; unset ylabel; unset x2label; unset y2label",
                 "  set xrange [*:*]; set yrange [*:*]  # Show the whole image.",
                 '  set lmargin at screen ' + gen.FloatText(left) + '; set '
                       'rmargin at screen ' + gen.FloatText(right) + locn_text,
                 '  set bmargin at screen ' + gen.FloatText(base) + '; set '
                       'tmargin at screen ' + gen.FloatText(top),
                 "  set size ratio " + str(aspect_ratio),
                ]
        # Check if the border optional argument was set.
        if "border" in optionals_dict and optionals_dict["border"] == "on":
            setup.append("  set border")
        else:
            setup.append("  unset border")
        if "rotate" in optionals_dict:
            rotate_text = " rotate = " + optionals_dict["rotate"] #+ "deg "
        else:
            rotate_text = ""
        image = ["plot " + gnuplot_path + " binary filetype="
                 + image_file_ext[1:] + rotate_text + " with rgbimage"
                 + "     # From line " + str(line_number)]



    for line in setup:
        gen.WriteOut(line, plt)

    # Check if we need to write the QA data before plotting.  We do
    # this only on the last graph or the last image.
    if last_doodad:
        PageQA1(settings_dict, sources_list, plt)



    # Process the other lines in the block of data.  We already have
    # everything we need to produce the image, but we read the block
    # in case there are any lines of verbatim data to be processed.
    # These must go in after the setup text has been written out.
    tr_index = 1

    # We set a flag to signal when we have processed the "filename"
    # keyword.  If any verbatim commands occur after it we complain.
    filename_found = False

    # Make a tuple of keywords that are associated with placing the image.
    # We do this because it has seven entries and is used three times
    # below - it is easier to amend it here if we want to.
    img_keywords = ("leftbase", "midbase", "rightbase", "leftmid",
                              "lefttop", "width", "height")
    while tr_index < len(img_triples):
        (line_number, line_data, line_text) = img_triples[tr_index]
        if debug1:
            print("Line1",str(line_number) + ":", line_data)
        # Here we would usually get the optional entries on the line
        # into a dictionary.   But we've already processed them, so
        # we don't.
        words = line_data.split()
        words_low = line_data.lower().split()

        if words_low[:2] == ["end", "image"]:
            break
        elif words_low[0] in img_keywords:
            # This is a line we have already processed outside the loop
            # (see above).  Ignore it.
            pass
        elif words_low[0] == "filename":
            # This is the line that is used to plot the image and let
            # gnuplot process any verbatim commands.  If there are any
            # lines of verbatim data after this, we raise an error
            # message.  We keep a note of the index so we can use it
            # in the error message.
            filename_found = True
            filename_line = tr_index
        elif filename_found and words_low[0] not in img_keywords:
            # This is a line of input that appears after the "filename"
            # keyword but is not involved in setting the image location,
            # image width or image height.  It is most likely a line of
            # verbatim input or the start of a verbatim block (the only
            # current options; but there may be more later).
            #
            # We don't want to give users the impression that they can
            # put verbatim commands after the command to plot the image
            # (which I've decided is the "filename" command).
            #
            # This is to keep consistent with the principle "things that
            # appear early in the file are plotted before things that
            # appear later in the file".  If we allowed verbatim input
            # after the line with "filename" on it and plotted it before
            # writing the image to file, users may be led to
            # believe that verbatim input in an image block can be made
            # to appear behind or in front of the image (as far as I
            # know they can only appear behind it.

            (earlier_num, discard, earlier_text) = img_triples[filename_line]
            err = ('> Found a keyword in an invalid location in\n'
                   '> an image block in "' + file_name + '".\n'
                   '> The keyword is "' + words[0] + '", and it appears\n'
                   '> after the line with the "filename" keyword\n'
                   '> on it.\n'
                   '> The "filename" keyword marks the point in\n'
                   '> an image block that creates the "plot"\n'
                   '> keyword in gnuplot.  Anything after it\n'
                   '> is ignored by gnuplot.\n'
                   '> Please edit the file to either remove the\n'
                   '> line(s) or move them to before the line\n'
                   '> with "filename" on it (line '
                     + str(earlier_num) + ').\n'
                   )

            gen.WriteError(6190, err, log)
            gen.ErrorOnTwoLines(earlier_num, earlier_text,
                                line_number, line_text, log, False)
            return(None)
        elif words_low[0] == "verbatim":
            # Write one line of verbatim data to the file.  First remove
            # the word verbatim from the start:
            rest_text = line_text.split(maxsplit = 1)[1]  \
                          + '       # From line ' + str(line_number)
            # Now write it out, changing "centre" to "center" if required.
            WriteVerbatim(SwitchCentre(rest_text), 2, plt)
        elif words_low[:2] == ["begin", "verbatim"]:
            # Call a routine that puts a block of verbatim text into the
            # .plt file.  We do not check its contents, we assume it is
            # valid gnuplot commands.  When the routine encounters
            # "end verbatim" it returns with tr_index pointing to after
            # the "end verbatim" line.
            tr_index = ProcessVerbatim(img_triples, tr_index, debug1, plt)
        else:
            # It isn't a recognised keyword for an image block and it is
            # not something meant to be passed verbatim.  Get the names
            # of all the recognised keywords and complain.
            keywords = list(valid_settings.keys())
            keywords.remove("#skip")
            keywords.sort()
            err = ('> Found an invalid keyword in an image\n'
                   '> block in "' + file_name + '".\n'
                   '> The keyword is "' + words[0] + '".  Please edit\n'
                   '> the file to correct it.  For what it\n'
                   '> is worth, the valid keywords are\n'
                    + gen.FormatOnLines(keywords)
                   )
            gen.WriteError(6191, err, log)
            gen.ErrorOnLine(line_number, line_text, log, False)
            return(None)
        tr_index += 1

    # Draw the image or placeholder.
    for line in image:
        gen.WriteOut(line, plt)


    # Get the default margins from settings_dict.
    lmargin = str(settings_dict["baselmargin"])
    rmargin = str(settings_dict["basermargin"])
    bmargin = str(settings_dict["basebmargin"])
    tmargin = str(settings_dict["basetmargin"])

    # Reset the graph extents and grid.
    commands = ('  set lmargin at screen ' + lmargin + '; set '
                       'rmargin at screen ' + rmargin,
                '  set bmargin at screen ' + bmargin + '; set '
                       'tmargin at screen ' + tmargin,
                '  reset; set grid',
                    )

    for line in commands:
        gen.WriteOut(line, plt)
    return(sources_list)


def CheckGnuplotName(test_file_name, test_file_path, settings_dict,
                     line_number, line_text, log):
    '''Take a file name and file path and check them for single quote
    and double quote characters.  If they contain both, complain and
    return None.  If they contain double quotes, return the file path and
    file name enclosed in single quotes.  If they don't, return the file
    path and file name enclosed in double quotes.

        Parameters:
            test_file_name  str             Name of a file that we want to
                                            pass to gnuplot.
            test_file_path  str             Name of a path that we want to
                                            pass to gnuplot.
            settings_dict   {}              Dictionary of the run settings.
            line_number     int             The line number.
            line_text       str             The entire line, including comments.
            log             handle          The handle of the logfile.

        Returns:
            quoted_path     str             The file name and file path enclosed
                                            in either double quotes or single
                                            quotes.

        Errors:
            Aborts with 6161 if there was ' and " in the file name.
            Aborts with 6162 if there was ' in the file name and " in the file path.
            Aborts with 6163 if there was " in the file name and ' in the file path.
            Aborts with 6164 if there was ' and " in the file path.
    '''
    file_name = settings_dict["file_name"]
    if "'" in test_file_name and '"' in test_file_name:
        # The image name has both single and double quotes in it,
        # which means that we can't enclose the path and file name
        # in one or the other in the plot command.  Complain.
        err = ('> Found a problem in a line of entry in a graph\n'
               '> block in "' + file_name + '".\n'
               "> The file named '''" + test_file_name + "'''\n"
               '> exists but gnuplot cannot be told to plot it\n'
               '> because the file name contains both double and\n'
               '> single quote characters ("'" and ').\n"
               '> Please rename the image file and change the line\n'
               '> in the file.'
               )
        gen.WriteError(6161, err, log)
        gen.ErrorOnLine(line_number, line_text, log, False)
        return(None)
    elif "'" in test_file_name and '"' in test_file_path:
        # ' in the image name and " in the image path.
        err = ('> Found a problem in a line of entry in a graph\n'
               '> block in "' + file_name + '".\n'
               '> The file named   "' + test_file_name + '"\n'
               '> exists but gnuplot cannot be told to plot it\n'
               '> because the file name contains a double quote\n'
               '> character and the file path contains a single\n'
               '> quote character.  The file path is\n'
               '> ' + test_file_path + '.\n'
               '> Please rename the image file or the image path\n'
               '> so that there is only " or '"' (not both) and\n"
               '> change the line in the file.'
               )
        gen.WriteError(6162, err, log)
        gen.ErrorOnLine(line_number, line_text, log, False)
        return(None)
    elif '"' in test_file_name and "'" in test_file_path:
        # " in the image name and ' in the image path.
        err = ('> Found a problem in a line of entry in a graph\n'
               '> block in "' + file_name + '".\n'
               "> The file named '" + test_file_name + "'\n"
               '> exists but gnuplot cannot be told to plot it\n'
               '> because the file name contains a single quote\n'
               '> character and the file path contains a double\n'
               '> quote character.  The file path is\n'
               '> ' + test_file_path + '.\n'
               '> Please rename the image file or the image path\n'
               '> so that there is only " or '"' (not both) and\n"
               '> change the line in the file.'
               )
        gen.WriteError(6163, err, log)
        gen.ErrorOnLine(line_number, line_text, log, False)
        return(None)
    elif "'" in test_file_path and '"' in test_file_path:
        # The path name has both single and double quotes in it.
        err = ('> Found a problem in a line of entry in a graph\n'
               '> block in "' + file_name + '".\n'
               "> The file named '" + test_file_name + "'\n"
               '> exists but gnuplot cannot be told to plot it\n'
               '> because the path name contains both double and\n'
               '> single quote characters ("'" and ').  The path\n"
               '> is ' + test_file_path + '.\n'
               '> Please rename the path file and change the line\n'
               '> in the file.'
               )
        gen.WriteError(6164, err, log)
        gen.ErrorOnLine(line_number, line_text, log, False)
        return(None)

    if '"' in test_file_path or '"' in test_file_name:
        # We have to use single quotes.
        quoted_path = "'" + test_file_path + test_file_name + "'"
    else:
        # We can use double quotes.
        quoted_path = '"' + test_file_path + test_file_name + '"'
    return(quoted_path)


def SwitchCentre(string):
    '''Take one line of gnuplot commands that may have formatting
    commands in them, such as
         set label "{/*0.8 footnote}" at graph 0.5, 0.5 centre
    or
         set label '{/*0.8 footnote}' at graph 0.5, 0.5 centre
    or
         set key top centre
    and replace any instances of 'centre' after the last double
    or single quote with the US equivalent, 'center'.  If there
    are no double or single quotes in the string replace all the
    instances of "centre".
    This is so that I don't keep having to remember to write 'center'
    instead of 'centre' for gnuplot formatting in verbatim blocks.

    This routine is probably fragile, as there may be valid gnuplot
    commands that have ' or " outside of strings, and if there are
    then this will break gnuplot.  If that happens, it can only be
    fixed by using the word 'center' directly as a temporary fix,
    then getting this routine modified to handle the newfound corner
    case.

        Parameters:
            string          str             A string of gnuplot commands,
                                            possibly including formatting
                                            commands.

        Returns:
            string          str             The string of gnuplot commands
                                            with some instances of "centre"
                                            changed to "center".
    '''
    # Set our default result, the same string.
    mod_string = string
    # First check if the word "centre" appears anywhere.  If it
    # doesn't we don't need to bother figuring out if we need to
    # replace it.
    if string.lower().find("centre") != -1:
        # The word "centre" does appear in the line.
        single_result = string.rsplit(sep = "'", maxsplit = 1)
        double_result = string.rsplit(sep = '"', maxsplit = 1)
        if len(single_result) == 1 and  len(double_result) == 1:
            # The line did not have single quotes or double quotes
            # in it, but it did have "centre".  We want to change
            # "centre" to "center" so we spoof the candidate list
            # and the split char for the code below.
            candidate = ['', string]
            split_char = ''
        elif len(single_result[0]) < len(double_result[0]):
            # The last double quote was before the last single
            # quote.  Assume that the label is enclosed in single
            # quotes.
            candidate = single_result
            split_char = "'"
        else:
            candidate = double_result
            split_char = '"'

        if "centre" in candidate[1].lower():
            # "Centre" does appear in the line after the end of
            # the text.  Replace all instances of "centre" with
            # "center" in the second part of the result.
            new_2nd = re.sub('centre', 'center', candidate[1],
                             flags=re.IGNORECASE)
            mod_string = candidate[0] + split_char + new_2nd
    return(mod_string)


def ChooseGnuplotString(string):
    '''Take a string of data that may be a string of text without any
    quote marks (' or ") or a correctly-formatted gnuplot string command
    with quote marks, size multipliers, offsets and suchlike.  Figure
    out which of these the string is and return a string that gnuplot
    will likely accept.

    If the string is of the first type (no quotes), assume it is a raw
    title with no multipliers or offsets.  Encase it in quotes and add
    'noenhanced' after it; this turns off gnuplot's enhanced text
    processing capabilities.

    If the string is of the second type (starts with ' or "), then
    assume it is a gnuplot title with offsets, size multipliers etc.
    all stated correctly.  Gnuplot can complain if the syntax is wrong.

        Parameters:
            string          str             A string of text after a "title"
                                            argument (or similar).

        Returns:
            The unaltered string or the same string enclosed in double
            quotes and with ' noenhanced' appended after the second
            double quote.
    '''
    # This routine takes an argument like 'Air velocity on the upline route'
    # (without single quotes) are returns it in double quotes, e.g.
    # '"Air velocity on the upline route" noenhanced'.
    # It is used to set titles and graph axis names.
    # It also takes arguments like
    #    "{/*1.2 Air velocity on the upline route}" offset 0,-0.5
    #    '{/*1.2 Air velocity on the upline route}' offset 0,-0.5
    # and returns them unchanged, because the user has already set specific
    # gnuplot adjustment commands in the line.
    #
    # Double quotes are chosen by default so that \n is processed
    # correctly.  The 'noenhanced' argument is appended so that the
    # presence of things like underscores don't make subscripts (the
    # underscores are printed instead).
    if len(string) == 0:
        # Blank entries for graph keys are common.
        result = '""'
    elif string[0] in ('"', "'"):# and string[0] == string[-1]:
        # The first character is " or '.  We assume it is a
        # correctly-formed gnuplot string and let gnuplot complain
        # about any syntax errors.
        # If the first character only appears in the string once, we
        # assume that the user wants that quote mark printed.  We
        # enclose the string in the other type of quotes.
        # We subtract any instances of \" or \' inside the string
        # from the count, as \" means "print me a double quote in
        # the string" and  \' means "print me a single quote in the
        # string.
        quotemark = string[0]
        count = string.count(quotemark) - string.count('\\' + quotemark)
        if count != 2:
            if quotemark == "'":
                enclose = '"'
            else:
                enclose = "'"
            string = enclose + string + enclose
        # We make one set of substitions: gnuplot requires the US
        # spelling of the word "centre" ("center").  We find any
        # instances of "centre" after the last single or double quote
        # and replace them with  "center".
        result1 = SwitchCentre(string)
        # We make a second check to see if the "noenhanced" or
        # "enhanced" flags have been set.  If they have, we leave
        # them as they are.  If they haven't, we set the "enhanced"
        # flag because that is Hobyah's default setting.
        result = AddEnhanced(string)
    else:
        # It's a random string, hopefully plain text.  Escape all
        # its oddities, enclose it in double quotes and turn off
        # the enhanced text setting.
        new_str = SanitiseUnEnhancedString(string)
        result = '"' + new_str + '" noenhanced'
    return(result)


def AddEnhanced(string):
    '''Take a string (typically a graph or axis name) and check if
    its gnuplot formatting instruction ends with " enhanced".  If it
    does, return the string unchanged.  If it doesn't, turn on the
    "enhanced" option.  The enhanced option is the default we set at
    the top of the .plt file but it may have been turned off in earlier
    title or in an x/y/x2/y2label command, and that state can persist.

        Parameters:
            string          str             A string of text after a "title"
                                            argument (or similar).

        Returns:
            The unaltered string or the same string enclosed in double
            quotes and with ' enhanced' appended after the second
            double quote.
    '''
    # Get the quote character (when this routine is called, the first
    # character in the string is a single quote or a double quote).
    start = string[0]

    parts = string.split(sep = start, maxsplit = -1)
    if " enhanced" not in parts[1].lower():
        # There is no setting for it.  Set this to enhanced text mode.
        string = string + " enhanced"
    return(string)


def SanitiseEnhancedString(string):
    '''Take a string (typically a file name) and alter it so that
    it is suitable for being included in a gnuplot label that is
    printed with double quotes.  It double escapes the following
    characters:
        _^{}&"'\
    The double escapes are needed because when Python sees two
    backslashes in a string it is writing to a file it writes one
    backslash to the file.

    Escaping these characters is needed so that gnuplot doesn't
    treat them as formatting characters like subscript (_) or
    superscript (^) etc.

        Parameters:
            string          str             A string of text after a
                                            "title" argument (or
                                            similar).

        Returns:
            The string with backslashes prepended to all instances of
            \, ^, &, _ {, }, " and '.
    '''
    # Backslashes, single quotes and double quotes seem to need one
    # backslash prepended.
    escapeables1 =  '\\"' + "'"
    for char in escapeables1:
        string = string.replace(char, '\\' + char)
    # LaTeX formatting characters seem to need two backslashes prepended.
    escapeables2 =  '^&_{}'
    for char in escapeables2:
        string = string.replace(char, '\\\\' + char)
    return(string)


def SanitiseUnEnhancedString(string):
    '''Take a string (typically a file name) and alter it so that
    it is suitable for being included in a gnuplot label that is
    printed with the "noenhanced" option.  It double escapes backslashes
    and double quotes.

        Parameters:
            string          str             A string of text after a "title"
                                            argument (or similar).

        Returns:
            The string with backslashes prepended to all instances of
            \, " and '.
    '''
    escapeables =  '\\"'
    for char in escapeables:
        string = string.replace(char, '\\' + char)
    return(string)


def ProcessFilesLoop(files_triples, settings_dict, files_dict, log):
    '''Read all the data defining one page and either a list of nicknames
    drawn from the list in the "begin files" block or an instruction to
    use all the names in the "begin files" block.

    Open a .plt file with a unique name for this loop (not the same .plt
    file used for the pages or the timeloops) and create text files with
    curve data (also with unique names).

    Loop over each file in the list and write a page to the .plt file,
    so that we get a .pdf file that has one page for each file.  This
    is intended for generating a .pdf file of SES incident runs with
    the fires in different direction.

        Parameters:
            files_triples [(int, str, str)] List of lines in the file.
            settings_dict   {}              Dictionary of the run settings.
            files_dict      {}              The dictionary of file names
                                            and their nicknames.
            log             handle          The handle of the logfile.

        Returns:
            string if successful, None if not.

        Errors:
            Aborts with 2981 if there was nothing defining which files
            to plot ("all files", "begin exclude" or "begin nicknames".
            Aborts with 2982 if more than one definition of which files
            in the "files" block to plot (inc. calc).
            Aborts with 2983 if a nickname in a "nicknames" block is
            not in the file's "files" block.
    '''
    # Break out the various settings we will need here.
    dir_name = settings_dict["dir_name"]
    file_stem = settings_dict["file_stem"]
    file_name = settings_dict["file_name"]
    ancill_path = settings_dict["ancill_path"]
    debug1 = settings_dict["debug1"]

    try:
        page_num = settings_dict["page_num"]
    except:
        # This plot file only had loops in it.  Spoof the page number, we
        # don't need it.
        page_num = 1
    loop_num = settings_dict["loop_num"]


    # Get the loop-specific settings.  There are three mutually exclusive
    # ways of setting the frames we want to print:
    #  * the word "all" followed by "files" on a line.  This means
    #    "process all the files in the 'begin files' block in the order
    #    they appear there".
    #  * a "begin nicknames...end nicknames" block containing a list of
    #    valid nicknames from the "begin files" block that we want to
    #    plot with.
    #  * a "begin exclude...end exclude" block containing a list of
    #    valid nicknames from the "begin files" block that we don't want
    #    to plot with (this is probably more useful than the previous
    #    one).
    #
    valid_settings = {"all": (("files",),),
                      "begin": (("exclude", "nicknames", "graph",
                                 "image", "verbatim"),),
                      "#skip": "discard",  # This catches all other lines
                     }
    # We make a list of required entries.
    requireds = []

    # We make a dictionary of the optional keywords that accepts
    # any optional value.
    optionals = {}
    # Allow duplicate "begin" blocks but we hope these are graph,
    # image etc., not both "nicknames" and "exclude".
    duplicates = ("begin")
    settings = (valid_settings, requireds, optionals, duplicates)
    # Now call ProcessBlock.  It returns None if an error occurred.
    # it returns a dictionary of the entries.
    if debug1:
        print("Calling ProcessBlock for filesloop settings")
    result = ProcessBlock(files_triples, 0, settings_dict, "filesloop",
                          {}, settings, log)
    if result is None:
        return(None)
    else:
        (discard, loop_dict) = result

    # Check for clashes between "all files", "begin nicknames" and
    # "begin exclude".  We make a list of the relevant keys for the
    # error message.
    keys = []
    for index, key in enumerate(loop_dict.keys()):
        if key[:3] == "all":
            keys.append(key)
        elif key[:5] == "begin":
            if loop_dict[key][0] in ("exclude", "nicknames"):
                keys.append(key)
    # Check that we have one, and only one of the file use definitions.
    if len(keys) < 1:
        # Get details of the line at the start of the "begin filesloop"
        # block.
        line_num, discard, line_text = files_triples[0]
        err = ('> The ' + gen.Enth(loop_num) + ' of "' + file_name + '"\n'
               '> (a filesloop) had no definition of which files to\n'
               '>  run the loop over - please add one.  Valid definitions\n'
               '>  are:\n'
               '>  * a line with "all files" (without quotes) between\n'
               '>    the "begin filesloop" line and the first "begin\n'
               '>    graph" or "begin image" line.\n'
               '>  * a "begin excluded...end excluded" block between\n'
               '>    the "begin filesloop" line and the first "begin\n'
               '>    graph" or "begin image" line.  Put the nicknames\n'
               '>    to NOT plot at inside the block.  All the other\n'
               '>    nicknames in the "begin files" block will be used.\n'
               '>  * a "begin nicknames...end nicknames" block between\n'
               '>    the "begin filesloop" line and the first "begin\n'
               '>    graph" or "begin image" line.  Put the nicknames\n'
               '>    to plot at inside the block.'
              )
        gen.WriteError(2981, err, log)
        gen.ErrorOnLine(line_num, line_text, log, word = "Relevant")
        return(None)
    elif len(keys) > 1:
        # We may have two or three entries.  We treat it as if it has
        # three entries and let the error writing routine ignore any
        # duplicates.
        tr_index1 = loop_dict[keys[0]][-1]
        tr_index2 = loop_dict[keys[1]][-1]
        tr_index3 = loop_dict[keys[-1]][-1]
        line1_num, discard, line1_text = files_triples[tr_index1]
        line2_num, discard, line2_text = files_triples[tr_index2]
        line3_num, discard, line3_text = files_triples[tr_index3]
        err = ('> The ' + gen.Enth(loop_num) + ' loop of "'
                 + file_name + '"\n'
               '> (a filesloop) had more than one definition of\n'
               '> which files to run the loop over - please use\n'
               '> just one of:\n'
               '>   "all files",\n'
               '>   a "begin exclude...end exclude" block, or\n'
               '>   a "begin nicknames...end nicknames" block.'
              )
        gen.WriteError(2982, err, log)
        gen.ErrorOnManyLines(line1_num, line1_text,
                              line2_num, line2_text,
                              line3_num, line3_text,
                              line3_num, line3_text, log,
                              word = "Conflicting")
        return(None)
    # If we get to here, we have one (and only one) definition of
    # which files to plot.
    allowed = files_dict.keys()
    if "all" in loop_dict:
        # The user wants all the files plotted, including the file
        # being run now (the last entry in "files_dict" is "calc").
        nicknames = tuple(allowed)
    else:
        # Loop over the lines in the file looking for a block of nicknames
        # to include or exclude.
        tr_index = 0
        while tr_index < len(files_triples):
            (line_number, line_data, line_text) = files_triples[tr_index]
            words = line_data.split()
            two_words = line_data.lower().split()[:2]
            if debug1:
                print("Line1",str(line_number) + ":", line_data)
            if two_words == ["begin", "exclude"]:
                # Make a list of nicknames to remove entries from.
                nicknames = list(allowed)
                blockname = "exclude"
                break
            elif two_words == ["begin", "nicknames"]:
                # Make an empty list to add nicknames to.
                nicknames = []
                blockname = "nicknames"
                break
            elif two_words in (["end", "exclude"], ["end", "nicknames"]):
                break
            tr_index += 1


        # Loop over all the lines in the dictionary, looking for
        # the key "begin".  When we find it, we read a list of
        # nicknames to either add to the list or remove from the
        # list.  When we read a line with "end" on it, we break out.
        in_block = False
        tr_index = 0
        while tr_index < len(files_triples):
            (line_number, line_data, line_text) = files_triples[tr_index]
            words = line_data.split()
            words_low = line_data.lower().split()
            if debug1:
                print("Line1",str(line_number) + ":", line_data)
            if words_low[:2] == ["end", blockname]:
                # We've finished, break out of the loop.
                break
            elif words_low[:2] == ["begin", blockname]:
                in_block = True
            elif in_block is True:
                # All of the words on the line should be nicknames to
                # add or remove from the list of nicknames to process.
                for candidate in words_low:
                    if candidate in allowed:
                        if blockname == "nicknames":
                            nicknames.append(candidate)
                        else:
                            # We use an if clause here in case the same
                            # nickname appears in the list twice.  It's
                            # removed the first time and ignored the
                            # second time.
                            if candidate in nicknames:
                                nicknames.remove(candidate)
                    else:
                        err = ('> The ' + gen.Enth(loop_num) + ' loop of "'
                                 + file_name + '"\n'
                               '> had a "begin ' + blockname
                                 + '" entry that referred\n'
                               '> to an invalid nickname, ' + candidate + '".\n'
                               '> Please change it to a valid nickname or\n'
                               "> remove it.  For what it's worth, here are all\n"
                               '> the valid nickname(s):\n')
                        err = err + gen.FormatOnLines(allowed)
                        gen.WriteError(2983, err, log)
                        gen.ErrorOnLine(line_number, line_text, log)
                        return(None)
            tr_index += 1

    # Open a new handle to hold the .plt file for the pages.  This has a
    # different name to the .plt file that holds the page plots.  We add
    # "-lp<num>" to the file stem, where num is the number of this loop.
    files_file_stem = file_stem + "-lp" + str(loop_num)
    files_plt_name = files_file_stem + ".plt"
    if settings_dict["splitpages"] == "true":
        # We want individual files for each page of output.
        files_pdf_name = file_stem + "-p001.pdf"
    else:
        files_pdf_name = files_file_stem + ".pdf"
    result = OpenPltFile(settings_dict, files_plt_name, log)
    if result is None:
        return(None)
    else:
        files_plt = result

    # Now set the default settings for a new .plt file.  These use the
    # same settings as the main .pdf file.
    WritePltStart(settings_dict, files_pdf_name, log, files_plt)

    # Spoof the file stem in settings_dict with files_file_stem so that
    # we get the correct curve file names.
    settings_dict.__setitem__("file_stem", files_file_stem)

    # Build a string that we use a lot in the for loop below.
    rest = " of " + str(len(nicknames)) + " in loop " + str(loop_num)

    print("> Building filesloop " + str(loop_num) + ":")
    pagecount = str(max(2, len(str(nicknames))))
    num_format = '{:0>' + pagecount + '}'
    settings_dict.__setitem__("num_format", num_format)


    for frame_num, nickname in enumerate(nicknames, start = 1):
        # Set the time we want as if we had set it as a constant.  When
        # ReadBinData sees the word "*name" where a name should be,
        # it will fetch this value from settings_dict.
        settings_dict.__setitem__("#*name", [nickname, "spoofed line ", -1])

        # Write something to the screen so that the user knows what is going
        # on.
        if frame_num == 1:
            print(">   Plotted page " + str(frame_num) + rest)
        else:
            print(">   Repeated page " + str(frame_num) + rest)

        # Set the page number so that the page number appears on the QA line.
        settings_dict.__setitem__("page_num", frame_num)
        ResetPage(frame_num, settings_dict, "filesloop", files_file_stem,
                  files_plt)
        # Now process the page at this time.  Every time a nickname
        # that is the constant "*name", it will substitute the
        # correct nickname of the current file to process.
        result = ProcessPage(files_triples, settings_dict, files_dict,
                             "filesloop", log, files_plt)
        if debug1:
            print("Timeloop returned: ", result)
        if result is None:
            # Something went wrong.
            files_plt.close()
            return(None)
    # If we get here we exited the loop successfully.  We should have
    # written one .plt file and a load of curve data files at each time
    # step.
    files_plt.close()

    # Restore the two settings we spoofed and remove the constant with
    # the name "*time" from the constants.  We might not have to do this
    # because we're not returning settings_dict: I really ought to check
    # that one of these days.  Too much passing arguments as REF in Algol
    # 68 in my younger days.
    settings_dict.__setitem__("file_stem", file_stem)
    settings_dict.__setitem__("page_num", page_num)
    settings_dict.__delitem__("#*name")

    return(files_plt_name, files_pdf_name, len(nicknames))


def ProcessTimeLoop(anim_triples, settings_dict, files_dict, log):
    '''Read all the data defining an animation (much the same as a page
    definition but it also has data to define a list of times to use).

    Open a .plt file with a unique name for this loop (not the same .plt
    file used for the pages) and create text files with curve data (also
    with unique names).

    Loop over each time in the list, setting the time as a constant with the
    name "#*time" (as if it were a constant in the constants block) and
    calling the page creation routine with the block of page data.  The
    asterisk at the start is a nod to the "*<num>" values in the graph axis
    extents where the leading "*" means "tell gnuplot to guess what to use".

    Plotting the same set of graph definitions at the different times on
    successive pages creates a flipbook pdf file.  This may be turned
    into a video or gif by using ImageMagick to convert the pages to .png
    files then using ffmpeg or avconvert to compile the .png files to
    a single animation, but the flipbooks are better, I think.

        Parameters:
            anim_triples [(int, str, str)]  List of lines in the file.
            settings_dict   {}              Dictionary of the run settings.
            files_dict      {}              The dictionary of file names
                                            and their nicknames.
            log             handle          The handle of the logfile.

        Returns:
            string if successful, None if not.

        Errors:
            Aborts with 2281 if the stop time is before the start time.
            Aborts with 2282 if the step time is 1 microsecond or less.
    '''
    # Break out the various settings we will need here
    dir_name = settings_dict["dir_name"]
    file_stem = settings_dict["file_stem"]
    file_name = settings_dict["file_name"]
    ancill_path = settings_dict["ancill_path"]
    debug1 = settings_dict["debug1"]
    time_accuracy = settings_dict["time_accuracy"]

    try:
        page_num = settings_dict["page_num"]
    except:
        # This plot file only had loops in it.  Spoof the page number, we
        # don't need it.
        page_num = 0
    loop_num = settings_dict["loop_num"]


    # Get the loop-specific settings.  There are two mutually exclusive
    # ways of setting the frames we want to print: three lines like
    #   start  0
    #   stop  12
    #   step  0.05
    # or
    #  timelist [0, 1, 2, 3] + range(3, 12, 0.1)
    #
    # The latter is more flexible because it allows a variable timestep
    # but the former is simpler.  If the latter is used, the former
    # don't have to be commented out.
    #
    valid_settings = {"start": ("float 0+ null the animation start time",),
                      "stop": ("float 0+ null the animation start time",),
                      "step": ("float + null the animation time step",),
                      "timelist": ("QAstr",), # mixture of lists and ranges.
                      "#skip": "discard",  # This catches all other lines
                     }
    # We make a list of required entries.  We either want one timelist
    # entry or all of start, stop and step.
    requireds = [("timelist", "start"),
                 ("timelist", "stop"),
                 ("timelist", "step")]
    # We make a dictionary of the optional keywords that accepts
    # any optional value.
    optionals = {}
    # Don't allow any duplicates.
    duplicates = ()
    settings = (valid_settings, requireds, optionals, duplicates)
    # Now call ProcessBlock.  It returns None if an error occurred.
    # it returns a dictionary of the entries.
    if debug1:
        print("Calling ProcessBlock for timeloop settings")
    result = ProcessBlock(anim_triples, 0, settings_dict, "timeloop",
                          {}, settings, log)
    if result is None:
        return(None)
    else:
        (discard, loop_dict) = result
    # Check if the user did not set a custom range of print times.  If
    # they did not, check if they set all three of "start", "stop" and
    # "step".

    if "timelist" in loop_dict:
        # Check if the value returned was a number or a Python-style list.
        # Fault if it was neither.
        value = loop_dict["timelist"]
        maybe_list = value[0]
        tr_index = value[-1]
        (line_number, line_data, line_text) = anim_triples[tr_index]
        try:
            # The user may have put one number only, as in "timelist 21.5".
            # This is eccentric and contrary to how timeloops are used,
            # but is not technically forbidden in the rules under which
            # "timeslist" entries are processed.  "Timelist [21.5]"
            # would be accepted because it is a list, and I can't be
            # bothered to trap lists with one element in them.  If you
            # are idiotic enough to want a timelist with one page, who
            # am I to write an error message for t
            times_list = [float(maybe_list)]
        except (ValueError, TypeError):
            # Check if the contents of the line after the key take the
            # form of a Python list.
            if debug1:
                print("Checking a timelist as a list", maybe_list)
            result = CheckListAndRange(maybe_list, "spoof_it",
                                       settings_dict, anim_triples,
                                       line_number, line_text, False, log)
            if result is None:
                return(None)
            else:
                times_list = result
    else:
        # We have all of the start, stop and step entries.
        # Now get the time settings and check that they are sensible.
        start = loop_dict["start"][0]
        stop = loop_dict["stop"][0]
        step = loop_dict["step"][0]
        if start > stop:
            # Figure out which lines the settings are on for the error
            # message.
            start_triple = anim_triples[loop_dict["start"][-1]]
            stop_triple = anim_triples[loop_dict["stop"][-1]]
            # Complain about the start and stop times.
            err = ('> The animation on loop ' + str(loop_num)
                      + ' of "' + file_name + '"\n'
                   '> has a start time that is later than its stop\n'
                   '> time. Please alter the settings so that the\n'
                   '> start time is equal to or before the stop time.'
                  )
            gen.WriteError(2281, err, log)
            gen.ErrorOnTwoLines(start_triple[0], start_triple[2],
                                stop_triple[0], stop_triple[2], log)

            return(None)
        elif math.isclose(step, 0.0, abs_tol = 1e-9):
            # Complain about the step time being so close to zero that it
            # effectively is zero.  We already checked for negative numbers
            # in ProcessBlock.
            (line_number, discard, line_text) = anim_triples[loop_dict["step"][-1]]
            err = ('> The animation on loop ' + str(loop_num)
                      + ' of "' + file_name + '"\n'
                   '> has a step time that is so close to zero that it is\n'
                   '> considered to be zero.  Please alter the step time to\n'
                   '> make it above 1 nanosecond.'
                  )
            gen.WriteError(2282, err, log)
            gen.ErrorOnLine(line_number, line_text, log)

            return(None)
        # Build a list of the times to use from "start", "stop" and "step".
        # We can't use a Python range because these are floating-point numbers,
        # so we use numpy's "arange" function instead.  The last time may not
        # be exactly "stop", it may be lower.  We add half a step to the end
        # time so that we can include the end time in the list (numpy's
        # "arange" function excludes the last value by default, same as a python
        # "range" function does.
        times_list = list(np.arange(start, stop + 0.5 * step, step))

    # We now have a list of times.  We round these to eight decimal places,
    # remove duplicate entries, then sort them into ascending order.  We
    # don't limit the times to the duration of the current run because we
    # may be plotting from other files and do not know at the moment how
    # long they ran for.
    times_list = SanitiseTimeSeries(times_list, time_accuracy, math.inf)

    # Open a new handle to hold the .plt file for the animation.  This has
    # a different name to the .plt file that holds the page plots.  We add
    # "-lp<num>" to the file stem, where num is the number of this "begin loop"
    # group (starting from 1).
    anim_file_stem = file_stem + "-lp" + str(loop_num)
    anim_plt_name = anim_file_stem + ".plt"
    anim_pdf_name = anim_file_stem + ".pdf"
    result = OpenPltFile(settings_dict, anim_plt_name, log)
    if result is None:
        return(None)
    else:
        anim_plt = result

    # Now set the default settings for a new .plt file.  These use the
    # same settings as the main .pdf file.
    WritePltStart(settings_dict, anim_pdf_name, log, anim_plt)

    # Spoof the file stem in settings_dict with anim_file_stem so that
    # we get the correct curve file names.
    settings_dict.__setitem__("file_stem", anim_file_stem)

    # Build a string that we use a lot in the for loop below.
    rest = " of " + str(len(times_list)) + " in loop " + str(loop_num)


    print("> Building loop animation " + str(loop_num) + ":")
    pagecount = str(max(2, len(str(times_list))))
    num_format = '{:0>' + pagecount + '}'
    settings_dict.__setitem__("num_format", num_format)

    for frame_num, frame_time in enumerate(times_list, start = 1):
        # Set the time we want as if we had set it as a constant.  When
        # ProcessCurve sees the word "*time" where a number should be
        # it will fetch this value from settings_dict.
        settings_dict.__setitem__("#*time", [gen.FloatText(frame_time),
                                  "spoofed line "
                                  + gen.FloatText(frame_time), -1])

        # Write something to the screen so that the user knows what is going
        # on.
        print(">   Frame " + str(frame_num) + rest)

        # Set the frame number as if it was the page number so that the
        # page number appears on the QA line.
        settings_dict.__setitem__("page_num", frame_num)
        ResetPage(frame_num, settings_dict, "timeloop", anim_file_stem,
                  anim_plt)

        # Now process the page at this time.  Every time a curve
        # that has its time replaced with the constant "*time" it
        # will substitute the current frame time.
        result = ProcessPage(anim_triples, settings_dict, files_dict,
                             "timeloop", log, anim_plt)
        if debug1:
            print("Timeloop returned: ", result)
        if result is None:
            # Something went wrong.
            anim_plt.close()
            return(None)
    # If we get here we exited the loop successfully.  We should have
    # written one .plt file and a load of curve data files at each time
    # step.
    anim_plt.close()

    # Restore the two settings we spoofed and remove the constant with
    # the name "*time" from the constants.  We might not have to do this
    # because we're not returning settings_dict: I really ought to check
    # that one of these days.  Too much passing arguments as REF in Algol
    # 68 in my younger days.
    settings_dict.__setitem__("file_stem", file_stem)
    settings_dict.__setitem__("page_num", page_num)
    settings_dict.__delitem__("#*time")

    return(anim_plt_name, anim_pdf_name, len(times_list))


def ProcessPage(page_triples, settings_dict, files_dict, block_type, log, plt):
    '''Read all the data defining a page.  Call the graph creation routine
    (which generates lines of gnuplot data and writes them to the .plt file)
    as many times as needed.

        Parameters:
            page_triples [(int, str, str)]  List of lines in the file.
            settings_dict   {}              Dictionary of the run settings.
            files_dict      {}              The dictionary of file names
                                            and their nicknames.
            block_type      str             "page" if we are processing pages,
                                            "timeloop" if we are processing an
                                            animation, "filesloop" if we are
                                            processing one page looping over
                                            a set of files.
            log             handle          The handle of the logfile.
            plt             handle          The handle of the gnuplot file.

        Returns:
            String if successful, None if not.
    '''

    # Break out the various settings we will need here
    dir_name = settings_dict["dir_name"]
    file_stem = settings_dict["file_stem"]
    file_name = settings_dict["file_name"]
    debug1 = settings_dict["debug1"]
    plot_units = settings_dict["plotunits"]

    when_who = settings_dict["when_who"]
    qa1 = settings_dict["qa1"]
    qa2 = settings_dict["qa2"]
    qa3 = settings_dict["qa3"]
    page_num = settings_dict["page_num"]

    # Set the default system of units to plot in (same as the plot units).
    # We do this every time we start processing a new page, so that if
    # a page has a setting for "pageunits" it does not persist to the
    # next page.
    settings_dict.__setitem__("pageunits", plot_units)

    # Get any page-specific settings.  We may have set a pageunits entry.
    # or changed the paper size or orientation.  The latter only apply
    # to this page.
    valid_settings = {"pageunits": (("si", "us"),),
                      "#skip": "discard",  # This catches all other lines
                     }
    # We make a list of entries that we must have (none).
    requireds = []
    # We make a dictionary of the optional keywords that accepts
    # any optional value.
    optionals = {}
    # We don't allow any duplicates.
    duplicates = ()
    settings = (valid_settings, requireds, optionals, duplicates)
    # Now call ProcessBlock.  It returns None if an error occurred.
    # it returns an updated settings_dict (if any lines setting
    # entries in valid_settings exist in the file).
    if debug1:
        print("Calling ProcessBlock for " + block_type + " settings")
    result = ProcessBlock(page_triples, 0, settings_dict, block_type,
                          settings_dict, settings, log)
    if result is None:
        return(None)
    else:
        (discard, settings_dict) = result
    # Flatten any new settings from tuples (as returned by ProcessBlock)
    # to their values.
    settings_dict = FlattenSettings(settings_dict)

    # If the page units are now different to the plot units, write a
    # message to the .plt file about it so that anyone reading the .plt
    # file can keep track.
    page_units = settings_dict["pageunits"]
    if page_units != plot_units:
        message = ("Plotting this page in " + page_units.upper() +
                   " units, which differ from the plot units.")
    # else:
    #     message = ("Plotting this page in " + page_units.upper() +
    #                ' units.')
        gen.WriteOut("# " + message, plt)

    # Get a list of indices in block triples that hold the start of
    # a valid sub-block of the page block.  These could be graph blocks
    # or verbatim blocks.
    begin_lines = syntax.FindBegins(page_triples)

    # Now get the list of pointers to where each graph on this
    # page starts.
    result = GetBegins(page_triples, begin_lines, "graph",
                        0, math.inf, file_name, debug1, log)
    if result is None:
        return(None)
    else:
        graph_starts = result

    # Get the list of pointers to where each image on this
    # page starts.
    result = GetBegins(page_triples, begin_lines, "image",
                        0, math.inf, file_name, debug1, log)
    if result is None:
        return(None)
    else:
        image_starts = result

    # Make a list of the graph starts and image starts, sort it into
    # ascending order.  Then process each image or graph in sequence.
    # We do this so that users can put graphs over images and images
    # over graphs.
    doodad_starts = graph_starts + image_starts
    doodad_starts.sort()


    # Make a list to hold the names of all the data sources on the graphs,
    # which we put in the page footer for QA purposes.
    sources_list = []

    if doodad_starts == []:
        # There were no graph definitions or images, which is weird but not
        # a fault.  Return.
        return("Completed without plotting anything.")

    # Get the list of graph starts that are not ignored.
    doodads_active = []
    for tr_index in doodad_starts:
        if CheckIgnore(page_triples[tr_index]):
            doodads_active.append(tr_index)

    # Make a slice of the page triples for each doodad (graph or image)
    # and process its lines.
    for (d_index, start) in enumerate(doodad_starts):
        # Generate a slice of the page triples that holds the lines
        # for this doodad.
        # It's the last graph or image on the page.  We set a Boolean
        # that generates the header and footer.
        d_triples = page_triples[start:]
        if start == doodads_active[-1]:
            last_doodad = True
        else:
            last_doodad = False

        if start in graph_starts:
            # Get the graph number for the comment.
            gr_num = graph_starts.index(start) + 1
            if CheckIgnore(d_triples[0]):
                gen.WriteOut('\n#   *' + '-'*20 + ' Start of graph ' + str(gr_num)
                             + ' on page ' + str(page_num) + ' ' + '-'*12, plt)
                settings_dict.__setitem__("graph_num", gr_num)
                # Unset the polygons that Hobyah generated on the
                # previous graph to represent trains, fires, jet fans
                # and suchlike.
                UnsetIcons(plt)
                result = ProcessGraph(d_triples, settings_dict, last_doodad,
                                      sources_list, files_dict, log, plt)
                if result is None:
                    return(None)
                else:
                    sources_list = result
            else:
                # Make a note in the .plt file that this graph is ignored.
                gen.WriteOut('\n#   *' + '-'*20 + ' Ignoring graph ' + str(gr_num)
                             + ' on page ' + str(page_num) + ' ' + '-'*12, plt)
        else:
            # It's an image, not a graph.  Get the image number for the comment.
            img_num = image_starts.index(start) + 1
            if CheckIgnore(d_triples[0]):
                gen.WriteOut('\n#   *' + '-'*20 + ' Start of image ' + str(img_num)
                             + ' on page ' + str(page_num) + ' ' + '-'*12, plt)
                settings_dict.__setitem__("image_num", img_num)
                UnsetIcons(plt)
                result = ProcessImage(d_triples, settings_dict, last_doodad,
                                      sources_list, log, plt)
                if result is None:
                    return(None)
                else:
                    # The name of the image may have been added to the list of
                    # sources (the user has to set an optional argument for this).
                    sources_list = result
            else:
                # Make a note in the .plt file that this image is ignored.
                gen.WriteOut('\n#   *' + '-'*20 + ' Ignoring image ' + str(img_num)
                             + ' on page ' + str(page_num) + ' ' + '-'*12, plt)

    return("Completed after plotting page")


def UnsetIcons(plt):
    '''Write three lines to the gnuplot command file unsetting all
    the object polygons (trains, fires, jet fans), arrows (jet fans)
    and labels (not used yet) that the previous graph may have generated.
    Hobyah's internally-generated objects, arrows and labels are
    all numbered over 2 billion so that most of the number space is
    available to expert users of gnuplot in their verbatim blocks.

         Parameters:
            log             handle          The handle of the logfile.

         Returns:
            None
    '''
    # Note that 'poly_num' is a global variable that tracks the number
    # of the last arrow, object or label that Hobyah generated for its
    # internal annotations.  Its initial value is 2,000,000,000, so
    # we only need to use these lines when it is over that number.
    if poly_num > 2000000000:
        line_part = 'unset for [i=2000000000:' + str(poly_num) + '] '
        for entity in ('object', 'label', 'arrow'):
            gen.WriteOut(line_part + entity + ' i', plt)


def CheckIgnore(line_triple):
    '''Take a line triple that should have something like "begin page"
    or "begin page ignore" (also works for loops, graphs and images).
    If there is a third word on the line triple's data and it is
    "ignore", return False, otherwise return True.

        Parameters:
            line_triples [(int, str, str)]  List of lines in the file.

        Returns:
            False if the third word on the line was "ignore", True
            otherwise.
    '''
    words = line_triple[1].split()
    result = True
    if len(words) > 2 and words[2].lower() == "ignore":
        result = False
    return(result)


def ProcessPlots(line_triples, tr_index, settings_dict, files_dict, log):
    '''Read all the data defining a set of plots (plot settings, definitions
    of pages and definitions of animations).  Call the page/loop creation
    routines to generate the gnuplot files needed to plot the output: one
    or more .plt files and lots of text files (in .csv form) with the
    curve data.

        Parameters:
            line_triples [(int, str, str)]  List of lines in the file.
            tr_index        int             Pointer to where the "plots"
                                            block starts in line_triples.
            settings_dict   {}              Dictionary of the run settings.
            files_dict      {}              The dictionary of file names
                                            and their nicknames.
            log             handle          The handle of the logfile.

        Returns:
            String if successful, None if not.

        Errors:
            Aborts with 6001 if the gnuplot file (.plt file) in the
            ancillaries folder cannot be written to.
            Aborts with 6002 if the gnuplot program is not installed
            on this computer.
            Aborts with 6003 if the "images" subfolder cannot be
            created.
            Aborts with 6004 if the "images" subfolder exists but cannot
            be written to.
            Complains (but does not abort) with error 6005 if the
            ImageMagick program is not installed on this computer.
    '''

    # Break out the various settings we will need here.
    run_units = settings_dict["units"]

    file_name = settings_dict["file_name"]
    debug1 = settings_dict["debug1"]

    dir_name = settings_dict["dir_name"]
    file_stem = settings_dict["file_stem"]

    user_data_dict =  settings_dict["user_data"]

    # Store the index of the line with "begin plots", we may need it
    # later.
    begin_index = tr_index

    # Set the default system of units to plot in (same as the run units).
    settings_dict.__setitem__("plotunits", run_units)

    # Set the default page size and orientation, A4 landscape.  The
    # user can change these at the plot level.
    settings_dict.__setitem__("pagesize", "a4")
    settings_dict.__setitem__("orientation", "default")

    # By default, the program puts all the pages of a block in one pdf
    # file (note there are different .pdf files for pages and loops).
    # We can tell the program to write individual .pdf files for the
    # pages (but not for the loops).  This is useful for pdf files
    # that are to be imported into LaTeX files as Figures.
    settings_dict.__setitem__("splitpages", "false")


    # Set the default graph margins.  These defaults leave a good
    # margin around one graph on an A4 page and make the graph's aspect
    # ratio on an landscape ISO page not too far from the golden ratio
    # (~1:1.618), a size that human brains find restful for some reason.
    settings_dict.__setitem__("baselmargin", 0.13)
    settings_dict.__setitem__("basermargin", 0.885)
    settings_dict.__setitem__("basebmargin", 0.17)
    settings_dict.__setitem__("basetmargin", 0.83)

    # These are the default entries in gnuplot's "set terminal" command.
    settings_dict.__setitem__("terminal", "pdfcairo")
    settings_dict.__setitem__("linewidth", 1)

    if sys.platform == 'win32':
        # Sans is what my old Windows laptop replaces Helvetica with,
        # might as well use it directly.
        settings_dict.__setitem__("font", "Sans")
        # Default text size is 16 points for Sans on Windows
        settings_dict.__setitem__("fontsize", 16)
    else:
        # settings_dict.__setitem__("font", "Helvetica")
        settings_dict.__setitem__("font", "Sans")
        # Default text size is 18 points for Sans on macOS.
        settings_dict.__setitem__("fontsize", 18)

    # After we create the .pdf file we may be able to use ImageMagick's
    # "convert" function to turn each page of the .pdf file into a .png
    # file in a subfolder named "images".  ImageMagick takes a bit of
    # time so we want this to be off by default.  When the user sets
    # "pngtrim" to True or False it tells Hobyah to generate the .png
    # files.
    settings_dict.__setitem__("pngtrim", "do not create")


    # Get any page settings.  Most should be self-explanatory.  The
    # base margins are the ones that will be set every time a new
    # page is begun in the gnuplot file.
    valid_settings = {"plotunits": (("si", "us"),),
                      "pagesize":  (("a4", "a3",
                                     "letter", "tabloid", "ledger",
                                     "none", "custom"), "QAstr"),
                      "orientation": (("landscape", "portrait"),),
                      "terminal": (("QAstr"),),
                      "font": (("QAstr"),),
                      "fontsize": (("int  0+  null  a font size"),),
                      "linewidth": (("int 0+  null  a line width"),),
                      "basemargins": ("float any null the graph left edge",
                                      "float any null the graph right edge",
                                      "float any null the graph bottom edge",
                                      "float any null the graph top edge",),
                      "pngtrim":  (("true", "false"),),
                      "splitpages":  (("true", "false"),),
                      "#skip": "discard"  # This catches all other lines
                     }
    # We make a list of entries that we must have (none).
    requireds = []
    # We make a dictionary of the optional keywords that accepts any
    # optional value.  There are two active ones for the 'pngtrim'
    # keyword here, here, the dpi (default is 300 dpi) and whether to
    # limit the pallete to 256 colours in the pngs so that it is easier
    # to create .gif files from a series of .png files with ffmpeg.

    # We also spoof one for pagesize, because if the user sets a
    # custom page size they may have an optional entry for which units
    # to use (cm, mm, in, pt).  This is ignored here and processed
    # in a second call below.
    optionals = {"pngtrim": {"dpi":  ("int  0+  null  the dpi in png files"),
                             "targetgif":  ("true", "false"),
                            },
                 "pagesize": {"units": "#name"},
                }
    # We don't allow any duplicates.
    duplicates = ()
    settings = (valid_settings, requireds, optionals, duplicates)
    block_name = "plots"
    # Now call ProcessBlock.  It returns None if an error occurred.  It
    # returns a dictionary of the entries in "valid_settings" (if any
    # such entries exist in the file).
    result = ProcessBlock(line_triples, tr_index, settings_dict,
                          block_name, settings_dict, settings, log)
    if result is None:
        return(None)
    else:
        (discard, settings_dict) = result
        # Check if the user set a custom page size.  If they did,
        # change the requirements to handle the custom page size and
        # call again.
        if settings_dict["pagesize"][0] == "custom":
            valid_settings.__setitem__("pagesize",  (("custom",),
                                                     ("float +  null  a page width"),
                                                     ("float +  null  a page height"),
                                                    )
                                      )
            # We make a dictionary of the optional keywords that accepts any
            # optional value.  There is one active one here, the dpi (default is
            # 300 dpi).  We also spoof one for pagesize, because if the user
            # sets a custom page size they may have an optional entry for which
            # units to use (cm, mm, in, pt).  This is ignored here and processed
            # in a second call below.
            optionals.__setitem__("pagesize", {"units": ("cm", "mm",
                                                         "in", "inch", "inches",
                                                         "pt")})
            settings = (valid_settings, requireds, optionals, duplicates)
            result = ProcessBlock(line_triples, tr_index, settings_dict,
                                  block_name, settings_dict, settings, log)
            if result is None:
                return(None)
            else:
                # Now we have the custom page size as two numbers and the
                # units as an optional argument.
                (discard, settings_dict) = result
                # Build text for a custom page size.
                custom_p = settings_dict["pagesize"]
                try:
                    # See if the user set a custom unit.  We take the first
                    # two characters so that "inch" and "inches" get
                    # truncated to "in", which is what gnuplot accepts.
                    # The other current valid types are two characters.
                    unit = custom_p[-2]["units"][:2]
                except KeyError:
                    # The user didn't set an optional entry for the units.
                    # Use centimetres as the default units.
                    unit = "cm"

                size_text = "#custom " + str(custom_p[1]) + unit + ", " \
                                       + str(custom_p[2]) + unit + " "
                settings_dict.__setitem__("pagesize", size_text)

        # Store the contents of the pngtrim entry, in case the user
        # set it.  We store it here to preserve the optional entry
        # for dpi.
        png_setting = settings_dict["pngtrim"]
        # Check if the user had an entry for basemargins.  If they
        # did, populate the four "basemargin" entries with the values
        # the user set and
        if "basemargins" in settings_dict:
            left, right, bottom, top = settings_dict["basemargins"][:4]
            settings_dict.__setitem__("baselmargin", left)
            settings_dict.__setitem__("basermargin", right)
            settings_dict.__setitem__("basebmargin", bottom)
            settings_dict.__setitem__("basetmargin", top)
            defmarginscomment = " # User's 'basemargins' entries"
        else:
            # Use the default margins set in the code above.
            defmarginscomment = " # Default graph margins"
        # Store the comment about whether the default graph margins
        # were set by the user or are the, so that the comment appears
        # in the .plt files.
        settings_dict.__setitem__("defmarginscomment", defmarginscomment)



    # Flatten any new settings from tuples (as returned by ProcessBlock)
    # to their values.  This destroys the four entries in basemargins
    # but we no longer need them.
    settings_dict = FlattenSettings(settings_dict)

    # Make a slice of line_triples from "begin plots" to "end plots".
    block_triples = line_triples[begin_index:]
    # Get a list of indices in block triples that hold the start of
    # a valid sub-block of the plots block.
    begin_lines = syntax.FindBegins(block_triples)

    # Now get the list of pointers to where each page block starts.
    result = GetBegins(block_triples, begin_lines, "page",
                        0, math.inf, file_name, debug1, log)
    if result is None:
        return(None)
    else:
        page_starts = result

    # Get the list of pointers to where each timeloop block starts.
    # A timeloop is one page plotted at multiple different times,
    # intended to be turned into an animation.
    result = GetBegins(block_triples, begin_lines, "timeloop",
                        0, math.inf, file_name, debug1, log)
    if result is None:
        return(None)
    else:
        tloop_starts = result

    # Get the list of pointers to where each filesloop block starts.
    # A filesloop is one page plotting the same graphs for multiple
    # files, intended to be used where we have dozens of similar
    # files (e.g. SES fire simulations with the trains and fire in
    # different locations) and we want broadly similar graphs for
    # each run.
    result = GetBegins(block_triples, begin_lines, "filesloop",
                        0, math.inf, file_name, debug1, log)
    if result is None:
        return(None)
    else:
        floop_starts = result

    if page_starts == [] and tloop_starts == [] and floop_starts == []:
        # There were no page or loop definitions, this is probably
        # a test file.  Return without creating a .plt file.
        if debug1:
            print("Found no pages or loops to plot")
        return("Completed without plotting anything")

    # Make a list of page and loop starts so we can distinguish them
    # when they are interleaved.
    pageloop_starts = page_starts + tloop_starts + floop_starts
    pageloop_starts.sort()

    if debug1:
        print("Found these pages:", page_starts)
        print("Found these timeloops:", tloop_starts)
        print("Found these filesloops:", floop_starts)


    # If we get to here we have either page definitions or loop
    # definitions in the plot block.  Get the path to the ancillaries
    # folder.
    ancill_path = dir_name + "ancillaries/"
    settings_dict.__setitem__("ancill_path", ancill_path)

    # Make a list of the names of all the .plt files we need gnuplot to
    # process.  We have one for all the "begin page" blocks and one for
    # each of the "begin loop" blocks.
    plt_list = []

    # Define counts of pages for each of the pdf files we create (one
    # pdf file for all the pages and one for each loop).
    pdf_pairs = []

    # Now process the pages in the plots block.
    if page_starts != []:
        # Process the pages, generate one .plt file and a set of text files
        # that hold the curve data.
        # First check if we can open the .plt file to write to it.  We
        # know we have permission to write to the "ancillaries" folder
        # because we tested that earlier, at error 2004.
        plt_name = file_stem + ".plt"
        result = OpenPltFile(settings_dict, plt_name, log)
        if result is None:
            return(None)
        else:
            plt = result

        if settings_dict["splitpages"] == "true":
            # We want individual files for each page of output.
            pagecount = str(max(3, len(str(len(page_starts)))))
            num_format = '{:0>' + pagecount + '}'
            pdf_name = file_stem + "-p" + num_format.format(1) + ".pdf"
        else:
            pdf_name = file_stem + ".pdf"
            num_format = 'We should not be able to use this string in .format()'
        settings_dict.__setitem__("num_format", num_format)
        # Write the QA data and .plt file initialization data at the
        # top of the .plt file.  We do this in a separate procedure
        # so that we can call it when we start a .plt file for an
        # animation.
        WritePltStart(settings_dict, pdf_name, log, plt)

        # Write out any verbatim blocks at the "plots" level (they
        # may alter the terminal, size settings and the output
        # file name).
        pass # Need to remember to add this capability here.

        # Now process all the pages (we have at least one).
        if debug1:
            print("Processing pages")
            print(page_starts)
        for (page_index, start) in enumerate(page_starts):
            # Get the page number.
            page_num = page_index + 1
            if debug1:
                print("Handling page " + str(page_num) + " starting at", start)
            if start == pageloop_starts[-1]:
                # We are at the last page definition or loop definition.
                page_triples = block_triples[start:]
            else:
                # Take a slice of line_triples that covers this page.
                # We need to figure out the entry after the location of
                # "start" in pageloop_starts (this is not elegant code).
                pl_index = pageloop_starts.index(start) + 1
                end = pageloop_starts[pl_index]
                page_triples = block_triples[start:end]
            # Check if the "begin page", command was "begin page ignore".
            if CheckIgnore(page_triples[0]):
                # We are not ignoring this page.
                # Write the default settings, one graph filling one page.
                # We write this out after we start each new page.
                ResetPage(page_num, settings_dict, "pages", file_stem, plt)
                # Store the page number in settings_dict.
                settings_dict.__setitem__("page_num", page_num)
                result = ProcessPage(page_triples, settings_dict, files_dict,
                                     "page", log, plt)
                if debug1:
                    print("Page returned: ", result)
                if result is None:
                    plt.close()
                    return(None)
            else:
                # Skip over this page.
                gen.WriteOut('\n#   *' + '-'*20 + ' Ignoring page '
                             + str(page_num) + ' ' + '-'*20, plt)
        plt.close()
        plt_list.append(plt_name)
        pdf_pairs.append((pdf_name[:-4], page_num))

    if tloop_starts != []:
        # Process the timeloops.  These generate a different .plt file
        # for each timeloop, so the .plt file is opened in a procedure.
        if debug1:
            print("Processing timeloops")
            print(tloop_starts)
        for (loop_index, start) in enumerate(tloop_starts):
            # Get the page number.
            loop_num = loop_index + 1
            if debug1:
                print("Handling timeloop " + str(loop_num)
                      + " starting at", start)
            if start == pageloop_starts[-1]:
                # We are at the last page.
                loop_triples = block_triples[start:]
            else:
                # Take a slice of line_triples that covers this loop.
                pl_index = pageloop_starts.index(start) + 1
                end = pageloop_starts[pl_index]
                loop_triples = block_triples[start:end]
            # Check if the "begin timeloop", command was "begin timeloop ignore".
            if CheckIgnore(loop_triples[0]):
                # We are not ignoring this loop.
                if debug1:
                    line_number = loop_triples[0][0]
                    mess = ("Processing animation " + str(loop_num)
                            + " at line " + line_number + ".")
                    gen.WriteMessage2(mess, log)
                settings_dict.__setitem__("loop_num", loop_num)
                result = ProcessTimeLoop(loop_triples, settings_dict,
                                         files_dict, log)
                if debug1:
                    print("Timeloop returned: ", result)
                if result is None:
                    return(None)
                else:
                    # The routine returned the name of a .plt file.  Add it
                    # to the list of .plt files to process.
                    (plt_name, anim_pdf_name, frame_count) = result
                    plt_list.append(plt_name)
                    pdf_pairs.append((anim_pdf_name[:-4], frame_count))
            else:
                if debug1:
                    line_number = str(loop_triples[0][0])
                    mess = ("Ignoring animation " + str(loop_num)
                            + " at line " + line_number + ".")
                    gen.WriteMessage2(mess, log)
    if floop_starts != []:
        # Process the filesloops.
        if debug1:
            print("Processing filesloops")
            print(floop_starts)

        try:
            old_loops = settings_dict["loop_num"]
        except KeyError:
            # We have a file with filesloops but no timeloops.  The
            # counter has not been set yet.
            old_loops = 1

        for (loop_index, start) in enumerate(floop_starts):
            # Get the page number.
            loop_num = loop_index + old_loops
            if debug1:
                print("Handling filesloop " + str(loop_num)
                      + " starting at", start)
            if start == pageloop_starts[-1]:
                # We are at the last page.
                loop_triples = block_triples[start:]
            else:
                # Take a slice of line_triples that covers this loop.
                pl_index = pageloop_starts.index(start) + 1
                end = pageloop_starts[pl_index]
                loop_triples = block_triples[start:end]
            # Check if the "begin filesloop", command was "begin filesloop ignore".
            if CheckIgnore(loop_triples[0]):
                # We are not ignoring this loop.
                if debug1:
                    line_number = str(loop_triples[0][0])
                    mess = ("Processing filesloop " + str(loop_num)
                            + " at line " + line_number + ".")
                    gen.WriteMessage2(mess, log)
                settings_dict.__setitem__("loop_num", loop_num)
                result = ProcessFilesLoop(loop_triples, settings_dict,
                                         files_dict, log)
                if debug1:
                    print("Filesloop returned: ", result)
                if result is None:
                    return(None)
                else:
                    # The routine returned the name of a .plt file.  Add it
                    # to the list of .plt files to process.
                    (plt_name, anim_pdf_name, frame_count) = result
                    plt_list.append(plt_name)
                    pdf_pairs.append((anim_pdf_name[:-4], frame_count))
            else:
                if debug1:
                    line_number = str(loop_triples[0][0])
                    mess = ("Ignoring filesloop " + str(loop_num)
                            + " at line " + line_number + ".")
                    gen.WriteMessage2(mess, log)
    if debug1:
        print("List of .plt files:", plt_list)
    if plt_list != []:
        if len(plt_list) == 1:
            print("> Trying to call gnuplot")
        else:
            print("> Trying to call gnuplot " + str(len(plt_list))
                  + " times.")
    for plt_index, plt_name in enumerate(plt_list):
        command_list = ("gnuplot", plt_name.replace("\\", "/"))
        # A boolean that we set true if gnuplot is installed.
        has_gnuplot = False
        has_line_num = False

        # Try to call gnuplot to convert the .plt file into graphs in pdf.
        try:
            if sys.platform == 'win32':
                result = subprocess.Popen(command_list, cwd = ancill_path,
                                          stdout = subprocess.PIPE,
                                          stderr = subprocess.STDOUT,
                                          shell = True)
            else:
                result = subprocess.Popen(command_list, cwd = ancill_path,
                                          stdout = subprocess.PIPE,
                                          stderr = subprocess.STDOUT,
                                          shell = False)
            # If we get to here then Gnuplot is installed on this machine.
            # Get the output from gnuplot as it prints it.  Take off the
            # junk at the start and end and print it.  We do some investigation
            # of the contents of the transcript:
            #  * We look for a generic line warning about stuff.  This might
            #    be OK or a serious problem, so we waffle.
            #  * We look for a caret (^) in the lines (this usually means
            #    that gnuplot is quoting a line in the .plt file and pointing
            #    to where it failed).
            has_caret = False
            gnuplot_warn = False
            # Set the line number so that if we get an error message with
            # a caret and can't find the line number, it raises error 1102
            # in gen.Enth().
            line_num = -1
            while True:
                line = result.stdout.readline()
                if not line:
                    break
                line_mod = str(line)[2:-3]
                print(line_mod)
                # Check if gnuplot warned about something.  Many of these
                # warnings are useful but not fatal.
                if line_mod[:8] == "Warning:":
                    gnuplot_warn = True

                # Check if the line is blank except for a caret (^).  If it
                # is, it means that gnuplot is complaining about a point on
                # a line in the .plt file.
                if has_caret:
                    # This is the line after the line with the caret.  We
                    # check if it has the name of the .plt file, some text
                    # and a line number, as in the following:
                    for seek in ('"' + plt_name + '", line ',
                                 '"' + plt_name + '" line '):
                        if line_mod[:len(seek)] == seek:
                            parts = line_mod.split(sep = seek)
                            line_num = int(parts[1].split(sep = ":")[0])
                            has_caret = False
                            has_line_num = True
                if line_mod.strip() == "^":
                    has_caret = True
                sys.stdout.flush()
            # Get the return code from gnuplot.  0 is OK, 1 means it
            # complained about something.
            retval = result.wait()
        except FileNotFoundError:
            err = ('> Ugh, it looks like gnuplot is not installed\n'
                   '> on this computer.  Hobyah needs gnuplot (a\n'
                   '> free and capable plotting program) to create\n'
                   '> pdf output.\n'
                   '> If you have an IT department, please ask them\n'
                   '> to install it for you then try again.  If you\n'
                   '> do not have an IT department and have to install\n'
                   '> it for yourself, good luck!  See section 1.8\n'
                   '> of the Hobyah User Guide for details of how\n'
                   '> I did it.'
                   )
            gen.WriteError(6002, err, log)
            # We don't return None here, we keep going.  But we make
            # a note that we don't have gnuplot so that later messages
            # don't give the user the impression that the .pdf file
            # was created.
        except Exception as err:
            # Something else occurred.  Print the basics of the error
            # and (if debug1 is True) print the full traceback too.
                err_intro = ('The call to gnuplot returned with the '
                         'following exception:')
                gen.UnexpectedException(err_intro, err, debug1, log)
        else:
            # We do have gnuplot installed.  Set the flag that adjusts
            # future error messages accordingly and get the ordinal of
            # the call to gnuplot (1st, 2nd, 3rd etc.).
            has_gnuplot = True
            pltth = gen.Enth(plt_index + 1)

            if retval == 0:
                if gnuplot_warn:
                    mess = ("Gnuplot issued warning(s) for the " + pltth \
                            + " call, but no fatal errors.")
                    gen.WriteMessage2(mess, log)
                else:
                    mess = "Gnuplot returned zero for the " + pltth \
                            + " call, so all's well."
                    gen.WriteMessage2(mess, log)
            else:
                if has_line_num is True:
                    # The transcript included a line with nothing on it except
                    # spaces and a caret, followed by a line with the file name
                    # and the line number where it found a syntax error.  This
                    # is fragile, and may break at any time.
                    mess = ("gnuplot returned with the following: "
                              + str(retval) + ", which means that it",
                            "encountered a problem.  The error message above gives the",
                            "details; the caret (^) points to where gnuplot spotted the",
                            "problem: the cause is likely to be to the left of it.  The",
                            "best way to solve these kinds of problems is to navigate",
                            'to the ancillaries folder, edit the file',
                            '   "' + plt_name + '"',
                            "in a text editor, then jump to the "
                            + gen.Enth(line_num) + " line of the file.",
                            "Some lines in the .plt file have comments at the end like",
                            '"# From line 336".  These are tags added by Hobyah that',
                            'tell which the line of the Hobyah input file that line',
                            "was generated from.  You can use the tags to figure out",
                            "which line of the Hobyah input file generated the flawed",
                            "line in the .plt file, then use gnuplot's error message",
                            "to fix up the problem.",)
                else:
                    # It was not complaining about stuff on a specific line.
                    mess = ("gnuplot returned with the following: "
                              + str(retval) + ", which means that it",
                            "encountered a problem.  The message above gives all the",
                            "detail that is available.")
                for line in mess:
                    gen.WriteMessage2(line, log)

    # Check if the user wants .png files of the pages and if they do,
    # create them.
    if settings_dict["pngtrim"] != "do not create" and has_gnuplot:
        # The user must have set an entry that tells whether to trim
        # the .png files or not, so they wanted the files created.
        # Figure out whether we need to add a trim argument or not
        # and whether the user set the dpi in an option.  We know that
        # this user entry is a tuple because we checked it.
        if png_setting[0].lower() == "true":
            pngtrim = True
        else:
            pngtrim = False


        # If the user set an optional entry for the dpi, use it.
        if "dpi" in png_setting[-2]:
            # We use "str(int())" to strip out any leading zeros.
            dpi = str(int(png_setting[1]["dpi"]))
            if dpi == "0":
                # The user set an optional entry for the dpi, but they
                # set it to zero.  Use 300.  If you're reading this,
                # shame on you.
                dpi = "300"
        else:
            # The user did not set an optional entry for the dpi.  Use 300.
            dpi = "300"

        # Check if the user set an optional entry that makes the .png
        # files more suitable for conversion into .gif files with
        # ffmpeg (they have fewer colours).
        if ("targetgif" in png_setting[-2] and
            png_setting[-2]["targetgif"] == "true"):
            targetgif = True
        else:
            targetgif = False

        for (pdf_stem, pages) in pdf_pairs:
            if pages == 1:
                print('> Trying to convert one .pdf page to a .png file.')
            else:
                print('> Trying to convert', pages,
                        '.pdf pages to .png files.')
            images_path = dir_name + "images"
            writeable = True
            if not os.access(images_path, os.F_OK):
                try:
                    os.mkdir(images_path)
                except PermissionError:
                        err = ('> Skipping the conversion of pdf pages to .png\n'
                               '> files in "' + file_name + '", because you do not\n'
                               '> have permission to create the "images" subfolder.')
                        gen.WriteError(6003, err, log)
                        writeable = False
            elif not os.access(images_path, os.W_OK):
                err = ('> Skipping the conversion of pages to images in\n'
                       '> "' + file_name + '", because you do not have\n'
                       '> permission to write to the "images" subfolder.')
                gen.WriteError(6004, err, log)
                writeable = False


            if writeable:
                # Convert the pages to images.  First we figure out how
                # many pages we have and give an informative message about
                # how long it will take (the conversion may take some time).
                # Also, we figure out how wide we need to make the number
                # string at the end of the filenames.  If we have 1000 pages
                # or more we will need to tell ImageMagick to make the number
                # string "-p%04d" or more instead of the default ("-p%03d").
                num_width = str(max(3, len(str(pages))))
                if pages < 5:
                    print("> This will probably take a few seconds...")
                elif pages < 10:
                    print("> This will probably take about ten seconds...")
                elif pages < 50:
                    print("> This will probably take about a minute...")
                elif pages < 100:
                    print("> This will probably take a few minutes...")
                else:
                    print("> This will take a LONG time.  Now would be a\n"
                          '> good time to go and do something productive\n'
                          '> instead of sitting here watching the screen.\n'
                          '>\n'
                          '> If you are testing out the capabilities of the\n'
                          '> .png conversion feature, consider testing it\n'
                          '> with a file that creates fewer pages than\n'
                          '> this file creates.\n'
                          '>')
                if pages >= 10:
                    print('> Press Ctrl+C at any time to break out of\n'
                          "> the conversion process.  Don't complain\n"
                          "> that you weren't warned...")


                # Create a list to hold all the arguments for ImageMagick's
                # "convert" command.  We use 'magick convert' instead of
                # 'convert' to be sure that it works even if someone
                # installs ImageMagick without checking the "legacy support"
                # box on Windows.
                command_list = ['magick', 'convert']

                # Add commands for the optional entries on the line.
                if pngtrim:
                    # Cut off all whitespace in the border.
                    command_list.append('-trim')
                if targetgif:
                    # The user has indicaed that these png files
                    # will be compiled into a gif file, so we want
                    # to reduce the palette to 256 colours.  Full
                    # disclosure: this doesn't use the same 256
                    # colours on every page (I don't know how to
                    # force ImageMagick to do that).  Instead we
                    # end up with about eight hundred colours that
                    # ffmpeg can more easily knock down to 256 when
                    # it creates the .gif file.x
                    pass#command_list.append('set colorspace RGB -depth 8')


                # Add the rest of the arguments to the convert command.
                command_list.extend([ '-density', dpi,
                                      dir_name + pdf_stem + ".pdf",
                                      '-background', 'white',
                                      # The .png filenames will be something like
                                      # foo-p001.png, foo-p002.png etc.
                                      # The "-scene" option below tells it to
                                      # start the numbering at 1 (the default is
                                      # to start the numbering at zero).
                                      '-scene', '1',
                                      pdf_stem + '-p%0' + num_width + 'd.png']
                                    )

                try:
                    if sys.platform == 'win32':
                        result = subprocess.Popen(command_list, cwd = images_path,
                                                  stdout = subprocess.PIPE,
                                                  stderr = subprocess.STDOUT,
                                                  shell = True)
                    else:
                        result = subprocess.Popen(command_list, cwd = images_path,
                                                  stdout = subprocess.PIPE,
                                                  stderr = subprocess.STDOUT,
                                                  shell = False)
                    retval = result.wait()
                except FileNotFoundError:
                    # ImageMagick is not installed on this machine.
                    # Generate an ImageMagick command that can be used on
                    # another machine.
                    # First we replace the source filename and path in
                    # command_list with the source filename only.
                    for index, entry in enumerate(command_list):
                        if pdf_stem + ".pdf" in entry:
                            command_list[index] = pdf_stem + ".pdf"
                            break
                    err = ('> Ugh, it looks like ImageMagick is not installed\n'
                           '> on this computer.  Hobyah uses ImageMagick\n'
                           '> (an open source image conversion program) to\n'
                           '> convert pages of pdf output to .png images.\n'
                           "> This is not a fatal error; Hobyah will keep\n"
                           "> going, but won't be able to generate .png images\n"
                           '> from the pages of the pdf (which is handy when\n'
                           '> you need to include images in reports).\n'
                           '> If you have an IT department, please ask them\n'
                           '> to install it for you then try again.  If you\n'
                           '> do not have an IT department and have to do\n'
                           '> it for yourself, good luck!  See the Hobyah\n'
                           '> User Guide for details of how I installed it.\n'
                           '> If you have another machine that does have\n'
                           '> it installed, you can take the .pdf file to\n'
                           '> that machine, fire up a Terminal or command\n'
                           '> window, navigate to the folder that contains\n'
                           '> the .pdf file and issue the following command:\n\n'
                            + ' '.join(word for word in command_list)
                           )
                    gen.WriteError(6005, err, log)
                    # We don't return None here, we keep going.
                except Exception as err:
                    # Something else occurred.  Print the basics of the error
                    # but not the full traceback.
                    err_intro = ('The call to ImageMagick returned with the'
                                 'following exception:')
                    gen.UnexpectedException(err_intro, err, debug1, log)
                else:
                    if debug1:
                        print("ImageMagick returned with the following:", retval)
                # This is a convenient place to store some command-line
                # arguments for converting to .png with ImageMagick and
                # converting to video files with ffmpeg.
                # Use ffmpeg on the command line:
                #
                #    ffmpeg -i imagesfoo-lp1-p%03d.png  test2.mp4
                #
                # The '%03d' means "a number with three digits in it".
                # Check how many are used in your image files: if you have
                # between 1 and 999 image files it will have three digits,
                # 1000 up to 9999 is four digits, and so on.
    # Send back a non-None result so that we keep processing.
    return("Completed without fatal errors")


def OpenPltFile(settings_dict, plt_name, log):
    '''Open a new .plt file and complain if we can't get write access to it.
    We do this in its own routine because it can be called when the plots
    block is processed and each time a new loop block is encountered.

        Parameters:
            settings_dict   {}              Dictionary of the run settings.
            plt_name        str             The name of the plt file to write to.
            log             handle          The handle of the logfile.

        Returns:
            plt             handle          The handle of the gnuplot file.
    '''
    ancill_path = settings_dict["ancill_path"]
    file_name =  settings_dict["file_name"]
    try:
        plt = open(ancill_path + plt_name, 'w', encoding='utf-8')
    except PermissionError:
        err = ('> Skipping "' + file_name + '", because\n'
               "> you do not have permission to write to\n"
               '> its gnuplot file "' + plt_name + '" in\n'
               '> the "ancillaries" folder.')
        gen.WriteError(6001, err, log)
        return(None)
    return(plt)


def WritePltStart(settings_dict, pdf_name, log, plt):
    '''Write a list of lines to the .plt file.  These define the
    setting at the start of a .plt file, such as the terminal type to use,
    the name of the pdf file to write to, page size, custom linetypes
    and some QA data.

        Parameters:
            settings_dict   {}              Dictionary of the run settings.
            pdf_name        str             The name of the pdf file to write to.
            log             handle          The handle of the logfile.
            plt             handle          The handle of the gnuplot file.

        Returns:
            None
    '''
    # Break out the various settings we will need here.
    run_units = settings_dict["units"]

    file_name = settings_dict["file_name"]
    debug1 = settings_dict["debug1"]

    dir_name = settings_dict["dir_name"]
    file_stem = settings_dict["file_stem"]

    when_who = settings_dict["when_who"]
    qa1 = settings_dict["qa1"]
    qa2 = settings_dict["qa2"]
    qa3 = settings_dict["qa3"]
    file_comments = settings_dict["file_comments"]

    user_data_dict =  settings_dict["user_data"]

    # Create a QA string that forms the header of the plot files
    # and any curve definition files.
    # Program, user name, date and time, project QA data.
    QAlist1 = ['# Hobyah source file, "' + file_name + '"',
               '# Timestamp & user, "' + when_who + '"',
               '# Project number, "' + qa1.replace('#','') + '"',
               '# Project name, "' + qa2.replace('#','') + '"',
               '# Project description, "' + qa3.replace('#','') + '"'
              ]

    # Write a header of QA lines to the .plt file.  .plt files
    # use "#" for the comment character, just like the input files
    # and Python.  Then store it in settings_dict so we can put
    # it into the files that hold the columns of plot data.
    for headerline in QAlist1:
        gen.WriteOut(headerline, plt)
    settings_dict.__setitem__("QAlist1", QAlist1)

    # Write a message about which units we are plotting in to the
    # dictionary.  A simple message if the input units and the plot
    # units are the same, and a more complex message if they differ.
    plot_units = settings_dict["plotunits"]
    if plot_units != run_units:
        message = ("Plotting in " + plot_units.upper() + " units, which "
                   "differ from the input units.")
    else:
        message = "Plotting in " + plot_units.upper() + " units."
    gen.WriteOut("# " + message, plt)
    # Get the page size and orientation.
    if settings_dict["pagesize"] == "a4":
        if settings_dict["orientation"] in ("landscape", "default"):
            sizetext = "29.7cm, 21cm "
        else:
            sizetext = "21cm, 29.7cm "
    elif settings_dict["pagesize"] == "a3":
        if settings_dict["orientation"] in ("landscape", "default"):
            sizetext = "42cm, 29.7cm "
        else:
            sizetext = "29.7cm, 42cm "
    elif settings_dict["pagesize"] ==  "letter":
        # Note that this is the ANSI/ASME Y14.1 "ANSI A"
        # letter size, not the older letter size that was
        # 10.5" instead of 11".
        if settings_dict["orientation"] in ("landscape", "default"):
            sizetext = "11in, 8.5in "
        else:
            sizetext = "8.5in, 11in "
    elif settings_dict["pagesize"] == "ledger":
        # This is the ANSI/ASME Y14.1 "ANSI B" size.  A few web
        # sites indicate that ledger is the page in landscape mode
        # and tabloid is the page in portrait mode.  We'll use
        # landscape mode for ledger if the user didn't set an
        # orientation.
        if settings_dict["orientation"] in ("landscape", "default"):
            sizetext = "17in, 11in "
        else:
            sizetext = "11in, 17in "
    elif settings_dict["pagesize"] == "tabloid":
        # This is the ANSI/ASME Y14.1 "ANSI B" size, and the name
        # "tabloid" for it appears to mean it is in portrait mode.
        if settings_dict["orientation"] == "landscape":
            sizetext = "17in, 11in "
        else:
            sizetext = "11in, 17in "
    elif settings_dict["pagesize"][:7] == "#custom":
        # The user set a custom page size.  We ignore the orientation
        # setting and use the custom text, which is after the word
        # "#custom " in settings_dict["pagesize"].  We have already
        # checked that it is valid (I hope).
        sizetext = settings_dict["pagesize"][7:]
    elif settings_dict["pagesize"] == "none":
        # Spoof the size text so that we don't write anything to set
        # the terminal.  The user needs to set the terminal in a
        # "verbatim" block in the first block (graph block or image block).
        sizetext = "none"
    else:
        # We will get to here when we add new page sizes in the
        # valid_settings dictionary at the top of this procedure but
        # failed to cater for them here.
        mess = "Fouled up adding new page sizes in ProcessPlots."
        gen.WriteMessage(mess, log)
        gen.OopsIDidItAgain(log)
        return(None)

    # Check if the user set "pagesize none".  If they did, comment out
    # the set terminal, set output and set multiplot commands in the
    # .plt file.
    if sizetext == "none":
        prefix = "# "
        gen.WriteOut('\n' + '# *---     The output file, terminal type '
                     'and multiplot are    ---*\n'
                     '# *---   all commented out because "pagesize none" '
                     'was used.   ---*', plt)
    else:
        prefix = ""
        gen.WriteOut('\n', plt)
    # Set the terminal as a .pdf file in the same directory
    # as the input file.  Set the default linewidth, size
    # and typeface.  If users want more complex settings than
    # these, they can use the "pagesize none" option and a
    # verbatim command.
    gen.WriteOut(prefix + 'set terminal '
                 + str(settings_dict["terminal"])
                 + ' size ' + sizetext
                 + 'linewidth ' + str(settings_dict["linewidth"])
                 + ' font "'    + settings_dict["font"] + ', '
                 + str(settings_dict["fontsize"])
                 + '" enhanced', plt
                )
    gen.WriteOut(prefix + 'set output "' + dir_name.replace("\\", "/")
                 + pdf_name + '"', plt)
    gen.WriteOut(prefix + 'set multiplot', plt)

    if sizetext == "none":
        # The user has set the "pagesize none" option.  Make three variables
        # with the default output file name and path so that users can
        # access them in their verbatim blocks.
        gen.WriteOut('outpath = "' + dir_name.replace("\\", "/"), plt)
        gen.WriteOut('outname = "' + pdf_name + '"', plt)
        gen.WriteOut('outstem = "' + pdf_name[:-3] + '"', plt)

    # Now define 18 line types of different colours.  The first nine
    # are width 2 and linetype 1 (solid), the last nine repeat the
    # same colours but have linetype 4 (dash-dot) instead.
    # The colours are taken from the colours defined by gnuplot v5 for
    # the pdfcairo terminal (you run gnuplot, tell it "set term pdfcairo"
    # then tell it "show palette colornames" to see a list of the names
    # and their RGB values.

                # Colour name           Width    linetype
    curves = ( ('rgb "web-blue"       ', 2,        1),
               ('rgb "red"            ', 2,        1),
               ('rgb "light-green"    ', 2,        1),
               ('rgb "black"          ', 2,        1),
               ('rgb "goldenrod"      ', 2,        1),  # 5
               ('rgb "dark-grey"      ', 2,        1),
               ('rgb "brown"          ', 2,        1),
               ('rgb "dark-chartreuse"', 2,        1),
               ('rgb "cyan"           ', 2,        1),
               ('rgb "web-blue"       ', 2,        4),  #10
               ('rgb "red"            ', 2,        4),
               ('rgb "light-green"    ', 2,        4),
               ('rgb "black"          ', 2,        4),
               ('rgb "goldenrod"      ', 2,        4),
               ('rgb "dark-grey"      ', 2,        4),  #15
               ('rgb "brown"          ', 2,        4),
               ('rgb "dark-chartreuse"', 2,        4),
               ('rgb "cyan"           ', 2,        4),
               # Set line type 19 to be the same as the default colour
               # of trains, so it's easy to add arrows indicating the
               # direction of motion of trains.
               ('rgb "#00BFFF"        ', 1,        1),
             )

    gen.WriteOut('\n# Set nineteen default line types', plt)
    for index, (colour, width, dashtype) in enumerate(curves, start = 1):
        gen.WriteOut('set linetype ' + str(index) + " lc " + colour
                     + " lw " + str(width) + " dt " + str(dashtype), plt)
    return()


def ResetPage(page_num, settings_dict, source, file_stem, plt):
    '''Write a list of lines to the .plt file.  These reset certain
    settings each time we start a new page.  Page size, gnuplot variables
    and a few other things are unchanged, and the file name may be changed
    if we are printing each page to its own file.

        Parameters:
            page_num         int            The number of this page in the
                                            gnuplot file.
            settings_dict   {}              Dictionary of the run settings.
            source          str             Either "pages", "timeloops"
                                            or "filesloops", depending on
                                            which routine called ResetPage.
            file_stem       str             The stem of the pdf file.  If
                                            we are writing pages to
                                            individual files, it is used to
                                            generate the new file name.
            plt             handle          The handle of the gnuplot file.

        Returns:
            None

        Errors:
            None
    '''
    # We include a line of dashes starting with "*-" so that when we want to
    # find the top of each page or graph we can search for "*-" in the .plt
    # file and find again.
    gen.WriteOut('\n\n# *' + '-'*20 + ' Start of page ' + str(page_num) + ' '
                 + '-'*20, plt)

    # Define commands to generate the second and subseqent pages.
    # The 'reset' command appears to reset everything except "set grid"
    # and "set linetype".
    splitpages = settings_dict["splitpages"]
    if splitpages == "true" and source in ("pages", "filesloop"):
        dir_name = settings_dict["dir_name"]
        num_format = settings_dict["num_format"]
        pdf_name = file_stem + '-p' + num_format.format(page_num) + '.pdf'
        # Note that the use of ".pdf" above may break the files of expert
        # gnuplot users (who may want to generate .gif or .png files).
        # Unfortunately there is no way to know what file extension they
        # used in their verbatim block (I think).
        reset_page = 'reset; unset multiplot;\n'   \
                     'set output "' + dir_name.replace("\\", "/")    \
                     + pdf_name + '"\n'   \
                     'set multiplot   # Start a new page in a new file'
    else:
        reset_page = 'reset; unset multiplot; set multiplot   # Start a new page'

    if page_num != 1:
        gen.WriteOut(reset_page, plt)

   # Get the default margins from settings_dict.  These were set at the
    # start of ProcessPlot but the user may have changed them.
    lmargin = str(settings_dict["baselmargin"])
    rmargin = str(settings_dict["basermargin"])
    bmargin = str(settings_dict["basebmargin"])
    tmargin = str(settings_dict["basetmargin"])

    default_lines = ('set lmargin at screen ' + lmargin + '; set '
                       'rmargin at screen ' + rmargin   \
                       + settings_dict["defmarginscomment"],
                     'set bmargin at screen ' + bmargin + '; set '
                       'tmargin at screen ' + tmargin,
                     'set grid   # Put gridlines across the graphs',
                    )
    for line in default_lines:
        gen.WriteOut(line, plt)
    return()


def SubBlockParts(line_triples, tr_index, block_name,
                  file_name, settings_dict, log):
    '''Get a list of lines that covers the contents of a
    begin...end sub-block.  At entry, tr_index points to the
    line with "begin <something>" on it.  Returns the slice of
    line_triples and a list of all the "begin <something else>"
    blocks at next level down.

        Parameters:
            line_triples  [(int, str, str)] List of lines in the file.
            tr_index        int             Pointer to where the block
                                            starts in line_triples.
            block_name      str             Name of the block we are in.
            file_name       str             The file name without the file
                                            path.
            settings_dict   {}              Dictionary of the run settings.
            log             handle          The handle of the logfile.

        Returns:
            block_lines     []              A list of the lines in the block
    '''
    # Generate a list of lines that we can use in GetBegins.
    block_lines = []
    while True:
        tr_index += 1
        (line_number, line_data, line_text) = line_triples[tr_index]
        parts = line_data.lower().split()
        if parts[:2] == ["end", block_name]:
            break
        else:
            block_lines.append(line_triples[tr_index][2])
    return(block_lines)


def ProcessPlotControl(line_triples, tr_index, settings_dict, log):
    '''Take the "begin plotcontrol...end plotcontrol" block in the input
    file and process its entries.  This controls how often the state
    of the system is saved to the binary file.

        Parameters:
            line_triples [(int, str, str)]  List of lines in the file.  First
                                            entry is the line number in the file
                                            (starting at one, not zero).
                                            Second is the valid data on the line.
                                            Third is the entire line (including
                                            comments) also used in error messages.
            tr_index        int             Where to start reading the files block.
            settings_dict   {}              Dictionary of the run settings.
            log             handle          The handle of the logfile.

        Returns:
            plotcontrol_dict  {}            A dictionary of the lines in
                                            the block, used to recreate
                                            the block in new input files.
            plot_times     []               A list of times at which to save
                                            the state of the system.
    '''
    debug1 = settings_dict["debug1"]
    file_name = settings_dict["file_name"]
    aero_time = settings_dict["aero_time"]
    # We make the valid settings.  These can be any name and any floating
    # point number or Python list.  When we make a substitution we will
    # check the type and the range it is being substituted for.
    valid_settings = {"aero": ("QAstr",),}
    # We make a list of entries that we must have.
    requireds = ["aero"]
    # We make a dictionary of the optional keywords (there are none) and
    # a list of the allowable duplicates (also none).
    optionals = {}
    duplicates = ()
    settings = (valid_settings, requireds, optionals, duplicates)

    block_name = "plotcontrol"
    # Now call ProcessBlock.  It returns None if an error occurred.  It
    # returns the updated constants block.
    result = ProcessBlock(line_triples, tr_index, settings_dict,
                          block_name, {}, settings, log)
    if result is None:
        return(None)
    else:
        (discard, plotcontrol_dict) = result
    if debug1:
        print("In plotcontrol", plotcontrol_dict)

    # When we get to here we know we have at least one line with one word
    # on it.  If the word is a number then that is our one timestep to
    # print at.  That would be a pretty eccentric setting, but it is
    # possible.

    for key, entry in plotcontrol_dict.items():
        if key != 'block_index':
            list_text =  entry[0]
            try:
                # Check to see if there is one number.  We'll accept it, even
                # if it is a bit weird to want to print at one timestep.
                plotcontrol_list = [float(list_text)]
            except (TypeError, ValueError):
                # Check if the contents of the line after the key take the
                # form of a Python list or a chain of lists.  First we get
                # all the text on the line.
                tr_index = plotcontrol_dict[key][-1]
                (line_number, line_data, line_text) = line_triples[tr_index]
                if debug1:
                    print("Checking a range of print times as a list", list_text)
                result = CheckListAndRange(list_text, "spoof_it",
                                           settings_dict, line_triples,
                                           line_number, line_text, False, log)
                if result is None:
                    return(None)
                else:
                    plotcontrol_list = result

        # Now do a check to see if the desired print times are not
        # multiples of the time step.  This could happen if (say) you
        # set the aero time step to 0.1 seconds but asked it to plot
        # every 0.05 seconds or every 0.25 seconds.  Both of these
        # are dumb things to do but if they aren't handled they cause
        # a crash, as the array of print times is not a subset of
        # array of calculation timesteps.
        aero_step = settings_dict["aero_step"]
        for index, time in enumerate(plotcontrol_list):
            divisor = time / aero_step
            if not(math.isclose(round(divisor,0), divisor)):
                tr_index = plotcontrol_dict[key][-1]
                (line_number, line_data, line_text) = line_triples[tr_index]
                # We have a problem.
                err = ('> The file named "' + file_name + '" has\n'
                       '> incorrect entries in its plotcontrol block.\n'
                       '> The numbers in the block must be multiples\n'
                       '> of the timestep (' + gen.FloatText(aero_step)
                         + ').  At least one of the\n'
                       '> entries is not, ' + gen.FloatText(time) + '.\n'
                       '> Please edit the file to correct this.'
                      )
                gen.WriteError(2401, err, log)
                gen.ErrorOnLine(line_number, line_text, log, False)
                return(None)
    return(plotcontrol_dict, plotcontrol_list)


def ProcessSESData(line_triples, tr_index, settings_dict, files_dict, log):
    '''Take a "begin SESdata...end SESdata" block in the input file
    file and process its entries.  Write an SES input file based on
    the Hobyah geometry, the entries in this block directing how
    to write the SES file and the pragmats in the tunnel blocks.

        Parameters:
            line_triples [(int, str, str)]  List of lines in the file.  First
                                            entry is the line number in the file
                                            (starting at one, not zero).
                                            Second is the valid data on the line.
                                            Third is the entire line (including
                                            comments) also used in error messages.
            tr_index        int             Where to start reading the SESdata
                                            block.
            settings_dict   {}              Dictionary of the run settings.
            log             handle          The handle of the logfile.

        Returns:
            plotcontrol_dict  {}            A dictionary of the lines in
                                            the block, used to recreate
                                            the block in new input files.
            plot_times        []            A list of times at which to save
                                            the state of the system.
    '''
    debug1 = settings_dict["debug1"]
    dir_name = settings_dict["dir_name"]
    file_name = settings_dict["file_name"]
    aero_time = settings_dict["aero_time"]
    units = settings_dict["units"]


    line_number, line_data, line_text = line_triples[tr_index]
    # Try to open the .hbn file for this calculation
    bdat = ReadBinData(line_number, line_text, "calc", settings_dict,
                       files_dict, log)
    if bdat is None:
        return(None)


    # Get a list of the names of tunnels and routes for form 4.
    routes = tuple(bdat.routes_dict.keys())
    firelocs = tuple(list(routes) + list(bdat.tunnels_dict.keys()))
    # Get a list of fan characteristic names for form 7A/7B and a list of
    # jet fan types for form 7C.
    fantypes = tuple(bdat.fanchars_dict.keys())
    JFtypes = tuple(bdat.JFcalc_dict.keys())

    # Build a dictionary of the valid SES names and their keys.  We
    # use the name of the target in the first line of form 1A.
    valid_progs = {"4.1": "SES 4.1",
                   "openses": "OpenSES 4.3",
                   "aurecon-ses": "Aurecon SES 107.0",
                   "offline-ses": "offline-SES 204.5",
                   "svs": "SVS 6.6.2"
                  }

    # We make the valid settings.  These set the entries in the SES
    # file that have no equivalent in Hobyah, like temperatures and
    # train performance.
    valid_settings = {"target": (tuple(valid_progs.keys()),),
                      "filename": ("QAstr",),
                      "form1b": ("float 0+ null design hour",
                                 "float + null design month",
                                 "float 0+ null year"),
                      "form1f": ("float any temp  an air DB temperature",
                                 "float any temp  an air WB temperature",
                                 "#name",  # This is air pressure
                                 "float any temp  an AM DB temperature",
                                 "float any temp  an AM WB temperature",
                                 "float any temp  a PM DB temperature",
                                 "float any temp  a PM WB temperature",
                                 "float any tdiff a temperature range"),
                      "form1g": ("float any mass1  a passenger mass",
                                 "float 0+  perc   % heat capture",
                                 "float 0+  perc   % heat capture",
                                 "float 0+  perc   % heat capture",
                                 "float 0+  perc   % heat capture",
                                 "float 0+  perc   a train speed",
                                 "float 0+  null   a fire simulation switch",
                                 "float 0+  null   an emissivity"),
                      "3temperatures":
                                ("float any temp  a wall temperature",
                                 "float any temp  an air DB temperature",
                                 "float any temp  an air WB temperature"),
                      "form3f": ("float 0+  dist1  a wall thickness",
                                 "float 0+  dist1  a tunnel separation",
                                 "float  +  v41_thcon  a wall thermal conductivity",
                                 "float  +  diff  a wall diffusivity",
                                 "float  +  v41_thcon  a ground thermal conductivity",
                                 "float  +  diff  a ground diffusivity",
                                 "float any temp  a deep sink temperature"),
                      "4a_locn":(firelocs, # a tunnel or route name
                                 "float 0+  dist1  a distance or chainage",
                                 "QAstr"),
                      "4b_heat": ("float 0+  null  a sensible heat rate",
                                  "float 0+  null  a latent heat rate",
                                  ("MW", "watts", "BTU/hr"),
                                  "float 0+  null  a fire start time",
                                  "float  +  null  a fire stop time"),
                      "4c_flames":("float  +  temp  a flame temperature",
                                   "float  +  null  an area for radiation",
                                   ("m^2", "ft^2", "m^2/MW", "ft^2/MW",
                                    "ft^2/BTU/hr")),
                      "form7ab": (fantypes,), # A list of fan names in Hobyah
                      "form7c1": (JFtypes, # A list of jet fan names in Hobyah
                                  ("forwards", "reverse"),
                                  "float 0+  null  a jet fan start time",
                                  "float  +  null  a jet fan stop time"),
                      "form7c2": ("float 0+  null  a jet fan thrust",
                                  ("n", "m^3/s", "lbf", "cfm"),
                                  "float 0+  null  an installation efficiency",
                                  "float any speed1  a jet velocity",
                                  "float 0+  null  a jet fan start time",
                                  "float  +  null  a jet fan stop time"),
                                  # Note that we don't allow the user to
                                  # set the air density the thrust was
                                  # measured at, we always use 1.2 kg/m^3.
                      "form8":   (routes,),
                     }


    # We make a list of entries that we must have.
    requireds = ["target", "filename"]
    # We make a dictionary of the optional keywords and a list of the
    # allowable duplicates.
    optionals = {"form8": {"orientation": ("reverse",)}}
    duplicates = ("form7ab", "form7c1", "form7c2", "form8")
    settings = (valid_settings, requireds, optionals, duplicates)

    block_name = "sesdata"
    result = ProcessBlock(line_triples, tr_index, settings_dict,
                          block_name, {}, settings, log)
    if result is None:
        return(None)
    else:
        (discard, SES_dict) = result
    if debug1:
        print("In SESdata", SES_dict)

    # Get the type of SES program we want to run and the name of the SES
    # file.
    target = SES_dict["target"][0]
    SES_name = SES_dict["filename"][0]

    # Set a Boolean True if we are writing an input file for Aurecon SES,
    # False if not.  It controls whether comments are enclosed in curly
    # braces or not.
    if target == "aurecon-ses":
        Aur = True
    else:
        Aur = False


    # Figure out what units the SES input file needs to be in.
    if target in ("svs", "aurecon-ses"):
        USunits = False
    else:
        USunits = True

    print('> Generating a skeleton SES input file "' + SES_name + '" for '
           + valid_progs[target] + '.')
    # Now pick out the various entries for forms.  If they are not set,
    # choose suitable defaults for them.

    # Build a form 1A based on the program type and the name of the
    # Hobyah input file and fill it full of caveats.
    form1A = (valid_progs[target] + ' skeleton file from "' + file_name
              + '", a Hobyah input file.',
              'This file has a geometry (forms 2, 3, 5 & 6), fans '
              '(form 7) & routes (form 8)',
              'based on the geometry, fans and routes of the '
              'above Hobyah run.  This SES file',
              "won't give correct results because it has dud values "
              'that you should adjust:',
              '  * outside air conditions,',
              '  * passenger mass, TES capture rules and fire '
              'simulation entries,',
              '  * segment types (all line and vent segments are type 1),',
              '  * stack heights.  Line segments in routes have '
              'had their stack heights set',
              '    from gradients in a route, which may '
              'or may not be the correct route.  Most',
              '    other segments have stack height zero.  Please '
              'review all the stack heights.',
              '  * fixed and steady heat gains, wetted wall perimeters '
              '& pressure loss factors,',
              '  * wall and ground properties and temperatures,',
              '  * node types (nodes are type 1 or 7: '
              'too many type 7s may be poor practice),',
              '  * there are no axial fans in vent segments or jet '
              'fans in line segments,',
              '  * train timetables, track radii, energy sectors and '
              'coasting parameters,',
              '  * train properties (form 9 has a terribly inefficient '
              'default train type),',
              '  * the zones (at the moment all segments are in one '
              'uncontrolled zone),',
              '  * print timesteps, ECZ and calculation intervals, '
              'timesteps and run time.',
              'Finally, many values permitted in Hobyah runs may be '
              'out of range in SES runs.',
             )

    if "form1b" in SES_dict:
        hour, month, year = SES_dict["form1b"][:3]
    else:
        hour = 17.
        month = 7
        year = 2024
    form1B = [hour, month, year]
    if target == "openses":
        # OpenSES has its version number as the fourth entry in form 1B.
        form1B.append(4.3)

    if "form1f" in SES_dict:
        DB, WB, p_text, AMDB, AMWB, PMDB, PMWB, tdiff = SES_dict["form1f"][:8]
        # The pressure is not read as a number.  This allows the user to
        # put the keyword "p_atm", meaning "use either the pressure set
        # in the file's "settings" block or the default pressure (101325 Pa)
        # if there is no entry in the settings block.
        if p_text == "p_atm":
            press = settings_dict["p_atm"]
        else:
            try:
                press = float(p_text)
            except ValueError:
                # Raise an error.
                if units == "si":
                    text1 = "Pascals"
                    text2 = str(settings_dict["p_atm"]) + ' Pa).'
                else:
                    text1 = "inches of mercury"

                    p_atm = settings_dict["p_atm"]
                    text2 = gen.RoundText(USc.ConvertToUS("press2", p_atm,
                                            False, log)[0], 3) + ' in. Hg).'
                err = ('> The file named "' + file_name + '" did not\n'
                       '> set a suitable value for atmospheric air pressure\n'
                       '> in form 1F of one of its "SESdata" blocks.\n'
                       '> Suitable values are either an atmospheric\n'
                       '> pressure in ' + text1 + ' or the phrase "p_atm"\n'
                       '> (without double quotes) to use the atmospheric\n'
                       '> pressure used in the method of characteristics\n'
                       '> calculation (' + text2
                      )
                gen.WriteError(3061, err, log)
                tr_index = SES_dict["form1f"][-1]
                gen.ErrorOnLine2(tr_index, line_triples, log, False)
                return(None)
            else:
                if units == "us":
                    # Convert the value in US units into SI units.
                    press = USc.ConvertToUS("press2", press, False, log)[0]
        if units == "us":
            # Convert everything except the pressure into SI units.
            DB = USc.ConvertToSI("temp", DB, False, log)[0]
            WB = USc.ConvertToSI("temp", WB, False, log)[0]
            AMDB = USc.ConvertToSI("temp", AMDB, False, log)[0]
            AMWB = USc.ConvertToSI("temp", AMWB, False, log)[0]
            PMDB = USc.ConvertToSI("temp", PMDB, False, log)[0]
            PMWB = USc.ConvertToSI("temp", PMWB, False, log)[0]
            tdiff = USc.ConvertToSI("tdiff", tdiff, False, log)[0]
    else:
        # Let's default to conditions in Darwin during December (nice
        # and hot!) and either the pressure set in the file's "settings"
        # block or the default pressure (101325 Pa).
        DB = 29.0
        WB = 27.0 # ~77% RH
        AMDB = 25.0
        AMWB = 23.4
        PMDB = 33.0
        PMWB = 30.0
        press = settings_dict["p_atm"]
        tdiff = 2.5  # Hot and dry (winter) or hot and wet (summer).
    form1F = [DB, WB, press, AMDB, AMWB, PMDB, PMWB, tdiff,
          "DB1    WB1  P_atm  AMDB  AMWB  PMDB   PMWB   annamp"]

    if "form1g" in SES_dict:
        (paxmass, pbTESstat, pbTESmov, apTESstat,
         apTESmov, speed, firopt, emiss) = SES_dict["form1g"][:8]
        if firopt != 0:
            # Set the fire option to 1 regardless of what the user
            # set.
            firopt = 1
        if units == "us":
            # Convert the entries with dimensions into SI units.
            paxmass = USc.ConvertToSI("mass1", paxmass, False, log)[0]
            speed = USc.ConvertToSI("speed2", speed, False, log)[0]
    else:
        # Use 100 kg for passenger mass (obese, because we want the
        # defaults to be extremely conservative), turn all TES capture
        # fractions to zero, turn off fire simulation and set the
        # emissivity of flames to 0.2.
        paxmass = 100.0
        pbTESstat = 0.0
        pbTESmov = 0.0
        apTESstat = 0.0
        apTESmov = 0.0
        speed = 30.0 # Turn off TES capture at a fairly low speed
        firopt = 0
        emiss = 0.2
    form1G = [paxmass, pbTESstat, apTESstat, pbTESmov, apTESmov, speed,
              firopt, emiss,
              "mass   cpBst  cpBmv cpAst cpAmv speed  firopt emiss"]

    if "3temperatures" in SES_dict:
        # Get the wall temperature, initial DB air temperature and
        # initial WB air temperature from the file.
        # This will be set in all line and vent segments (forms 3E and 5B).
        init_wall, init_DB, init_WB = SES_dict["3temperatures"][:3]
        if units == "us":
            init_wall = USc.ConvertToSI("temp", init_wall, False, log)[0]
            init_DB = USc.ConvertToSI("temp", init_DB, False, log)[0]
            init_WB = USc.ConvertToSI("temp", init_WB, False, log)[0]
    else:
        # Set conditions in the tunnels to be similar to outside, with
        # hot walls.
        init_wall = DB
        init_DB = DB
        init_WB = WB

    if "form3f" in SES_dict:
        # Get the wall and ground conditions.
        lin_thk, sep, thcon1, diff1, thcon2, diff2, deep = SES_dict["form3f"][:7]
        if units == "us":
            # Convert the entries with dimensions into SI units.
            lin_thk = USc.ConvertToSI("dist1", lin_thk, False, log)[0]
            sep = USc.ConvertToSI("dist1", sep, False, log)[0]
            thcon1 = USc.ConvertToSI("v41_thcon", thcon1, False, log)[0]
            diff1 = USc.ConvertToSI("diff", diff1, False, log)[0]
            thcon2 = USc.ConvertToSI("v41_thcon", thcon2, False, log)[0]
            diff2 = USc.ConvertToSI("diff", diff2, False, log)[0]
            deep = USc.ConvertToSI("temp", deep, False, log)[0]
    else:
        # Set an 0.25 m thick concrete wall, tunnel separation of
        # 25 m, thermal conductivities/diffusivities for dry concrete
        # and dry clay for the tunnel walls and surrounding ground
        # (from Appendix G of the SES v4.1 User's Manual).  Set hot
        # ground for Darwin.
        lin_thk = 0.25 # metres
        sep = 25.0 # metres
        thcon1 = 0.934 # dry concrete, W/m-K
        diff1 = 4.903E-07 # dry concrete, m^2/s
        thcon2 = 0.242 # dry clay, W/m-K
        diff2 = 2.581E-07 # dry clay, m^2/s
        deep = 25.0 # deg C

    # Process the segments in the tunnels and convert then into
    # SES line segments or vent segments.  We make a set of
    # counters for each block of hundred segment numbers and
    # increment each as it is used.  We start at 32 so that if
    # someone uses a number under 100 and the segment and subsegment
    # number end up in a spreadsheet as "9-1", the spreadsheet won't
    # convert the name to a date (that used to really infuriate me).
    # The range function gives us [101, 201, 301 etc. all the way
    # up to 11001].  Standard SES numbering only goes as far as 999,
    # while Aurecon SES goes up to 9,999.
    tcounts = [32] + list(range(101, 11002, 100))
    ncounts = tcounts.copy()

    # Make a dictionary to hold the names of nodes and their
    # corresponding SES node number.
    known_nodes = {}

    # Make a dictionary to hold the lines of input for each form2, form 3
    # and form 5.
    form2A_dict = {}
    form2B_dict = {}
    form3_dict = {}
    form5_dict = {}

    # Make a list of the tunnels that are turned into vent segments.
    # We need this because SES can't put fires in vent segments or
    # have routes go through vent segments.
    vent_tunnels = []
    # Make a dictionary to identify which routes to use to take the
    # stack heights in tunnels from.
    grad_routes = {}


    for tun_name, tun_data in bdat.tunnels_dict.items():
        # Get the name of the tunnel before it was converted to lower
        # case.
        UC_name = tun_data["block_index"][1]

        # Figure out which block of counters to use for this tunnel's
        # segments and nodes.
        if "sespragmat" in tun_data:
            block, seg_type = tun_data["sespragmat"][:2]
            optionals_dict = tun_data["sespragmat"][-2]
            t_index, discard = divmod(block, 100)
            n_index = t_index
            if seg_type == "vent":
                lineseg = False
                vent_tunnels.append(tun_name)
            else:
                lineseg = True
            # Check if there is an optional argument to choose a route.
            if "route" in optionals_dict:
                grad_route = optionals_dict["route"]
                grad_routes.__setitem__(tun_name, ["routeset", grad_route])
                if grad_route not in bdat.routes_dict:
                    route_names = tuple(bdat.routes_dict.keys())
                    err = ('> The file named "' + file_name + '" had\n'
                           '> conflicting SES input.  The tunnel named\n'
                           '> "' + tun_name + '" had an "SESpragmat" optional\n'
                           '> entry that told Hobyah to get the SES stack\n'
                           '> heights from the gradients in route "'
                             + grad_route + '".\n'
                           '> Unfortunately that route does not exist.\n'
                           '> The following routes exist:\n'
                             + gen.FormatOnLines(route_names) + '\n'
                           '> Please edit the file to fix the conflict,\n'
                           '> either by removing the optional argument in\n'
                           '> the "SESpragmat" line or adding a definition\n'
                           '> for route "' + grad_route + '".'
                          )
                    pragmat_index = bdat.tunnels_dict[tun_name]["sespragmat"][-1]
                    line_num, discard, line_text = line_triples[pragmat_index]
                    gen.WriteError(3062, err, log)
                    gen.ErrorOnLine(line_num, line_text, log,
                                        False, False, "Problematic")
                    return(None)
        else:
            # Default to starting at 101 when building line segments.
            lineseg = True
            t_index = 1
            n_index = 1

        seg_entries = bdat.tuns2frames[tun_name]
        # Get a number that we use to test if we are at the last
        # segment.
        lastseg = len(seg_entries) - 1
        for tun_index, seg_idx in enumerate(seg_entries):
            # Allocate a segment number and update the counter in this
            # block.
            seg_num, tcounts = AllocateCounter(tcounts, t_index)

            descrip = 'Tunnel "' + UC_name + '", segment '   \
                        + str(tun_index + 1) +' of ' + str(lastseg + 1)
            if lineseg:
                form3A1 = [seg_num,  1,  descrip]
            else:
                form5A = [seg_num,  1,  descrip]
                vent_tunnels.append(seg_num)

            # Get the properties of this tunnel segment.
            gridpoints = bdat.dists[seg_idx]
            length = gridpoints[-1] - gridpoints[0]
            # Figure out the count of subsegments by assuming the subsegments
            # can be 20 m minimum.
            subsegs, discard = divmod(length, 19.999999999)
            if math.isclose(subsegs, 0.0):
                # The tunnel is shorter than 20 m.  We put in one subsegment.
                subsegs = 1
            else:
                subsegs = int(subsegs)

            seg_data = bdat.segments_consts[seg_idx]
            area, perim, D_h, roughness = seg_data[:4]


            # Form 3B has length, area, stack height zero and fireseg 1
            # If this tunnel is in a route that has a profile, we will
            # figure out what the stack height is and substitute it in
            # after we read the route definitions.
            if lineseg:
                form3A2 = [length, area, 0.0, 1]
                form3B1 = (perim,)
                if roughness < 0:
                    # The user set a fixed friction factor, which we turned
                    # into a Fanning friction factor.  Back-calculate the
                    # roughness from the friction factor using Moody's
                    # approximation (because SES uses that to calculate
                    # friction in open tunnels from roughness height).
                    roughness = D_h / 20000 * math.pow(-roughness / 0.001375 - 1, 3)
                form3B2 = [roughness]
            else:
                # We set the stack height of the vent segment to zero.
                # Once we have processed the route definitions and
                # figured out the node elevations, we change it if we
                # know the elevations of both nodes at the ends of the
                # vent segment.
                form5B = [1, subsegs, 25., 12, init_wall, init_DB,
                          init_WB, 0.0]
                form5C = (0, 30, 9999, 1) # No fan set
                # SES has no friction factor in its vent shafts.  We
                # calculate the loss factor due to friction in the
                # Hobyah calculation and add it to the losses at the
                # back end.
                if roughness < 0:
                    zeta_fric = -roughness * length * perim / area
                else:
                    # Get the Reynolds number that equates to 5 m/s
                    # and use the friction factor at that Re.
                    Re = 5. * D_h / 1.5E-5
                    c_f = Colebrook(roughness/3.7, Re)
                    zeta_fric = c_f * length * perim / area


            zeta_bf_b, zeta_fb_b, zeta_bf_f, zeta_fb_f = seg_data[6:10]
            if lineseg:
                form3C = [zeta_bf_f, zeta_fb_f, zeta_bf_b, zeta_fb_b,
                           0., subsegs, 0]
                form3E = [1, subsegs, init_wall, init_DB, init_WB]
                form3F = [lin_thk, sep, thcon1, diff1, thcon2, diff2, deep]
                forms = [form3A1, form3A2, form3B1, form3B2, form3C,
                         form3E, form3F]
                form3_dict.__setitem__(seg_idx, forms)
            else:
                # Add the friction loss to the losses at the back end.
                zeta_bf_b += zeta_fric
                zeta_fb_b += zeta_fric
                form5D = [length, area, perim, zeta_bf_f, zeta_fb_f,
                          zeta_bf_b, zeta_fb_b]
                forms = [form5A, form5B, form5C, form5D]
                form5_dict.__setitem__(seg_idx, forms)


            # Figure out if we need to pick a new node number at the
            # back end or can use an existing one.
            back_type, back_value = seg_data[10:12]
            if back_type[0] == "n":
                node_name = back_value.lower()
                if node_name in known_nodes:
                    # This is a named node or named join that we have
                    # encountered before when processing an earlier
                    # tunnel.
                    result = known_nodes[node_name]
                    back_node_num = result[0]
                    known_nodes[node_name].append(seg_num)
                elif tun_index == 0:
                    # This is a named node we haven't encountered before.
                    back_node_num, ncounts = AllocateCounter(ncounts, n_index)
                    known_nodes.__setitem__(node_name, [back_node_num, seg_num])
                else:
                    # This node is internal to the tunnel, we can use
                    # the node number at the forward end of the previous
                    # segment in this tunnel.
                    back_node_num = fwd_node_num
                    known_nodes[node_name].append(seg_num)
            elif tun_index != 0:
                # This is not the first segment in this tunnel, so
                # we use the forward node number of the previous segment.
                back_node_num = fwd_node_num
                back_value = "# " + str(fwd_node_num)
                known_nodes[back_value].append(seg_num)
            else:
                # This is a new node that isn't named.  Probably a
                # portal if it is at the back end.
                back_node_num, ncounts = AllocateCounter(ncounts, n_index)
                back_value = "# " + str(back_node_num)
                known_nodes.__setitem__(back_value, [back_node_num, seg_num])

            # Now get the node number at the forward end, either generating
            # it or reusing an existing one.
            fwd_type, fwd_value = seg_data[12:14]
            # if fwd_type[0] == "n":
            #     if fwd_value in known_nodes:
            #         fwd_node_num = known_nodes[fwd_value.lower()]
            #     else:
            #         fwd_node_num, ncounts = AllocateCounter(ncounts, n_index)
            #         known_nodes.__setitem__(fwd_value.lower(), [fwd_node_num])
            # else:
            #     fwd_node_num, ncounts = AllocateCounter(ncounts, n_index)
            if fwd_type[0] == "n":
                node_name = fwd_value.lower()
                if node_name in known_nodes:
                    # This is a named node or named join that we have
                    # encountered before when processing an earlier
                    # tunnel.
                    result = known_nodes[node_name]
                    fwd_node_num = result[0]
                    known_nodes[node_name].append(seg_num)
                else:
                    # This is a named node we haven't encountered before.
                    fwd_node_num, ncounts = AllocateCounter(ncounts, n_index)
                    known_nodes.__setitem__(node_name, [fwd_node_num, seg_num])
            else:
                # This is a new node that isn't named.  Probably a
                # change of sectype, a damper or an axial fan.
                fwd_node_num, ncounts = AllocateCounter(ncounts, n_index)
                fwd_value = "# " + str(fwd_node_num)
                known_nodes.__setitem__(fwd_value, [fwd_node_num, seg_num])

            # Store the form 2 definition for this segment.
            if lineseg:
                form2 = (seg_num, back_node_num, fwd_node_num, 1, 0.0)
                form2A_dict.__setitem__(seg_idx, form2)
            else:
                form2 = (seg_num, back_node_num, fwd_node_num, 0.0)
                form2B_dict.__setitem__(seg_idx, form2)
    if False:
        for key, entry in known_nodes.items():
            print(key, entry)
        for key, entry in form2A_dict.items():
            print(key, entry)
        for key, entry in form2B_dict.items():
            print(key, entry)
        for key, entry in form3_dict.items():
            print(key, entry)
        # for key, entry in form5_dict.items():
        #     print(key, entry)

    form4_dict = {}
    if "4a_locn" in SES_dict:
        key, dist, comment, optionals_dict, tr_index = SES_dict["4a_locn"]
        if key in bdat.routes_dict:
            # The fire is located in a route.  Check if the chainage
            # of the fire is above ground or entirely outside the route's
            # extents.
            route_dict = bdat.routes_dict[key]
            tunnel_chs = route_dict["tunnel_chs"]
            up_ptl = tunnel_chs[0]
            down_ptl = tunnel_chs[-1]
            if not (up_ptl <= dist < down_ptl):
                err = ('> The file named "' + file_name + '" had an\n'
                       '> SES fire definition (form 4) in which the fire\n'
                       '> location was not in the tunnels of the route.\n'
                       '> The fire was located at chainage '
                         + gen.RoundText(dist, 2) + ' m and the\n'
                       '> tunnels in the route start at '
                         + gen.RoundText(up_ptl, 2) + ' m and stop\n'
                       '> at ' + gen.RoundText(down_ptl, 2) + ' m.\n'
                       '> Please edit the fire definition to place the\n'
                       '> fire inside the tunnels on the route.'
                      )
                gen.WriteError(3063, err, log)
                tr_index = SES_dict["4a_locn"][-1]
                gen.ErrorOnLine2(tr_index, line_triples, log, False)
                return(None)
            elif len(tunnel_chs) == 2:
                # The route only passes through one tunnel.
                mult, offset = route_dict["route2tun"][0]
                dist = dist * mult + offset
                start, stop = tunnel_chs
                tun_name = route_dict["tunnel_names"][0]
            else:
                # Figure out which tunnel it is in and turn the
                # location into a distance in the tunnel instead
                # of a chainage in the route.
                for index, start in enumerate(tunnel_chs):
                    stop = tunnel_chs[index + 1]
                    if start <= dist < stop:
                        mult, offset = route_dict["route2tun"][0]
                        dist = dist * mult + offset
                        tun_name = route_dict["tunnel_names"][index]
                        break
                start = (start - offset) * mult
                stop = (stop - offset) * mult
            tun_dict = bdat.tunnels_dict[tun_name]
        else:
            # It must be in a tunnel, not a route.
            tun_name = key
            tun_dict = bdat.tunnels_dict[key]
            start = tun_dict["back"][0]
            stop = tun_dict["fwd"][0]

            if not (start <= dist < stop):
                err = ('> The file named "' + file_name + '" had an\n'
                       '> SES fire definition (form 4) in which the fire\n'
                       '> location was outside the tunnel that you want\n'
                       '> to put it in (tunnel "' + key + '").  The fire\n'
                       '> was located at distance ' + gen.RoundText(dist, 2)
                         + ' m.  \n'
                       '> The back end of the tunnel is at '
                         + gen.RoundText(start, 2) + ' m and the\n'
                       '> forward end is at ' + gen.RoundText(stop, 2)
                         + ' m.\n'
                       '> Please edit the fire definition to place the\n'
                       '> fire inside the tunnel or change to another\n'
                       '> tunnel.'
                      )
                gen.WriteError(3064, err, log)
                tr_index = SES_dict["4a_locn"][-1]
                gen.ErrorOnLine2(tr_index, line_triples, log, False)
                return(None)

        # Now check if the tunnel has been turned into a vent segment
        # and complain if it has.
        if key in vent_tunnels:
            tr1_index = tun_dict["sespragmat"][-1]
            line1_num, discard, line1_text = line_triples[tr1_index]
            line2_num, discard, line2_text = line_triples[tr_index]
            err = ('> The file named "' + file_name + '" had\n'
                   '> an SES fire definition (form 4) in which the fire\n'
                   '> location was placed in tunnel "' + tun_name + '").\n'
                   '> That tunnel was turned into a vent segment in\n'
                   '> the SES input file, as per the instruction in the\n'
                   '> "SESpragmat" keyword. '"You can't put fires into\n"
                   "> SES vent segments, which SES will complain about.\n"
                   '> Please either edit the fire definition to place\n'
                   '> it in a tunnel that maps to an SES line segment\n'
                   '> or tell Hobyah to turn the tunnel into a line\n'
                   '> segment.'
                  )
            gen.WriteError(3065, err, log)
            gen.ErrorOnTwoLines(line1_num, line1_text,
                                line2_num, line2_text, log,
                                False, False, "Conflicting")
            return(None)
        # If we get here the fire is in a valid tunnel.  Figure out
        # which segment and subsegment it is in.
        t_segs = bdat.tuns2segs[tun_name]['segs']
        t_dists = bdat.tuns2segs[tun_name]['dists']
        for index, dist_back in enumerate(t_dists[:-1]):
            dist_fwd = t_dists[index + 1]
            if dist_back < dist <= dist_fwd:
                # The fire is in this segment.
                seg_ID = t_segs[index]
                seg_idx = seg_ID - 1
                break
        ses_seg = form2A_dict[seg_idx][0]
        # Get the count of SES subsegments from form 1C of the SES
        # input data.
        subsegs = form3_dict[seg_idx][4][5]
        sublength = (dist_fwd - dist_back) / subsegs
        subseg = (dist - dist_back) / sublength
        if math.isclose(subseg, int(subseg)):
            # The fire location is on a subsegment boundary.
            # Put the fire in the subsegment on the back side of the
            # boundary.
            subseg -= 1
        subseg = int(max(subseg, 1))

    if "4b_heat" in SES_dict:
        if "4a_locn" not in SES_dict:
            err = ('> The file named "' + file_name + '" had an\n'
                   '> incomplete SES fire definition (form 4).  You\n'
                   '> included an entry starting with "4b_heat" to\n'
                   '> set a fire size, start time and stop time but\n'
                   '> did not include an entry starting with "4A_locn"\n'
                   '> to set the location and description of the fire.\n'
                   '> Please either add one or remove the line with\n'
                   '> "4B_heat" from the SESdata block.'
                  )
            gen.WriteError(3066, err, log)
            tr_index = SES_dict["block_index"][-1]
            gen.ErrorOnLine2(tr_index, line_triples, log, False, False,
                "Relevant")
            return(None)
        sens, lat, f_units, f_start, f_stop = SES_dict["4b_heat"][:5]
        # Turn the heat gains into watts.  Note that we told ProcessBlock
        # to treat them as unitless so we convert them here.  Three units
        # may have been set: watts, MW, or btu/hr.
        if f_units == "watts":
            sens = sens / 1e6
            lat = lat / 1e6
        elif f_units == "btu/hr":
            sens =  USc.ConvertToSI("v41_Mwatt", sens, False, log)[0]
            lat =  USc.ConvertToSI("v41_Mwatt", lat, False, log)[0]
    elif "4a_locn" in SES_dict:
        err = ('> The file named "' + file_name + '" had an\n'
               '> incomplete SES fire definition (form 4).  You\n'
               '> included an entry starting with "4A_locn" to\n'
               '> set the location and description but did not\n'
               '> include an entry starting with "4b_heat" to\n'
               '> set the fire size, start time and stop time.\n'
               '> Please either add one or remove the line with\n'
               '> "4A_locn" from the SESdata block.'
              )
        gen.WriteError(3067, err, log)
        tr_index = SES_dict["block_index"][-1]
        gen.ErrorOnLine2(tr_index, line_triples, log, False, False,
            "Relevant")
        return(None)

    if "4c_flames" in SES_dict:
        if "4a_locn" not in SES_dict:
            err = ('> The file named "' + file_name + '" had an\n'
                   '> incomplete SES fire definition (form 4).  You\n'
                   '> included an entry starting with "4c_flames" to\n'
                   '> set the radiative heat transfer properties of\n'
                   '> the fire but did not include an entry starting\n'
                   '> with "4A_locn" to set the location and description\n'
                   '> of the fire. Please either add one or remove the \n'
                   '> line with "4C_flames" from the SESdata block.'
                  )
            gen.WriteError(3068, err, log)
            tr_index = SES_dict["block_index"][-1]
            gen.ErrorOnLine2(tr_index, line_triples, log, False, False,
                "Relevant")
            return(None)
        flametemp, flamearea, flameunits = SES_dict["4c_flames"][:3]
        # Turn the flame area into m^2.  We told ProcessBlock to
        # treat them as unitless so we do that here.  Four units
        # may have been set: m^2, ft^2, m^2/MW (my favourite) or
        # ft^2/MW.  We store the heat releases in MW, not watts, to
        # be consistent with Aurecon SES.
        if flameunits == "ft^2":
            flamearea = USc.ConvertToSI("area", flamearea, False, log)[0]
        elif flameunits == "m^2/mw":
            flamearea = flamearea * sens
        elif flameunits == "ft^2/mw":
            flamearea = flamearea * sens / 0.3048**2
        form4A = (comment.ljust(40), ses_seg, subseg)
        form4B = [sens, lat, f_start, f_stop, flametemp, flamearea]
        form4_dict.__setitem__(1, (form4A, form4B))
    elif firopt == 1 and "4a_locn" in SES_dict:
        # Note that we only raise this error if a fire simulation
        # has been set.  The radiative heat transfer properties of
        # fires are not used in non-fire runs.
        err = ('> The file named "' + file_name + '" had an\n'
               '> incomplete SES fire definition (form 4).  You\n'
               '> included an entries starting with "4A_locn" and\n'
               '> "4B_heat" to set the location, description, heat\n'
               '> release rate and start and stop times but did not\n'
               '> include an entry starting with "4c_flames" to\n'
               '> set the radiative heat transfer properties of\n'
               '> the fire.\n'
               '> Please either add one or remove the lines with\n'
               '> "4A_locn" and "4B_heat" from the SESdata block.'
              )
        gen.WriteError(3069, err, log)
        tr_index = SES_dict["block_index"][-1]
        gen.ErrorOnLine2(tr_index, line_triples, log, False, False,
            "Relevant")
        return(None)
    # Once we get to here we have either processed all three entries for
    # the fire or none of them.


    axial_fans = []
    jetfans = []
    routes = []
    for key, values in SES_dict.items():
        if "form7ab" in key:
            # This is an axial fan characteristic definition.  Add the
            # name of the fan characteristic we want to copy to the list.
            axial_fans.append(values)
        elif "form7c1" in key:
            # This is a fan definition that copies over properties
            # from a Hobyah jet fan definition.  Get them.
            name, direction = values[:2]
            JFprops = bdat.JFcalc_dict[name]
            if direction == "forwards":
                SESJFprops = list(JFprops[1:4]) + list(values[2:4])
            elif math.isclose(JFprops[5], -1):
                err = ('> The file named "' + file_name + '" had an\n'
                       '> incorrect SES jet fan definition (form 7C).\n'
                       '> You tried to create a jet fan definition for\n'
                       '> SES from a unidirectional jet fan in Hobyah\n'
                       '> (the jet fan type called "' + name + '").\n'
                       '> You asked for the properties of that jet fan\n'
                       '> type when running it in reverse, but properties\n'
                       '> of a unidirectional jet fan running in reverse\n'
                       '> are not defined.  Please edit the SES jet fan\n'
                       '> to correct the error.'
                      )
                gen.WriteError(3070, err, log)
                tr_index = values[-1]
                gen.ErrorOnLine2(tr_index, line_triples, log, False)
                return(None)
            else:
                SESJFprops = list(JFprops[4:7]) + list(values[2:4])
            jetfans.append(SESJFprops)
        elif "form7c2" in key:
            # This is a fan definition that directly sets all the
            # properties.  We need to figure out what values the
            # input file needs.
            SESJFprops = list(values[:6])
            JF_units = SESJFprops.pop(1)
            if JF_units == "n":
                # No action is needed yet
                pass
            elif JF_units == "lbf":
                # Convert it to Newtons
                thrust = SESJFprops[0]
                SESJFprops[0] == USc.ConvertToSI("Force2", thrust, False, log)[0]
            elif JF_units == "m3/s":
                # Convert it to Newtons, even though we may convert
                # it back later.
                SESJFprops[0] == SESJFprops[0] * 1.2 * abs(SESJFprops[2])
            elif JF_units == "cfm":
                # Convert it to Newtons, even though we may convert
                # it back later.
                flow = USc.ConvertToSI("volflow", SESJFprops[0], False, log)[0]
                SESJFprops[0] == flow * 1.2 * abs(SESJFprops[2])
            jetfans.append(SESJFprops)
        elif "form8" in key:
            # Store the name of the route we want to copy over.
            routes.append(values)

    # Now process the axial fan characteristics and ensure they have
    # all been defined by "ses_7b" keywords.
    form7AB = {}
    for values in axial_fans:
        fan_name, discard, SESfan_tr_index = values
        fan_dict = bdat.fanchars_dict[fan_name]
        density = fan_dict["density"][0]
        # We'll use 30 seconds for the runup and rundown times.
        runup = 30.
        flowlimits = fan_dict["flowlimits"]
        # Set the extents.  SES has limits of -100,000 cfm and
        # +20,000,000 cfm so we include those too (in SI units)
        minflow = max(min(flowlimits[0], flowlimits[2]), -47.185)
        maxflow = min(max(flowlimits[1], flowlimits[3]), 943.89)
        # That's all we need for form 7A, we'll use the fan name as the
        # comment.
        UC_name = fan_dict["block_index"][1].split()[-1]
        descrip = ['"' + UC_name + '" fan characteristic']
        form7A = [density, runup, minflow, maxflow]
        keys = []
        for key, values in fan_dict.items():
            if "fan_7b" in key:
                keys.append(key)
            elif "datasource" in key:
                # Ugh.  This fan characteristic has been defined from
                # a datasource, not using the "SES_7B" keyword.
                tr_index = values[-1]
                line1_num, discard, line1_text = line_triples[SESfan_tr_index]
                line2_num, discard, line2_text = line_triples[tr_index]
                err = ('> The file named "' + file_name + '" had an\n'
                       '> invalid SES axial fan definition (forms 7A\n'
                       '> and 7B).  You tried to copy over data from\n'
                       '> Hobyah fan characteristic definition named\n'
                       '> "' + fan_name
                         + '".  Unfortunately it uses a datasource\n'
                       '> to define its characteristic, not the four\n'
                       '> pairs of flow and pressure that SES needs\n'
                       '> in its input files.\n'
                       '> Please edit the file to define one or more\n'
                       '> fan characteristics using the "form_7B" way\n'
                       '> of defining flow/pressure points and use\n'
                       '> those characteristics when exporting fan\n'
                       '> characteristics to SES.'
                      )
                gen.WriteError(3071, err, log)
                gen.ErrorOnTwoLines(line1_num, line1_text,
                                    line2_num, line2_text, log, False)
                return(None)
        # If we get to here, we only used the "fan_7B" keyword to set
        # fans.  These have already been converted to m^3/s and Pa if
        # they were originally in US units.  Also, these values have
        # not been adjusted for air density in the Hobyah calculation,
        # they are at the stated density.

        fwds_7B = fan_dict["fwd_for_SES"]
        rev_7B = fan_dict["rev_for_SES"]
        form7AB.__setitem__(fan_name, [descrip, form7A, fwds_7B, rev_7B])

    # The "tunnel" blocks have an optional argument in their "sespragmat"
    # entry.

    # Process the route entries and figure out which routes set entries
    # in which tunnels.  We use this to raise errors caused by dud or
    # absent entries in the "SESpragmat" line in the "tunnel" blocks.
    # grad_routes.__setitem__(tun_name, ["routeset", grad_route])
    for index, values in enumerate(routes):
        route_name, route_options, SESroute_index = values
        route_dict = bdat.routes_dict[route_name]
        tunnels_list = route_dict["tunnel_names"]
        for tun_name in tunnels_list:
            if tun_name in grad_routes:
                grad_routes[tun_name].append(route_name)
            else:
                grad_routes.__setitem__(tun_name, [route_name])
    # We now have entries in grad_routes for all the tunnels that
    # pass through routes.  We check for the following:
    #  * Cases in which the "route" optional entry told the program
    #    to use a route that the tunnel is not in.
    #  * Cases in which the "route" optional entry did not exist
    #    and the tunnel has more than one route passing through it.
    for tun_name, values in grad_routes.items():
        if values[0] == "routeset":
            # The tunnel definition has a pragmat that specified which
            # route to to take the gradients in this tunnel from.
            route_name = values[1]
            other_routes = values[2:]
            pragmat_index = bdat.tunnels_dict[tun_name]["sespragmat"][-1]
            if len(other_routes) == 0:
                # The tunnel definition a pragmat to choose a route but
                # the tunnel does not pass through any routes.
                err = ('> The file named "' + file_name + '" had\n'
                       '> conflicting SES input.  The tunnel named\n'
                       '> "' + tun_name + '" had an "SESpragmat" optional\n'
                       '> entry that told Hobyah to get the SES stack\n'
                       '> heights from the gradients in route "'
                         + route_name + '".\n'
                       '> Unfortunately that tunnel does not run through\n'
                       '> this route (it does not pass through any routes).\n'
                       '> Please edit the file to fix the conflict,\n'
                       '> either by removing the optional argument in\n'
                       '> the "SESpragmat" line or adding a definition\n'
                       '> for route "' + route_name + '".'
                      )
                line_num, discard, line_text = line_triples[pragmat_index]
                gen.WriteError(3072, err, log)
                gen.ErrorOnLine(line_num, line_text, log,
                                False, False, "Problematic")
                return(None)
            elif route_name not in other_routes:
                # The optional entry in the tunnel definition told the
                # program to use a route that does not pass through the
                # tunnel.
                err = ('> The file named "' + file_name + '" had\n'
                       '> conflicting SES input.  The tunnel named\n'
                       '> "' + tun_name + '" had an "SESpragmat" optional\n'
                       '> entry that told Hobyah to get the SES stack\n'
                       '> heights from the gradients in route "'
                         + route_name + '".\n'
                       '> Unfortunately that tunnel does not run through\n'
                       '> this route, it pass through the following routes:\n'
                         + gen.FormatOnLines(other_routes) + '\n'
                       '> Please edit the file to fix the conflict,\n'
                       '> either by removing the optional argument in\n'
                       '> the "SESpragmat" line or adding a definition\n'
                       '> for route "' + route_name + '".'
                      )
                line_num, discard, line_text = line_triples[tr_index]
                gen.WriteError(3073, err, log)
                gen.ErrorOnLine(line_num, line_text, log,
                                False, False, "Problematic")
                return(None)
            # If we get to here, all is well.  The route set in the
            # optional entry is the one that the segment stack heights
            # are taken from.
        elif len(values) > 1:
            # The tunnel passes through more than one route but the
            # user has not included an optional entry in the tunnel
            # definition to tell it which one to use.
            # Check if the tunnel definition has an "SESpragmat"
            # line of entry.
            if "sespragmat" in bdat.tunnels_dict[tun_name]:
                tr_index = bdat.tunnels_dict[tun_name]["sespragmat"][-1]
                descrip = "Problematic"
                text1 = ('> Please edit the file to fix the conflict by\n'
                         '> adding the optional argument "route" to the\n'
                         '> "SESpragmat" entry in the tunnel definition\n'
                         '> and choosing one of the above routes.')
            else:
                tr_index = bdat.tunnels_dict[tun_name]["block_index"][-1]
                descrip = "Relevant"
                text1 = ('> Please edit the file to fix the conflict\n'
                         '> by adding an "SESpragmat" entry with the\n'
                         '> optional argument "route" in it too choose\n'
                         '> one of the above routes.')
            err = ('> The file named "' + file_name + '" had\n'
                   '> conflicting SES input.  The tunnel named\n'
                   '> "' + tun_name + '" passes through the following\n'
                   '> routes:\n'
                     + gen.FormatOnLines(values) + '\n' + text1
                  )
            line_num, discard, line_text = line_triples[tr_index]
            gen.WriteError(3074, err, log)
            gen.ErrorOnLine(line_num, line_text, log,
                            False, False, descrip)
            return(None)
        else:
            # This tunnel is in one route.  We have no choice
            route_name = grad_routes[tun_name][0]
            grad_routes.__setitem__(tun_name, ("default route", route_name))

    # Now process the route definitions.  While we are doing this
    # we will get the elevations of all the SES nodes along the route
    # and add it to a dictionary so we can calculate stack heights
    # and turn them into form 3/form 5 stack heights.  We won't get
    # stack heights for everything but we'll get a lot.
    form8 = []
    node_elevs = {}

    for index, values in enumerate(routes):
        route_name, route_options, SESroute_index = values
        route_dict = bdat.routes_dict[route_name]

        if "orientation" in route_options:
            # The user wants the alignment reversed.  Oh joy!  Note
            # that we don't have to check the orientation because the
            # only valid entry is "orientation:=reverse".
            reverse = True
            descrip = ("SES route " + str(index + 1) + ', from Hobyah route "'
                      + route_name + '" in reverse')
        else:
            reverse = False
            descrip = ("SES route " + str(index + 1) + ', from Hobyah route "'
                      + route_name + '"')
        if Aur:
            form8A1 = "{" + descrip.ljust(80) + " Form 8A " + "}"
        else:
            form8A1 = descrip.ljust(80) + " Form 8A"

        # Figure out our form 8C, the tricky one.  Start by getting
        # all the chainages we will need to correctly place changes
        # of gradient and speed limit.  We get the entries and run
        # them through the 'set' function to remove duplicates.

        gradients, grad_chs = route_dict["gradients"]
        speed_chs = route_dict["speed_chs"]


        if route_dict["speed_plots"] == "Not set":
            # The route has no speed limits in it.  Set the speed
            # limits to 120 km/h.
            speeds = [120.] * len(speed_chs)
        else:
            speeds = route_dict["speed_plots"]

        origin = route_dict["elevgrad_chs"][0]
        values, chs = route_dict["radii"][:2]
        rad_chs, radii = ClipProfile(chs, values, origin)
        values, chs = route_dict["sectors"][:2]
        sector_chs, sectors = ClipProfile(chs, values, origin)
        values, chs = route_dict["coasting"][:2]
        coast_chs, coasting = ClipProfile(chs, values, origin)
        values, chs = route_dict["regenfractions"][:2]
        regen_chs, regenfractions = ClipProfile(chs, values, origin)

        # We need to do a little work on the content of 'speeds', which
        # may not be in the format we expect.  We need to double up the
        # first speed and remove the last before we call BothSidesProps.
        speeds = [speeds[0]] + speeds[:-1]
        gradlen = len(grad_chs)
        speedlen = len(speed_chs)
        chs = grad_chs + speed_chs + rad_chs + sector_chs + coast_chs
        # If we are writing to SVS we need to add the regen fraction
        # stuff.
        if target == "svs":
            chs.extend(regen_chs)
        # Remove duplicates by running the list through 'set' and making
        # remaking the list.
        chs = list(set(chs))

        if reverse:
            chs.sort(reverse=True)
            # Route origin, count of groups of trains, count of track sections,
            # spawn time of first train, first train type, coasting speed,
            # coasting option.  In reversed routes we can't use negative
            # numbers, so we'll start the route at zero.
            origin = chs.pop(0)
            form8A2 = [0.0, 1, len(chs), 9999, 1, 60.0, 0]
        else:
            chs.sort()
            origin = chs.pop(0)
            form8A2 = [origin, 1, len(chs), 9999, 1, 60.0, 0]
        # No need for a form 8B as we only have one group of trains
        # and it is in form 8A.
        # We make a list of lists of numbers for form 8C, one list per
        # line.
        form8C = []
        # Get the data setting the elevations in this route.  We
        # interpolate along these to get elevations at every SES node.
        elevgrad_chs = route_dict["elevgrad_chs"]
        elevations = route_dict["elevations"]
        if reverse:
            if origin > elevgrad_chs[-1]:
                # We extrapolate from the last pair of values.
                elev_offset = gen.Interpolate(elevgrad_chs[-2], elevgrad_chs[-1],
                                         elevations[-2], elevations[-1],
                                         origin, True, log)
            else:
                elev_offset = np.interp(origin, elevgrad_chs, elevations)
            x_text = "xmult:=-1 xoffset:=" + gen.RoundText(origin, 3) + ' )'
        else:
            elev_offset = elevations[0]
            x_mult = 1.0
            x_offset = 0.0
            x_text = " )"
        if math.isclose(elev_offset, 0.0):
            elev_text = " ( "
        else:
            elev_text = " ( yoffset:=" + gen.RoundText(elev_offset, 3)


        # Now build a line of text that we can append to the first line
        # of form 8C in the SES input file to the right of column 81.
        # This line of text gives the optional arguments needed in the
        # curve definition to translate the curves from SES so they sit
        # atop the curves from Hobyah.  This is particularly helpful
        # when the SES route definition is the reverse of the Hobyah
        # route definition - the multiplier is always -1 but figuring
        # out the offset is always a pain.
        if elev_text == " ( " and x_text == " )":
            # There are no optional arguments that need to be applied
            # to get the SES route curves to align with the Hobyah
            # route curves.
            offset_text = ' (no curve options needed)'
        elif elev_text == " ( " or x_text == " )":
            # We need to set xmult and xoffset, but not yoffset (or
            # vice-versa).
            offset_text = elev_text + x_text
        else:
            # We need to set all three.
            offset_text = elev_text + " " + x_text

        # Store the offset_text as the first entry in form 8C.
        # We append it to the words "Form 8C" after column 81 when we
        # write the SES input file below.
        form8C.append(offset_text)
        for ch in chs:
            # Set the chainage for this line of entry
            if reverse:
                loc_8C = [origin - ch]
            else:
                loc_8C = [ch]
            # Now figure out the gradient and speed limits here.
            gradpair = BothSidesProps(grad_chs, gradients, ch, log)
            speedpair = BothSidesProps(speed_chs, speeds, ch, log)
            radpair = BothSidesProps(rad_chs, radii, ch, log)
            sectorpair = BothSidesProps(sector_chs, sectors, ch, log)
            coastpair = BothSidesProps(coast_chs, coasting, ch, log)
            # We set the gradient, the elevation (always zero) the speed
            # limit there, the energy sector number, the coasting switch
            # and the regen braking fraction (SVS only).  The user can
            # adjust these in the SES input files as they see fit.
            if reverse:
                if math.isclose(gradpair[1], 0.):
                    grad = gradpair[1]
                else:
                    grad = -gradpair[1]
                loc_8C.extend([radpair[1], grad * 100, 0.0, speedpair[1],
                               sectorpair[1], coastpair[1]])
            else:
                loc_8C.extend([radpair[0], gradpair[0] * 100, 0.0, speedpair[0],
                               sectorpair[0], coastpair[0]])
            if target == "svs":
                regenpair = BothSidesProps(regen_chs, regenfractions, ch, log)
                if reverse:
                    loc_8C.append(regenpair[1])
                else:
                    loc_8C.append(regenpair[0])
            form8C.append(loc_8C)

        # Set no stops in the route and put one person on the train.
        form8D = (0, 1)

        tunnels_list = route_dict["signed_names"]

        # Get the chainage in the route that the tunnels start at and
        # build a list of section/segment numbers.
        if reverse:
            up_ptl_ch = origin - route_dict["tunnel_chs"][-1]
        else:
            up_ptl_ch = route_dict["tunnel_chs"][0]
        sec_list = []
        for index, tun_name in enumerate(tunnels_list):
            # Get the factors to convert the route chainages to tunnel
            # distances and vise-versa
            (mult, offset) = route_dict["route2tun"][index]

            if tun_name[0] == '-':
                tun_dict = bdat.tunnels_dict[tun_name[1:]]
                grad_route = grad_routes[tun_name[1:]][1]
                H_seg_list = bdat.tuns2frames[tun_name[1:]]
                H_seg_list.reverse()
                reverse2 = True
            else:
                tun_dict = bdat.tunnels_dict[tun_name]
                H_seg_list = bdat.tuns2frames[tun_name]
                grad_route = grad_routes[tun_name][1]
                reverse2 = False
            try:
                # Look for values.  We split this in two to make it
                # easy to add extra entries without losing track of
                # the optionals dictionary.
                block, seg_type = tun_dict["sespragmat"][:2]
                optionals_dict = tun_dict["sespragmat"][-2]
            except:
                # Set the defaults.
                block = 101
                seg_type = "line"
                optionals_dict = {}
            t_index, discard = divmod(block, 100)
            n_index = t_index
            if seg_type == "vent":
                lineseg = False
                vent_tunnels.append(tun_name)
            else:
                lineseg = True
            for H_seg_idx, H_seg in enumerate(H_seg_list):
                if H_seg not in form2A_dict:
                    # This is a vent segment, which an SES route cannot
                    # pass through.  Complain.
                    if tun_name[0] == '-':
                        tun_name = tun_name[1:]
                    err = ('> The file named "' + file_name + '" had\n'
                           '> conflicting SES input.  The segments in\n'
                           '> tunnel "' + tun_name + '" were converted\n'
                           '> into SES vent segments in the "SESdata"\n'
                           '> block but this tunnel is in route "'
                             + route_name + '".\n'
                           '> SES vent segments cannot be put into routes.\n'
                           '> Please edit the file to fix the conflict,\n'
                           '> either by changing the SES segment type\n'
                           '> to line segments or removing the tunnel\n'
                           '> from all routes.\n'
                          )
                    line1_num, discard, line1_text = line_triples[SESroute_index]
                    tr_index = bdat.tunnels_dict[tun_name]["sespragmat"][-1]

                    line2_num, discard, line2_text = line_triples[tr_index]
                    gen.WriteError(3075, err, log)
                    gen.ErrorOnTwoLines(line1_num, line1_text,
                                        line2_num, line2_text, log,
                                        False, False, "Conflicting")
                    return(None)

                # If we get to here, this is a line segment.
                S_sec, back_node, fwd_node = form2A_dict[H_seg][:3]
                if reverse2:
                    sec_list.append(-S_sec)
                else:
                    sec_list.append(S_sec)
                if route_name == grad_route:
                    # The gradients in this tunnel are to be taken from
                    # this route.
                    S_sec, back_node, fwd_node = form2A_dict[H_seg][:3]
                    # Get the distances of the gridpoints at the back and
                    # forward ends of the cells.
                    back_dist = bdat.dists[H_seg][0]
                    fwd_dist = bdat.dists[H_seg][-1]

                    back_ch = (back_dist - offset) / mult
                    back_elev = np.interp(back_ch, elevgrad_chs, elevations)
                    fwd_ch = (fwd_dist - offset) / mult
                    fwd_elev = np.interp(fwd_ch, elevgrad_chs, elevations)

                    # We do a check that we don't expect to be triggered,
                    # but you never know.
                    try:
                        old_elev = node_elevs[back_node]
                    except:
                        node_elevs.__setitem__(back_node, back_elev)
                    else:
                        if not math.isclose(back_elev, old_elev):
                            # We have a mismatch in node elevations.
                            print('Found two elevations at SES node '
                                   + str(back_node) + '!\n'
                                  'First was ' + gen.RoundText(old_elev,4)
                                   + 'second was ' + gen.RoundText(back_elev,4))
                            gen.OopsIDidItAgain(log)
                    try:
                        old_elev = node_elevs[fwd_node]
                    except:
                        node_elevs.__setitem__(fwd_node, fwd_elev)
                    else:
                        if not math.isclose(fwd_elev, old_elev):
                            # We have a mismatch in node elevations.
                            print('Found two elevations at SES node '
                                   + str(fwd_node) + '!\n'
                                  'First was ' + gen.RoundText(old_elev,4)
                                   + ', second was '
                                   + gen.RoundText(fwd_elev,4))
                            gen.OopsIDidItAgain(log)

        # If the SES route is the reverse of the Hobyah route,
        # we account for that by reversing the order of the segments
        # and flipping the signs.
        if reverse:
            sec_list.reverse()
            sec_list = [-sec for sec in sec_list]
        form8F = [[len(sec_list), up_ptl_ch], sec_list]
        form8.append((form8A1, form8A2, form8C, form8D, form8F))

    # We now have as many SES node elevations that we are going to get.
    # Loop over the entries in form 2, check for segments that have the
    # elevations set at both nodes and set the stack heights in the
    # segment definitions.
    keys = list(form2A_dict.keys()) + list(form2B_dict.keys())
    for key in keys:
        try:
            (seg_num, back_node, fwd_node) = form2A_dict[key][:3]
            lineseg = True
        except KeyError:
            (seg_num, back_node, fwd_node) = form2B_dict[key][:3]
            lineseg = False
        if back_node in node_elevs and fwd_node in node_elevs:
            # We know the elevations at both nodes.
            back_elev = node_elevs[back_node]
            fwd_elev = node_elevs[fwd_node]
            stack = fwd_elev - back_elev
            if lineseg:
                form3_dict[key][1][2] = stack
            else:
                form5_dict[key][1][7] = stack

    # Build a fake form 9 with a train on a heavy rail suburban system
    # using European loading gauge. This train has low efficiency,
    # poor aerodynamics and zero regen braking.  The entries here
    # are in SI units and set up as if we are writing to Aurecon SES
    # (roughness heights and grid diameters in metres, Davis terms in
    # N/kg, rotating mass fraction as a percentage of tare mass).
    # There are four motors per powered car, each with 19 kN TE.
    # SES has a problem in which it limits line current to no more
    # than twice the motor current, even if there are four motors
    # in parallel.  This crappy train definition gets around that
    # by telling SES it has two motors per powered car and each motor
    # has the TE, motor current and line current of two of these
    # motors.
    form9 = [
        ["A rough, blocky, inefficient train", 8, 6, 160, 10.5, "Form 9A totcars, pwdcars, length, area"],
        [12.8, 0.028, 0.0, 0.64, "perim, Darcy_fricfac, bogies, noseloss"],
        [12000, 1500., 100., 75., 17., 0.06, "sens & lat/car, sens & lat/pax, kW/car & pax"],
        [900., 700., 0.06, 0.6, 4., 4., 4., 4., "accel & decel: mass, diam, areas (conv & rad)"],
        [0.89, 0.9, 500., 500., 300., 300., "accel & decel: emissivity, specheat, temp"],
        [40.2, 2, 0.0064, 516, 0.00003806, 10., "mass (t), motors/car, N/kg, N, N-s/kg-m, [0 or %tare]"],
        ["Two 19 kN, 210 kW motors in one motor", 0.75, 0.75, "motor descrip, 2 wheel diameters"],
        [4.4, 4.4, 1500., 1500., 750., "2 gear ratios, 3 voltages (3rd is discarded)"],
        [34.9, 35.1, 77.5, 116, "Train speeds"],
        [38000., 38000., 17500., 11000., "Motor tractive efforts"],
        [292., 290., 280., 265., "Motor amps"],
        [2, "1 = cams (no forms 9H, 9J-9L), 2 = choppers"],
        [80.8, 832., 590., 560., 530., "Form 9H Line amps"],
        [87., 40, 89., 0., 1, "effic1, speed, effic2, regen, 2=flywheels"],
        [0., 0., 0., 0., 0.01, "Choppers: 4 zeros & 1 motor ohms (0.001-0.3)"],
        [1.3, 1.1, 40, 0.9, 120, "accel, decel<V1, V1, decel>V1, max V"],
            ]

    # We have no trains in the system and we put all the segments into
    # one zone below.
    # Run for 10 seconds and plot every second.
    form12 = [(3.5, 1), [10, 1., 1, 0, 2, 20, "count seconds abbrev ECZs aero & thermo cycles"]]
    form13 = [5, 10., 2, 5, "timestep, runtime, train cycles, wall cycles"]

    # Build the early forms with the counters.
    form1C = (1, 1, 1, 1, 0, 1, 0, 0, "tpopt  humid W/WB/RH ECZs therm supopt simerr inperr")
    form1D = [len(form3_dict), len(form2A_dict) + len(form2B_dict),
              len(form2B_dict), len(known_nodes), 1, 0, len(form4_dict),
              len(axial_fans)]
    if target == 'svs':
        # For some reason SVS removed the counter of portals entry and
        # moved everything after it ten characters to the left.
        form1D.pop(6)
        form1D.append("lsegs  sects  vsegs nodes use1   fires  fans")
    else:
        form1D.append("lsegs  sects  vsegs nodes use1  use0   fires  fans")

    form1E = (len(routes), 1, 1, 1, 0, len(jetfans), 0, 0,
              "routes trtyp  zones fcark trns  JFs    write  read")

    (s_name, s_dir, s_stem, s_ext) = gen.GetFileData(SES_name, '', debug1,
                                                     dir_name)
    if s_ext == '':
        # The user did not set an extension.
        if target == "open-ses":
            # OpenSES developers like using .INP.
            s_ext = '.INP'
        if target == "svs":
            s_ext = '.SIN'
        else:
            # Use .ses for the other program types.
            s_ext = '.ses'
    sesfile = open(s_dir + s_stem + s_ext, "w", encoding='utf-8')

    for line in form1A:
        gen.WriteOut(line, sesfile)
    # Do form 1B, which can have 3 or 4 entries (OpenSES)
    line = gen.FormatInpLine(form1B, (2, 0, 0, 5), "1B", USunits, log, Aur)
    gen.WriteOut(line, sesfile)

    if target == "offline-ses":
        # offline-SES's version number is on a line of its own.
        gen.WriteOut("  offline-SES  204.5", sesfile)

    line = gen.FormatInpLine(form1C[:-1], (0,) * 8, "1C", USunits,
                      log, Aur, form1C[-1])
    gen.WriteOut(line, sesfile)

    line = gen.FormatInpLine(form1D[:-1], (0,) * 8, "1D", USunits,
                      log, Aur, form1D[-1])
    gen.WriteOut(line, sesfile)

    line = gen.FormatInpLine(form1E[:-1], (0,) * 8, "1E", USunits,
                      log, Aur, form1E[-1])
    gen.WriteOut(line, sesfile)

    comment = form1F.pop(-1)
    decpls = (3, ) * 8
    if USunits:
        # Convert the units to US.
        keys =   ("temp", "temp", "press2", "temp", "temp",
                  "temp", "temp", "tdiff")

        form1F = [USc.ConvertToUS(keys[i], form1F[i], False, log)[0]
                        for i in range(len(keys))]
    elif target == "svs":
        # SVS wants t atmospheric pressure in kPa rather than in Pa.
        form1F[2] = form1F[2] / 1000.
    line = gen.FormatInpLine(form1F, decpls, "1F", USunits, log, Aur,
                             comment)
    gen.WriteOut(line, sesfile)

    # Do form 1G
    comment = form1G.pop(-1)
    decpls = (3, 3, 3, 3, 3, 3, 0, 2)
    if USunits:
        # Convert the units to US and adjust the count of decimal places.
        keys =   ("mass1", "null", "null", "null", "null",
                  "speed2", "null", "null")

        form1G = [USc.ConvertToUS(keys[i], form1G[i], False, log)[0]
                        for i in range(len(keys))]
    line = gen.FormatInpLine(form1G, decpls, "1G", USunits, log, Aur,
                             comment)
    gen.WriteOut(line, sesfile)

    # Two target programs have an extra form here.
    if target == "aurecon-ses":
        # Write Aurecon's form 101H with zero for the counters of templates,
        # the aero settings and the thermo settings.
        line = gen.FormatInpLine((5, 0, 0, 0, 0, 0), (0,) * 6, "101H", USunits,
                          log, Aur, "Form 101H")
        gen.WriteOut(line, sesfile)
    elif target == "svs":
        # Write WSP's form 1H with all the SVS features turned off or to
        # their defaults (cook the ground for 30 years and assume a
        # daily mean SHTC of 0.5 * the mean SHTC over the duration of
        # the ECZ estimate.
        decpls = (0, 1, 3, 0, 0)
        line = gen.FormatInpLine((0, 30., 0.5, 0, 0), decpls, "1H", USunits,
                          log, Aur, "Form 1H")
        gen.WriteOut(line, sesfile)


    # Write form 2.  If we ever start using Hobyah to set non-zero
    # values of volume flow we will need to convert to US units.  But
    # at the moment the numbers will always be zero.
    for index, (key, values) in enumerate(form2A_dict.items()):
        if index == 0:
            line = gen.FormatInpLine(values, (0, 0, 0, 0, 3), "2A", USunits,
                   log, Aur, "Form 2A")
        else:
            line = gen.FormatInpLine(values, (0, 0, 0, 0, 3), "2A", USunits,
                   log, Aur, "")
        gen.WriteOut(line, sesfile)

    for index, (key, values) in enumerate(form2B_dict.items()):
        if index == 0:
            line = gen.FormatInpLine(values, (0, 0, 0, 3), "2B", USunits,
                   log, Aur, "Form 2B")
        else:
            line = gen.FormatInpLine(values, (0, 0, 0, 3), "2B", USunits,
                   log, Aur, "")
        gen.WriteOut(line, sesfile)

    # Do form 3 with suitable conversions.
    for index1, (key, lines) in enumerate(form3_dict.items()):
        if index1 == 0:
            if Aur:
                # Add 3A to the segment description.
                lines[0][2] = lines[0][2].ljust(60) + " Form 3A"
                line = gen.FormatInpLine(lines[0], (0, 0), "3A1", USunits,
                                         log, Aur)
            else:
                line = gen.FormatInpLine(lines[0], (0, 0), "3A1", USunits,
                                         log, Aur, "Form 3A")
        else:
            line = gen.FormatInpLine(lines[0], (0, 0), "3A1", USunits,
                                     log, Aur)
        gen.WriteOut(line, sesfile)

        decpls = [2, 2, 4, 0]
        if USunits:
            # Convert the units to US and adjust the count of decimal places.
            keys = ["dist1", "area", "dist1", "null"]
            lines[1] = [USc.ConvertToUS(keys[i], lines[1][i], False, log)[0]
                            for i in range(len(keys))]
        elif target == "svs":
            # We add a zero value for fixed segment pressure rise (Pa)
            decpls.append(3)
            lines[1].append(0.0)

        line = gen.FormatInpLine(lines[1], decpls, "3A2", USunits,
                                 log, Aur, "")
        gen.WriteOut(line, sesfile)

        if USunits:
            # Convert the units to US and adjust the count of decimal places.
            keys = ["dist1"] * len(lines[2])
            lines[2] = [USc.ConvertToUS(keys[i], lines[2][i], False, log)[0]
                            for i in range(len(keys))]
        line = gen.FormatInpLine(lines[2], (3, ) * 8, "3B1", USunits,
                                 log, Aur, "")
        gen.WriteOut(line, sesfile)

        decpl = (4, ) * 8
        if USunits:
            # Convert the units to US and adjust the count of decimal places.
            keys = ["dist1"] * len(lines[3])
            lines[3] = [USc.ConvertToUS(keys[i], lines[3][i], False, log)[0]
                            for i in range(len(keys))]
        elif target == "svs":
            # SVS wants the roughness heights in mm instead of m.
            for index in range(len(lines[3])):
                lines[3][index] = lines[3][index] * 1000.
                decpl = (2, ) * 8
        line = gen.FormatInpLine(lines[3], decpl, "3B2", USunits,
                                 log, Aur)
        gen.WriteOut(line, sesfile)

        # All entries in form 3D are dimensionless.
        line = gen.FormatInpLine(lines[4], (3, 3, 3, 3, 2, 0, 0), "3C",
                                 USunits, log, Aur)
        gen.WriteOut(line, sesfile)


        if USunits:
            # Convert the units to US and adjust the count of decimal places.
            keys = ["null", "null", "temp", "temp", "temp"]
            lines[5] = [USc.ConvertToUS(keys[i], lines[5][i], False, log)[0]
                            for i in range(len(keys))]
        line = gen.FormatInpLine(lines[5], (0, 0, 3, 3, 3), "3E", USunits,
                                 log, Aur)
        gen.WriteOut(line, sesfile)

        decpl = (3, 2, 1, 12, 4, 12, 3)
        if USunits:
            # Convert the units to US and adjust the count of decimal places.
            keys = ["dist1", "dist1", "v41_thcon", "diff", "v41_thcon",
                    "diff", "temp"]
            decpl = (3, 2, 4, 4, 4, 4, 3)
            lines[6] = [USc.ConvertToUS(keys[i], lines[6][i], False, log)[0]
                            for i in range(len(keys))]
        line = gen.FormatInpLine(lines[6], decpl, "3F", USunits, log, Aur)
        gen.WriteOut(line, sesfile)

    for fire_num, (form4A, form4B) in form4_dict.items():
        line = gen.FormatInpLine(form4A, (0, 0, 0), "4_1", USunits,
                                 log, Aur)
        gen.WriteOut(line, sesfile)

        if target == 'aurecon-ses':
            # Fire sizes in Aurecon SES are MW and this is what the
            # heat release rates were stored as above.
            decpl = (6, 6, 1, 1, 3, 3)
        elif target == "svs":
            # Fire sizes in SVS are watts.
            form4B[0] = form4B[0] * 1e6
            form4B[1] = form4B[1] * 1e6
            decpl = (0, 0, 1, 1, 3, 3)
        else:
            # Convert the units to US and adjust the count of decimal places
            # to match BTU/hr.
            keys = ["v41_Mwatt", "v41_Mwatt", "null", "null", "temp", "area"]
            decpl = (0, 0, 1, 1, 3, 3)
            form4B = [USc.ConvertToUS(keys[i], form4B[i], False, log)[0]
                            for i in range(len(keys))]
        line = gen.FormatInpLine(form4B, decpl, "4_2", USunits,
                                 log, Aur, "Form 4A")
        gen.WriteOut(line, sesfile)

    for index1, (key, lines) in enumerate(form5_dict.items()):
        if index1 == 0:
            if Aur:
                # Add 3A to the segment description.
                lines[0][2] = lines[0][2].ljust(60) + " Form 5A"
                line = gen.FormatInpLine(lines[0], (0, 0), "5A", USunits,
                                         log, Aur)
            else:
                line = gen.FormatInpLine(lines[0], (0, 0), "5A", USunits,
                                         log, Aur, "Form 5A")
        else:
            line = gen.FormatInpLine(lines[0], (0, 0), "5A", USunits,
                                     log, Aur)
        gen.WriteOut(line, sesfile)

        decpl = (0, 0, 4, 3, 3, 3, 3, 3)
        if USunits:
            # Convert the units to US and adjust the count of decimal places.
            keys = ["null", "null", "area", "speed1",
                    "temp", "temp", "temp", "dist1"]
            decpl = (0, 0, 2, 2, 3, 3, 3, 4)
            lines[1] = [USc.ConvertToUS(keys[i], lines[1][i], False, log)[0]
                            for i in range(len(keys))]
        line = gen.FormatInpLine(lines[1], decpl, "5B",
                                 USunits, log, Aur)
        gen.WriteOut(line, sesfile)

        # Do the axial fans in form 5C.  No fan type set and turned off.
        line = gen.FormatInpLine(lines[2], (0, 1, 1, 0), "5C",
                                 USunits, log, Aur)
        gen.WriteOut(line, sesfile)

        decpl = (2, 3, 2, 2, 2, 2, 2)
        if USunits:
            # Convert the units to US and adjust the count of decimal places.
            keys = ["dist1", "area", "dist1", "null", "null", "null", "null"]
            decpl = (3, 3, 3, 2, 2, 2, 2)
            lines[3] = [USc.ConvertToUS(keys[i], lines[3][i], False, log)[0]
                            for i in range(len(keys))]
        line = gen.FormatInpLine(lines[3], decpl, "5D", USunits, log, Aur)
        gen.WriteOut(line, sesfile)

    # Do form 6.  All nodes are either straight-through (portals) or type 7
    # (zero pressure loss).
    comment = "Form 6A"
    for index, (key, node_defn) in enumerate(known_nodes.items()):
        node_num = node_defn[0]
        if len(node_defn) == 2:
            # There is only one segment attached here, so it is type 0.
            line = gen.FormatInpLine((node_num, 0, 3), (0, 0, 0), "6A",
                                     USunits, log, Aur, comment)
            gen.WriteOut(line, sesfile)
            # Set the boundary temperatures to zero so they are the
            # outside air conditions.
            line = gen.FormatInpLine((0.,) * 6, (3,)*6, "6B",
                                     USunits, log, Aur)
            gen.WriteOut(line, sesfile)
        else:
            # We put the attached segment numbers after the 81st character.
            comment = 'Segments: ' + '  '.join([str(seg_num) for seg_num
                                                in node_defn[1:]])
            line = gen.FormatInpLine((node_num, 7, 1), (0, 0, 0), "6A",
                                     USunits, log, Aur, comment)
            gen.WriteOut(line, sesfile)
        # Clear the comment text after writing the first entry.
        comment = ""

    # Do forms 7A and 7B, if there are any.
    for index, (key, lines) in enumerate(form7AB.items()):
        decpl = [5, 2, 3, 3]
        if USunits:
            # Convert the units to US and adjust the count of decimal places.
            keys = ["dens1", "null", "volflow", "volflow"]
            lines[1] = [USc.ConvertToUS(keys[i], lines[1][i], False, log)[0]
                                        for i in range(len(keys))]
            decpl = [4, 1, 0, 0]
        # Add a zero value for the description
        decpl = [0] + decpl
        line = gen.FormatInpLine(lines[0] + lines[1], decpl, "7A",
                                 USunits, log, Aur)
        gen.WriteOut(line, sesfile)

        # Loop over two lines for flows and pressures and write them out.
        for char_index in (2, 3):
            flows, pressures = lines[char_index]
            form7B = []
            for p, Q in zip(pressures, flows):
                form7B.append(p)
                form7B.append(Q)
            decpl = (1, 3) * 4
            if USunits:
                # Convert the units to US and adjust the count of decimal places.
                keys = ["press1", "volflow"] * 4
                decpl = (4, 0) * 4
                form7B = [USc.ConvertToUS(keys[i], form7B[i], False, log)[0]
                                for i in range(len(keys))]
            if index == 0 and char_index == 2:
                # First fan characteristic definition, tag it.
                line = gen.FormatInpLine(form7B, decpl, "7B", USunits, log, Aur, "Form 7A")
            else:
                line = gen.FormatInpLine(form7B, decpl, "7B", USunits, log, Aur)
            gen.WriteOut(line, sesfile)

    # Do form 7C, if there are any.
    for index, form7C in enumerate(jetfans):
        if index == 0:
            comment = "Form 7C"
        else:
            comment = ""

        if target == "aurecon-ses":
            # Aurecon SES has volume flow in m^3/s and jet velocity
            # in m/s.  It assumes that the air density the jet fan was
            # tested at is 1.2 kg/m^3.
            decpl = (2, 5, 2, 1, 1)
            form7C[0] = form7C[0] / (1.2 * abs(form7C[2]))
        elif USunits:
            # We need to calculate the volume flow needed to give the
            # eqiuvalent thrust.
            form7C[0] = form7C[0] / (1.2 * abs(form7C[2]))
            # Convert the units to US and adjust the count of decimal places.
            keys = ("volflow", "null", "speed1", "null", "null")
            decpl = (1, 5, 2, 1, 1)
            form7C = [USc.ConvertToUS(keys[i], form7C[i], False, log)[0]
                            for i in range(len(keys))]
        elif target == "svs":
            # SVS has thrust in N and jet velocity in m/s.  SVS adds
            # air density, which we'll set 1.2 kg/m^3.
            # SVS also has a switch to turn jet fan density derating
            # on or off, which we'll turn on.
            form7C.extend([1.2, 1.0])
            decpl = (2, 5, 2, 1, 1, 3, 1)
        line = gen.FormatInpLine(form7C, decpl, "7C", USunits,
                                 log, Aur, comment)
        gen.WriteOut(line, sesfile)

    # Do forms 8, if there are any.
    for (form8A1, form8A2, form8C, form8D, form8F) in form8:
        gen.WriteOut(form8A1, sesfile)

        # Do form 8A2.
        decpl = (3, 0, 0, 1, 0, 3, 0)
        if USunits:
            # Convert the units to US and adjust the count of decimal places.
            keys = ("dist1", "null", "null", "null", "null", "speed2", "null")
            form8A2 = [USc.ConvertToUS(keys[i], form8A2[i], False, log)[0]
                             for i in range(len(decpl))]
        line = gen.FormatInpLine(form8A2, decpl, "8A", USunits, log, Aur)
        gen.WriteOut(line, sesfile)

        # Do form 8C
        if USunits:
            keys = ("dist1", "dist1", "null", "dist1", "speed2", "null",
                    "null", "null")
            decpl = [3, 3, 4, 1, 2, 0, 0]
        elif target == "svs":
            # SVS has an extra column of entries for local regen factor.
            decpl = [3, 3, 4, 2, 2, 0, 0, 3]
        else:
            decpl = [3, 3, 4, 2, 2, 0, 0]

        offset_text = form8C.pop(0)
        for index, values in enumerate(form8C):
            # When we rebuild form 8C, we cannot tell from the output file
            # whether form 8C in the original input file had inputs of gradient
            # or stack height.  I generally use gradients, so we will rebuild
            # the form with the gradients, not the heights.  Competent users
            # who prefer stack height can change it if they wish.
            if USunits:
                # Convert the units to US and adjust the count of decimal places.
                values = [USc.ConvertToUS(keys[i], values[i], False, log)[0]
                                for i in range(len(decpl))]
            if index == 0:
                line = gen.FormatInpLine(values, decpl, "8C", USunits,
                                         log, Aur, "Form 8C" + offset_text)
            else:
                line = gen.FormatInpLine(values, decpl, "8C", USunits, log, Aur)
            gen.WriteOut(line, sesfile)

        line = gen.FormatInpLine(form8D, (0, 0), "8D", USunits, log, Aur)
        gen.WriteOut(line, sesfile)

        # Rewrite form 8F
        form8F1 = form8F[0]
        decpl = (0, 3)
        if USunits:
            # Convert the units to US and adjust the count of decimal places.
            keys = ("null", "dist1")
            decpl = (0, 3)
            form8F1 = [USc.ConvertToUS(keys[i], form8F1[i], False, log)[0]
                            for i in range(len(form8F1))]
        line = gen.FormatInpLine(form8F1, decpl, "8F_1", USunits,
                                 log, Aur, "Form 8F")
        gen.WriteOut(line, sesfile)
        for sec in form8F[1]:
            gen.WriteOut(str(sec), sesfile)

    # Form 9A
    descrip = form9[0].pop(-1)
    decpl = (0, 0, 0, 2, 3)
    if Aur:
        # We can't write the form 9A guidance on the line for form 9A
        # because having two strings in brackets on the one line isn't
        # allowed in Aurecon SES. We add 9A to the train description.
        form9[0][0] = form9[0][0] + ", 9A"
        descrip = ''

    if USunits:
        keys = ("null", "null", "null", "dist1", "area")
        decpl = (0, 0, 0, 2, 2)
        # Convert the units to US and adjust the count of decimal places.
        form9[0] = [USc.ConvertToUS(keys[i], form9[0][i], False, log)[0]
                        for i in range(len(keys))]
    line = gen.FormatInpLine(form9[0], decpl, "9A", USunits, log, Aur, descrip)
    gen.WriteOut(line, sesfile)

    # Form 9B
    descrip = form9[1].pop(-1)
    decpl = [2, 5, 3, 3]
    if USunits:
        keys = ("dist1", "null", "null", "null")
        # Convert the units to US and adjust the count of decimal places.
        form9[1] = [USc.ConvertToUS(keys[i], form9[1][i], False, log)[0]
                        for i in range(len(keys))]
    elif target == "svs":
        # SVS lets the user set a custom tail loss factor.  If it is
        # set to zero, SVS apparently uses the Hoerner calculation that
        # SES v4.1 has Garage.for.
        decpl.append(3)
        form9[1].append(0.0)
        descrip = descrip + ",  0/tailloss"
    line = gen.FormatInpLine(form9[1], decpl, "9B", USunits, log, Aur, descrip)
    gen.WriteOut(line, sesfile)

    # Form 9C
    descrip = form9[2].pop(-1)
    decpl = (3,)*6
    if USunits:
        keys = ("v41_watt1", "v41_watt1", "v41_watt1",
                "v41_watt1", "null", "null")
        decpl = (2, 2, 2, 2, 3, 3)
        # Convert the units to US and adjust the count of decimal places.
        form9[2] = [USc.ConvertToUS(keys[i], form9[2][i], False, log)[0]
                        for i in range(len(keys))]
    line = gen.FormatInpLine(form9[2], decpl, "9C", USunits, log, Aur, descrip)
    gen.WriteOut(line, sesfile)

    # Form 9D part 1
    descrip = form9[3].pop(-1)
    decpl = (2, 2, 5, 5, 3, 3, 3, 3)
    if USunits:
        keys = ("mass1", "mass1", "dist4", "dist4",
                "area", "area", "area", "area")
        decpl = (2, 2, 2, 2, 2, 2, 2, 2)
        # Convert the units to US and adjust the count of decimal places.
        form9[3] = [USc.ConvertToUS(keys[i], form9[3][i], False, log)[0]
                        for i in range(len(keys))]
    line = gen.FormatInpLine(form9[3], decpl, "9D_1", USunits, log, Aur, descrip)
    gen.WriteOut(line, sesfile)

    # Form 9D part 2
    descrip = form9[4].pop(-1)
    decpl = [3, 3, 4, 4, 3, 3]
    if USunits:
        keys = ("null", "null", "v41_specheat", "v41_specheat", "temp", "temp")
        decpl = (2, 2, 3, 3, 3, 3)
        # Convert the units to US and adjust the count of decimal places.
        form9[4] = [USc.ConvertToUS(keys[i], form9[4][i], False, log)[0]
                        for i in range(len(keys))]
    elif target == "svs":
        # SVS lets the user set a blower air speed for the grids.  If it
        # is set to zero, the annulus air velocity is apparently used.
        decpl.extend([3, 3])
        form9[4].extend([0.0, 0.0])
        descrip = descrip + ", blower speeds"
    line = gen.FormatInpLine(form9[4], decpl, "9D_2", USunits, log, Aur, descrip)
    gen.WriteOut(line, sesfile)

    # Form 9E part 1
    descrip = form9[5].pop(-1)
    decpl = (3, 0, 5, 2, 7, 3)
    if USunits:
        keys = ("mass3", "null", "Aterm1a", "Force1", "Bterm1a", "rotmass")
        decpl = (3, 0, 3, 2, 5, 3)
        # Convert the units to US and adjust the count of decimal places.
        form9[5] = [USc.ConvertToUS(keys[i], form9[5][i], False, log)[0]
                        for i in range(len(keys))]
        descrip = "mass (tons), motors/car, lb/ton, lb, lb/ton-mph, "  \
                  "[0 or (lb/ton)/(mph/sec)]"
    elif target == "svs":
        # SVS sets the gyroscopic mass in kg rather than as a percentage of
        # tare mass.  SVS also needs two Davis coefficients in terms of
        # N/tonne and N/tonne-kph.  Both programs accept a zero value
        # and turn it into 9.65 % tare mass.
        # Hobyah uses the same units as Aurecon SES does; % tare mass,
        # N/kg and N-s/kg-m.
        form9[5][2] = form9[5][2] * 1000.  # N per kg to N per tonne
        form9[5][4] = form9[5][4] * 3600.  # N per kg-m/s to N per tonne-kph
        form9[5][-1] = 0.0 # If we set 0.0, SVS apparently uses 9.65% tare mass
        decpl = (3, 0, 3, 2, 4, 2)
        descrip = "mass (t), motors/car, N/tonne, N, N/tonne-kph, [0 or kg]"
    line = gen.FormatInpLine(form9[5], decpl, "9E", USunits, log, Aur, descrip)
    gen.WriteOut(line, sesfile)

    # Form 9F part 1, (unitless entries)
    descrip = form9[6].pop(-1)
    if Aur:
        descrip = ''
    decpl = (0, 3, 3)
    if USunits:
        keys = ("null", "dist4", "dist4")
        decpl = (0, 2, 2)
        # Convert the units to US and adjust the count of decimal places.
        form9[6] = [USc.ConvertToUS(keys[i], form9[6][i], False, log)[0]
                        for i in range(len(keys))]
    line = gen.FormatInpLine(form9[6], decpl, "9F1", USunits, log, Aur, descrip)
    gen.WriteOut(line, sesfile)

    # Form 9F part 2
    descrip = form9[7].pop(-1)
    decpl = (3, 3, 1, 1, 1)
    line = gen.FormatInpLine(form9[7], decpl, "9F2", USunits, log, Aur, descrip)
    gen.WriteOut(line, sesfile)

    # Form 9G part 1
    descrip = form9[8].pop(-1)
    decpl = (3,)*4
    if USunits:
        keys = ("speed2",) * 4
        # Convert the units to US and adjust the count of decimal places.
        form9[8] = [USc.ConvertToUS(keys[i], form9[8][i], False, log)[0]
                        for i in range(len(keys))]
    line = gen.FormatInpLine(form9[8], decpl, "9G1", USunits, log, Aur, descrip)
    gen.WriteOut(line, sesfile)

    # Form 9G part 2
    descrip = form9[9].pop(-1)
    decpl = (3,)*4
    if USunits:
        keys = ("Force1",) * 4 # tractive effort in Newtons
        # Convert the units to US and adjust the count of decimal places.
        form9[9] = [USc.ConvertToUS(keys[i], form9[9][i], False, log)[0]
                        for i in range(len(keys))]
    line = gen.FormatInpLine(form9[9], decpl, "9G2", USunits, log, Aur, descrip)
    gen.WriteOut(line, sesfile)

    # Form 9G part 3
    descrip = form9[10].pop(-1)
    decpl = (3,)*4
    line = gen.FormatInpLine(form9[10], decpl, "9G3", USunits, log, Aur, descrip)
    gen.WriteOut(line, sesfile)

    # Form 9G part 4
    descrip = form9[11].pop(-1)
    decpl = (0,)
    line = gen.FormatInpLine(form9[11], decpl, "9G4", USunits, log, Aur, descrip)
    gen.WriteOut(line, sesfile)


    # Form 9H part 1
    descrip = form9[12].pop(-1)
    decpl = (3, 3, 3, 3, 3)
    line = gen.FormatInpLine(form9[12], decpl, "9H1", USunits, log, Aur, descrip)
    gen.WriteOut(line, sesfile)

    # Form 9H part 2
    descrip = form9[13].pop(-1)
    decpl = (3, 3, 4, 4, 4)
    if USunits:
        keys = ("null", "speed2", "null", "null", "null")
        # Convert the units to US and adjust the count of decimal places.
        form9[13] = [USc.ConvertToUS(keys[i], form9[13][i], False, log)[0]
                        for i in range(len(keys))]
    line = gen.FormatInpLine(form9[13], decpl, "9H2", USunits, log, Aur, descrip)
    gen.WriteOut(line, sesfile)


    # Form 9I
    descrip = form9[14].pop(-1)
    decpl = (3, 3, 4, 4, 4)
    if USunits:
        keys = ("speed2", "speed2", "null", "null", "null")
        # Convert the units to US and adjust the count of decimal places.
        form9[14] = [USc.ConvertToUS(keys[i], form9[14][i], False, log)[0]
                        for i in range(len(keys))]
    line = gen.FormatInpLine(form9[14], decpl, "9I", USunits, log, Aur, descrip)
    gen.WriteOut(line, sesfile)

    # Form 9J
    descrip = form9[15].pop(-1)
    decpl = (3,)*5
    if USunits:
        keys = ("accel", "accel", "speed2", "accel", "speed2")
        # Convert the units to US and adjust the count of decimal places.
        form9[15] = [USc.ConvertToUS(keys[i], form9[15][i], False, log)[0]
                        for i in range(len(keys))]
    line = gen.FormatInpLine(form9[15], decpl, "9I", USunits, log, Aur, descrip)
    gen.WriteOut(line, sesfile)

    # Put all the segments into one uncontrolled zone.
    line = '2         ' + str(len(form2A_dict) + len(form2B_dict))
    if Aur:
        line = line.ljust(80) + " {Form 11A}"
    else:
        line = line.ljust(81) + "Form 11A"

    gen.WriteOut(line, sesfile)

    # Run for 10 seconds and plot every second.
    decpl = (3, 0)
    if USunits:
        keys = ("tdiff", "null")
        # Convert the units to US and adjust the count of decimal places.
        form12[0] = [USc.ConvertToUS(keys[i], form12[0][i], False, log)[0]
                        for i in range(len(keys))]
    line = gen.FormatInpLine(form12[0], decpl, "12A", USunits, log, Aur, "Form 12A")
    gen.WriteOut(line, sesfile)

    descrip = form12[1].pop(-1)
    decpl = (0, 1, 0, 0, 0, 0)
    line = gen.FormatInpLine(form12[1], decpl, "12B", USunits, log, Aur, descrip)
    gen.WriteOut(line, sesfile)

    descrip = form13.pop(-1)
    decpl = (0, 1, 0, 0)
    line = gen.FormatInpLine(form13, decpl, "13", USunits, log, Aur, descrip)
    gen.WriteOut(line, sesfile)
    # Add a blank line so that anyone adding turning on a read option
    # doesn't get an EOF error if they forget to add the restart read
    # filename at the end.

    gen.WriteOut("\n\n*** End of input  ***\n", sesfile)

    # Write the segments that would be in form 11B, so that people can
    # slice and dice it if they need to populate zones.
    seg_list = [form3_dict[key][0][0] for key in form3_dict] +  \
               [form5_dict[key][0][0] for key in form5_dict]
    quot, rem = divmod(len(seg_list), 8)
    if rem != 0:
        quot += 1
    decpl = (0,)*8
    start = 0
    comment = "Form 11B"
    for index_11B in range(quot):
        start = index_11B * 8
        form11B = (seg_list[start:start + 8])

        line = gen.FormatInpLine(form11B, decpl, "11B", USunits,
                                 log, Aur, comment)
        gen.WriteOut(line, sesfile)
        # Reset the comment
        comment = ""

    sesfile.close()
    return("SES file written out")





def AllocateCounter(counters, index):
    '''Take a list of counters of SES segments or SES nodes and an
    index in the list.  Take the current number from that place
    in the list and increment the stored value for the next call.
    Each entry in the list of counters covers a block of 100 numbers
    (e.g. 101 to 200).  If the act of generating the segment/node
    numbers leads to us taking more than 100 numbers from a block,
    we call recursively so that we don't re-use numbers in the next
    block.
    '''
    new_num = counters[index]
    counters[index] += 1
    quot, rem = divmod(counters[index], 100)
    if rem == 1:
        # We have used all 100 numbers in this block and are just
        # about to start overwriting numbers in the next block up.
        # Instead we choose to steal the current number from the
        # next block up instead.
        # We call recursively so that if we overflow the block above,
        # we go on to the next one.  We have 120 of these blocks
        # of 100 numbers to choose from, so we probably won't reach
        # the last one even if we're writing new Aurecon SES input
        # files - we'd need to over 1,100 overflowed numbers.
        counters[index] -= 1
        new_num, counters = AllocateCounter(counters, index + 1)
    return(new_num, counters)


def SanitiseTimeSeries(time_list, accuracy, duration):
    '''Take a list of times, round them to eight decimal places, remove
    any duplicates, sort the times into ascending order and remove
    any times above 'duration'.
    We do this because different calls to numpy.arange() can generate
    slightly different times, depending on what parameters are passed.
    Also, chained ranges may have accidental overlaps.

        Parameters:
            time_list       []              A list of times that may contain
                                            duplicates and not be sorted into
                                            ascending order.
            accuracy        int             A counter of decimal places to
                                            round to.  Typically 8.
            duration        float           All times higher than this are
                                            removed.

        Returns:
            new _list        []             A list of rounded times with
                                            duplicates removed, sorted
                                            into ascending order and
                                            possibly truncated.
    '''
    # First we round all the times and remove those that are over the
    # duration.
    rounded = [round(time, accuracy) for time in time_list if time <= duration]
    # "set" turns a list with possibly duplicate items into a set of
    # unique items in no particular order.  Note that we can't put the 'set'
    # in the list comprehension above because it may treat two non-rounded
    # times that are very close together as different times.
    new_list = list(set(rounded))
    new_list.sort()
    return(new_list)


def ProcessFanChar(line_triples, tr_index, settings_dict, log):
    '''Take "begin fanchar...end fanchar" block in the input
    file and process its entries.

        Parameters:
            line_triples [(int, str, str)]  List of lines in the file.  First
                                            entry is the line number in the file
                                            (starting at one, not zero).
                                            Second is the valid data on the line.
                                            Third is the entire line (including
                                            comments) also used in error messages.
            tr_index        int             Where to start reading the fan
                                            characteristic definition.
            settings_dict   {}              Dictionary of the run settings.
            log             handle          The handle of the logfile.

        Returns:
            char_name        str            Name to assign to this fan char,
                                            by which fans in tunnels can refer
                                            to it by.
            char_defn        {}             A dictionary that defines a fan char.
    '''
    debug1 = settings_dict["debug1"]
    file_name = settings_dict["file_name"]
    psi = settings_dict["psi"]

    # Set the default values for fans.  These are the reference air density
    # that the pressures are recorded at and the fan start-up and
    # run-down times in the "fan1" keyword (these are ignored in the "fan2"
    # keyword).
    fan_defn = {"density": (1.2, {}, math.nan)} # 1.2 kg/m^3

    # Build the list of valid settings.  Some of these are worth explaining
    # for future reference:
    #  * "datasource" takes the name of a "begin data <name>"..."end data"
    #    block and two columns in a data block.  It uses the first column
    #    as the volume flows on the fan characteristic and the second column
    #    as the fan total pressures on the fan characteristic.  Note that
    #    all the inputs are fan total pressure.  Fan static pressure is
    #    never used as input to Hobyah, though there is an entry below
    #    for fan diameter, allows user to plot fan static pressure.
    #  * SES_7B takes four pressures and four volume flows.  The first
    #    volume flow should be zero and the fourth pressure should be zero.
    #    The program builds a spline curve from the points and calls that a
    #    fan characteristic.  This is the same interpolation algorithm that
    #    is used in SES's routine "FINS.FOR".  It is only needed for
    #    compatibility with SES.  It laid out in the same way as SES form 7B
    #    (pressure first, then volume flow).  Not implemented yet.
    valid_settings = {"density": ("float + dens1 a fan reference density",),
                      "datasource": ("#name", "#name", "#name",),
                      "ses_7b":   ("float any press1   1st pressure",
                                    "float any volflow  1st flowrate",
                                    "float any press1   2nd pressure",
                                    "float any volflow  2nd flowrate",
                                    "float any press1   3rd pressure",
                                    "float any volflow  3rd flowrate",
                                    "float any press1   4th pressure",
                                    "float any volflow  4th flowrate"),
                      "limits":  ("float -0 volflow  a minimum volume flow",
                                   "float  + volflow  a maximum volume flow"),
                      "diameter": ("float + null dist1 a fan diameter",),
                     }
    # We make a list of entries that we must have, which is none.  We
    # actually need one of "datasource" or "SES_7B", but we can't check
    # that in ProcessBlock, we'll have to check it here.
    requireds = []

    # We make a dictionary of the optional keywords and a list of the
    # allowable duplicates.
    # We allow the values in the SES_7B line of entry to be in US
    # units even in files that expect SI units.
    optionals = {"ses_7b":   {"direction": ("forwards", "reverse"),
                              "units": ("si", "us")},
                 "limits":   {"units": ("si", "us")},
                 "datasource":{"direction": ("forwards", "reverse")},
                }
    duplicates = ("ses_7b", "datasource")
    settings = (valid_settings, requireds, optionals, duplicates)

    block_name = "fanchar"
    # Now call ProcessBlock.  It returns None if an error occurred.  It
    # returns the updated fan dictionary.
    result = ProcessBlock(line_triples, tr_index, settings_dict,
                          block_name, fan_defn, settings, log)
    if result is None:
        return(None)
    else:
        (char_name, new_char_dict) = result
    if debug1:
        print("In fanchar", new_char_dict)

    # Get a list of the keys and strip off the words after the "#".
    # We check if there was at least one "datasource" entry or an
    # "SES_7B" entry.

    simple_keys = [key.split(sep = "#")[0] for key in new_char_dict]
    # When we get to here we could have a block without a curve definition.
    if "ses_7b" not in simple_keys and "datasource" not in simple_keys:
        data_names = settings_dict["user_data"].keys()
        err = ('> The file named "' + file_name + '" does\n'
               '> not have an entry that defines characteristics\n'
               '> to use in fan curve definition "' + char_name + '".\n'
               '> You need to have a "datasource" entry that tells \n'
               '> the program the nickname of a source of data.\n'
               '> Alternatively (and this is only recommended if\n'
               '> you are using SES) you can have an "SES_7B" entry\n'
               '> with four pairs of pressures and flows that can\n'
               '> be processed into a characteristic by splines.\n'
               '> Please add one or two "datasource" or "SES_7B"\n'
               '> entries.  The following nickname(s) of data \n'
               '> sources are available:\n'
                 + gen.FormatOnLines(data_names) + '\n'
              )
        gen.WriteError(2461, err, log)
        block_start = ('> The fan curve definition started at the '
                         + gen.Enth(tr_index) + '\n'
                       '> line of the file.')
        gen.WriteOut(block_start, log)
        return(None)

    # Now check whether there are any clashes in the definitions of
    # the curve data.  We want one of the following:
    #  * only one datasource entry
    #  * only one SES_7B entry
    #  * two datasource entries, one with optional argument "direction:=forwards"
    #    and the other with no optional entries or the optional argument
    #    "direction:=reverse".
    #  * two SES_7B entries, one with optional argument "direction:=forwards"
    #    and the other with no optional entries or the optional argument
    #    "direction:=reverse".
    #  * a datasource entry and an SES_7B entry, one with optional argument
    #    and the other with no optional entries or the optional argument
    #    "direction:=reverse".
    # Anything else raises one of a number of errors.
    #
    # First figure out how many of each type (datasource and SES_7B) we
    # have and get their keys.
    block_keys = [key for key in new_char_dict if "datasource#" in key]
    ses_keys = [key for key in new_char_dict if "ses_7b#" in key]
    block_count = len(block_keys)
    ses_count = len(ses_keys)
    tot_count = block_count + ses_count

    # Check for more than two "datasource" entries or more than two "SES_7B"
    # entries.
    if tot_count > 2:
        # Too many characteristics were defined. Find the indices of the
        # lines that defined the entries.

        # Get lists of the line numbers and text of the lines of input,
        # which we will need for error messages.
        line_nos = []
        line_texts = []
        for key in block_keys + ses_keys:
            tr_index = new_char_dict[key][-1]
            line_no, discard, line_text = line_triples[tr_index]
            line_nos.append(line_no)
            line_texts.append(line_text)

        # Build text for the error messages.
        if block_count == 1:
            bl_text = 'one "datasource"\n> entry'
        else:
            bl_text = str(block_count) + ' "datasource"\n> entries'

        if ses_count == 1:
            ses_text = 'one "SES_7B" entry.\n'
        else:
            ses_text = str(ses_count) + ' "SES_7B" entries.\n'

        if ses_count == 0:
            # More than two datasource entries, no SES entries.
            # Complain about the datasources.
            #
            err = ('> The file named "' + file_name + '" has a fan\n'
                   '> curve definition ("' + char_name
                     + '") that has ' + bl_text + '.\n'
                   "> You can't define more than two characteristics\n"
                   '> (one for forwards mode, one for reverse mode).\n'
                   '> Please pick the two definitions you want the fan\n'
                   '> to use and remove the others.\n'
                   '>'
                  )
        elif block_count == 0:
            # More than two SES entries, no datasource entries.
            # Complain about the SES entries.
            #
            err = ('> The file named "' + file_name + '" has a fan\n'
                   '> curve definition ("' + char_name
                     + '") that has ' + ses_text +
                   "> You can't define more than two characteristics\n"
                   '> (one for forwards mode, one for reverse mode).\n'
                   '> Please pick the two definitions you want the fan\n'
                   '> to use and remove the others.\n'
                   '>'
                  )
        else:
            # It's a mixture of the two.  Complain about everything.
            #
            err = ('> The file named "' + file_name + '" has a fan\n'
                   '> curve definition ("' + char_name
                     + '") that has ' + bl_text + ' and ' + ses_text +
                   "> You can't define more than two characteristics\n"
                   '> (one for forwards mode, one for reverse mode).\n'
                   '> Please pick the two definitions you want the fan\n'
                   '> to use and remove the others.\n'
                   '>'
                  )
        gen.WriteError(2462, err, log)
        if tot_count == 3:
            gen.ErrorOnThreeLines(line_nos[0], line_texts[0],
                                  line_nos[1], line_texts[1],
                                  line_nos[2], line_texts[2],
                                  log, False)
        else:
            if tot_count > 4:
                # We don't have a general routine capable of writing
                # more than four lines of error message.
                gen.WriteMessage2("Here are the first four:", log)
            gen.ErrorOnFourLines(line_nos[0], line_texts[0],
                                 line_nos[1], line_texts[1],
                                 line_nos[2], line_texts[2],
                                 line_nos[3], line_texts[3],
                                 log, False)
        return(None)
    # When we get to here we know we have either one or two curve
    # definitions.  Check the optional directions.
    curve_keys = block_keys + ses_keys
    if len(curve_keys) == 1:
        # We only have one fan characteristic, so we don't need to check
        # for the directions.  We use the one characteristic for running
        # in forwards mode and reverse mode.
        fwd_char_key = curve_keys[0]
        rev_char_key = curve_keys[0]
        optionals_fwd = new_char_dict[fwd_char_key][-2]
        optionals_rev = optionals_fwd
    else:
        # We have two characteristics.  Check for conflicting directions.
        curve1 = new_char_dict[curve_keys[0]]
        curve2 = new_char_dict[curve_keys[1]]
        optionals_1 = curve1[-2]
        optionals_2 = curve2[-2]

        # Get the directions (if defined).
        try:
            dir_1st = optionals_1["direction"]
        except KeyError:
            dir_1st = "unstated"
        try:
            dir_2nd = optionals_2["direction"]
        except KeyError:
            dir_2nd = "unstated"
        # First check for no direction specifications at all.
        if dir_1st == "unstated" and dir_2nd == "unstated":
            err = ('> The file named "' + file_name + '" has a fan\n'
                   '> curve definition ("' + char_name
                     + '") that has two\n'
                   '> characteristics.\n'
                   '> When you define two characteristics, you must\n'
                   '> include optional arguments to specify which\n'
                   '> one to use when running in forwards mode and\n'
                   '> which one to use when running in reverse mode.\n'
                   '> Please add the optional argument\n'
                   '>          "direction:=forwards"\n'
                   '> to the one intended for forwards mode or add\n'
                   '>          "direction:=reverse"\n'
                   "> to the other (you can add both if you want)."
                  )
            gen.WriteError(2463, err, log)
            line1_num, discard, line1_text = line_triples[curve1[-1]]
            line2_num, discard, line2_text = line_triples[curve2[-1]]
            gen.ErrorOnTwoLines(line1_num, line1_text,
                                line2_num, line2_text,
                                log, False)
            return(None)
        elif dir_1st == dir_2nd:
            # Both were specified with "direction:=forwards" or
            # "direction:=reverse".
            err = ('> The file named "' + file_name + '" has a fan\n'
                   '> curve definition ("' + char_name
                     + '") that has two\n'
                   '> characteristics.\n'
                   '> When you define two characteristics, you must\n'
                   '> include an optional argument to specify which\n'
                   '> curve to use when running in forwards mode and\n'
                   '> which one to use when running in reverse mode.\n'
                   '> Unfortunately, you specified in both entries\n'
                   '> that they should be running in ' + dir_1st + ' mode.\n'
                   '> Please edit the file to change one so that it\n'
                   '> runs in the opposite direction or remove one\n'
                   '> optional entry (this has the same effect).'
                  )
            gen.WriteError(2464, err, log)
            line1_num, discard, line1_text = line_triples[curve1[-1]]
            line2_num, discard, line2_text = line_triples[curve2[-1]]
            gen.ErrorOnTwoLines(line1_num, line1_text,
                                line2_num, line2_text,
                                log, False)
            return(None)
        elif dir_1st == "forwards" or dir_2nd == "reverse":
            fwd_char_key = curve_keys[0]
            rev_char_key = curve_keys[1]
            optionals_fwd = optionals_1
            optionals_rev = optionals_2
        else:
            fwd_char_key = curve_keys[1]
            rev_char_key = curve_keys[0]
            optionals_fwd = optionals_2
            optionals_rev = optionals_1
    # Once we get to here we have two keys, one for the fan characteristic
    # in forwards mode and one for it in reverse mode.  They may be the
    # same curve or different.
    result = BuildFanChar(char_name, new_char_dict, fwd_char_key, True,
                          settings_dict, line_triples, log)
    if result is None:
        return(None)
    else:
        fwd_char, fwd_press3, fwd_minflow, fwd_maxflow = result

    if fwd_char_key == rev_char_key:
        rev_char = copy.deepcopy(fwd_char)
        rev_press3 = fwd_press3
        rev_minflow = fwd_minflow
        rev_maxflow = fwd_maxflow
        mess = "  Reverse characteristic is the same as the forwards."
        gen.WriteOut(mess, log)
    else:
        result = BuildFanChar(char_name, new_char_dict, rev_char_key, False,
                              settings_dict, line_triples, log)
        if result is None:
            return(None)
        else:
            rev_char, rev_press3, rev_minflow, rev_maxflow = result

    # We now have fan characteristics for forward and reverse mode.
    # The volume flow values are in m^3/s and the pressure values are
    # dimensionless pressure (P/rho, in m^2/s^2).
    new_char_dict.__setitem__("forwards", fwd_char)
    new_char_dict.__setitem__("reverse", rev_char)
    new_char_dict.__setitem__("fwd_for_SES", [fwd_char[0], fwd_press3])
    new_char_dict.__setitem__("rev_for_SES", [rev_char[0], rev_press3])
    new_char_dict.__setitem__("flowlimits", (fwd_minflow, fwd_maxflow,
                                             rev_minflow, rev_maxflow))
    # Add the "begin fanchar" line to the dictionary so we can use
    # it in error 7142 in classHobyah.
    new_char_dict.__setitem__("block_index", line_triples[tr_index])

    # If the diameter was given, add an entry with the fan area so that it
    # can be used to turn fan total pressures into fan static pressures.
    if "diameter" in new_char_dict:
        diameter = new_char_dict["diameter"][0]
        area = math.pi / 4 * diameter**2
        new_char_dict.__setitem__("area", area)
    return(char_name, new_char_dict)


def BuildFanChar(char_name, fanchar_dict, curve_key, forwards,
                 settings_dict, line_triples, log):
    '''Take a dictionary defining a fan characteristic and the key to the
    line that defines one of the characteristics (forwards or reverse).
    Return lists of the appropriate volume flows and total pressure rises
    for the characteristic, after adjusting the characteristic to the
    run's base air density.

    The characteristic could be a set of points in a begin data...end data
    block or it could be eight numbers defining an SES fan characteristic.

        Parameters:
            char_name       str             The name of the fan char.  Used
                                            in error messages.
            fanchar_dict    {}              The dictionary of the fan, which
                                            could contain one or two definitions
                                            of fan characteristics (forwards
                                            and reverse).
            curve_key                       A key that accesses the definition
                                            of the data block (or .csv file)
                                            that has the fan characteristic
                                            we want.
            forwards        bool            If True, this is the curve for
                                            the fan in forwards mode.  If
                                            False, it's for reverse mode.
                                            Used in error messages and entries
                                            in the log file.
            settings_dict   {}              Dictionary of the run settings.
            line_triples [(int, str, str)]  List of lines in the file.  First
                                            entry is the line number in the file
                                            (starting at one, not zero).
                                            Second is the valid data on the line.
                                            Third is the entire line (including
                                            comments) also used in error messages.
            log             handle          The handle of the logfile.

        Returns:
            fan_char    tuple([], [])       A tuple of two lists. The first is
                                            a list of flowrates (m^3/s), the
                                            second is a list of dimensionless
                                            fan total pressure rises (values
                                            of c^2 to use in TwoWayMoC2Fan).
    '''
    psi = settings_dict["psi"]
    gamma = settings_dict["gamma"]
    file_name = settings_dict["file_name"]
    curve_defn = fanchar_dict[curve_key]
    if curve_key[:10] == "datasource":
        # Call a routine that extracts two columns from a datasource.
        tr_index = curve_defn[-1]
        result = GetTabulated(settings_dict, line_triples, tr_index, log,
                              True, curve_defn[0], curve_defn[1],curve_defn[2])
        if result is None:
            return(None)
        else:
            # We get a list of two lists and a list of QA data back.
            ((flow_pts, press_pts), QA_list) = result

    else:
        # It is a form 7B fan characteristic.  We've already checked that
        # it has eight numbers.
        press_pts = (curve_defn[0], curve_defn[2], curve_defn[4], curve_defn[6])
        flow_pts = (curve_defn[1], curve_defn[3], curve_defn[5], curve_defn[7])
        QA_list = ("discard", " a form 7B definition")


        # Check that the pressures all decrease.
        if not (press_pts[0] > press_pts[1] > press_pts[2] > press_pts[3]):
            # Figure out the first instance of an increase in pressure.
            if press_pts[0] <= press_pts[1]:
                which = PairText(1, 2, "pressures", press_pts)
            elif press_pts[1] <= press_pts[2]:
                which = PairText(2, 3, "pressures", press_pts)
            elif press_pts[2] <= press_pts[3]:
                which = PairText(3, 4, "pressures", press_pts)
            else:
                # We ought not to be able to get here, but it never
                # hurts to have an else clause.
                gen.WriteMessage2("Fouled up in a form 7B check (1)", log)
                gen.OopsIDidItAgain(log, file_name)
                return(None)
            err = ('> The file named "' + file_name + '"\n'
                   '> had an SES fan characteristic definition in\n'
                   '> the fan named "' + char_name
                     + '".  The four pressure\n'
                   '> values in such definitions must all decrease\n'
                   '> as you go from left to right in the line that\n'
                   '> defines the characteristic.\n'
                   '> The ' + which + '\n'
                   "> don't follow that rule.  Please edit the file\n"
                   '> to correct this.')
            gen.WriteError(2501, err, log)
            gen.ErrorOnLine2(curve_defn[-1], line_triples, log, False)
            return(None)

        # Check that the volume flows all increase.
        if not (flow_pts[0] < flow_pts[1] < flow_pts[2] < flow_pts[3]):
            # Figure out the first instance of a decrease in flow.
            if flow_pts[0] >= flow_pts[1]:
                which = PairText(1, 2, "flowrates", flow_pts)
            elif flow_pts[1] >= flow_pts[2]:
                which = PairText(2, 3, "flowrates", flow_pts)
            elif flow_pts[2] >= flow_pts[3]:
                which = PairText(3, 4, "flowrates", flow_pts)
            else:
                gen.WriteMessage2("Fouled up in a form 7B check (2)", log)
                gen.OopsIDidItAgain(log, file_name)
                return(None)
            err = ('> The file named "' + file_name + '"\n'
                   '> had an SES fan characteristic definition in\n'
                   '> the fan named "' + char_name
                     + '".  The four flow values\n'
                   '> in such definitions must all increase as you\n'
                   '> go from left to right in the line defining\n'
                   '> the characteristic.\n'
                   '> The ' + which + '\n'
                   "> don't follow that rule.  Please edit the file\n"
                   '> to correct this.')
            gen.WriteError(2502, err, log)
            gen.ErrorOnLine2(curve_defn[-1], line_triples, log, False)
            return(None)

    # Get the optional argument for units on this line if there is one.
    # Get the units for the file too.
    units = settings_dict["units"]
    try:
        line_units = curve_defn[-2]["units"]
    except:
        # There was no optional argument on this line, use the default.
        line_units = units

    # Once we get to here we have a list of flows and a matching list of
    # pressures.  Write them to the log file along with the density.
    try:
        # Note that this density is always in the file's main input
        # units, not the optional units on the line.  This may be
        # confusing, but that's too bad.  Far better not to set a
        # base density and use the default.
        base_dens_SI = fanchar_dict["density"][0]
    except:
        # There was no optional argument setting a fan base density,
        # use 1.2 kg/m^3.
        base_dens_SI = 1.2
        dens_fac = 1.0
    else:
        # Calculate the density correction factor we need to apply to
        # the fan total pressures.
        dens_fac = 1.2 / base_dens_SI
    base_dens_US = USc.ConvertToUS("dens1", base_dens_SI, False, log)[0]

    if units == "us":
        dens_text1 = str(base_dens_US) + ' lb/ft^3'
        dens_text2 = '(at ' + '{:7.5F}'.format(base_dens_US) + ' lb/ft^3)'
    else:
        dens_text1 = str(base_dens_SI) + ' kg/m^3'
        dens_text2 = ' (at ' + '{:5.3F}'.format(base_dens_SI) + ' kg/m^3) '
    if forwards:
         mess = ('\nProcessing fan characteristic "' + char_name + '".\n'
                '  Fan base density = ' + dens_text1 + '\n'
                '  Density correction = ' + "{:5.5F}".format(dens_fac) + '\n'
                '  Forwards characteristic from' + QA_list[1] )
    else:
        mess = "  Reverse characteristic from" + QA_list[1]
    gen.WriteOut(mess, log)
    if line_units == "us":
        # We need to print the original curve to the log file in US units
        # so that we can check it.  We also print the conversion to SI
        # units and to 1.2 kg/m^3
        mess = "      Source values (US)          Adjusted to std. density\n"  \
               "     Flow         Pressure           Flow       Pressure\n"     \
               "      cfm          i.w.g            m^3/s          Pa\n"        \
               "            "  + dens_text2 +  "             (at 1.2 kg/m^3)"
               #   420,000         3.100            198.2         805.7

    elif not math.isclose(dens_fac, 1.0):
        # We need to print the original curve to the log file and the
        # conversion to air density 1.2 kg/m^3.
        mess = "   Nonstandard density values     Adjusted to std. density\n"  \
               "      Flow        Pressure            Flow       Pressure\n"      \
               "     m^3/s           Pa              m^3/s          Pa\n"         \
               "            "  + dens_text2 +  "              (at 1.2 kg/m^3)"
               #      50.0         2200.0             50.0        2336.3
    else:
        # The density is so close to 1.2 kg/m^3 that we might as well
        # treat it as exactly 1.2.
        mess = "      Flow        Pressure\n"     \
               "     m^3/s           Pa\n"        \
               "               (at 1.2 kg/m^3)"
               #      60.0         2100.0
    gen.WriteOut(mess, log)


    # Store the volume flows (in m^3/s), the pressures as both m^2/s^2 (for
    # the calculation) and as Pa (in case we need to write them in an SES
    # input file).
    flow2_pts = []
    press2_pts = []  # m^2/s^2
    press3_pts = []  # Pa
    for flow, press in zip(flow_pts, press_pts):
        if line_units == "us" and units == "si":
            # We read a bunch of numbers in US units and treated them as
            # if they were SI (no conversion was made).
            SI_flow = USc.ConvertToSI("volflow", flow, False, log)[0]
            SI_press = USc.ConvertToSI("press1", press, False, log)[0]
            US_flow = flow
            US_press = press
        else:
            US_flow = USc.ConvertToUS("volflow", flow, False, log)[0]
            US_press = USc.ConvertToUS("press1", press, False, log)[0]
            SI_flow = flow
            SI_press = press
        press_adj = SI_press * dens_fac
        if line_units == "us":
            # Get the volume flow in cfm with commas every three digits.
            cfm_text = "{:,.0F}".format(US_flow)
            mess = " " * (10 - len(cfm_text)) + cfm_text + " " \
                 + "{:>13.3F}".format(US_press) + " "   \
                 + "{:>16.1F}".format(SI_flow) + " " \
                 + "{:>13.1F}".format(press_adj)
        elif not math.isclose(dens_fac, 1.0):
            mess = "{:>10.1F}".format(SI_flow) + " "    \
                 + "{:>14.1F}".format(SI_press) + " "   \
                 + "{:>16.1F}".format(SI_flow) + " "    \
                 + "{:>13.1F}".format(press_adj)
        else:
            mess = ("{:>10.1F}".format(SI_flow) + " "   \
                    + "{:14.1F}".format(SI_press))
        gen.WriteOut(mess, log)
        flow2_pts.append(SI_flow)
        press3_pts.append(SI_press)
        # Take the modified pressure rise and turn it into m^2/s^2 for
        # use in the calculation.  I believe this calculation is not
        # correct (I think it should be P / (rho * psi) ) but putting
        # gamma^2 in the numerator seems to gives the correct answer.
        # No doubt I'll figure it out one of these days.
        press2_pts.append(gamma**2 * press_adj / ( base_dens_SI * psi))
        # We now have a list of volume flows of m^3/s and pressure rises
        # in m^2/s^2.  If we put this fan in a tunnel during a calculation,
        # we convert the volume flows to velocities in the tunnel where the
        # fan is.  This happens in PROC GetCharData.

    # Check that the flow values increase.  We already did this for the
    # SES volume flows with a more detailed message but we still need to
    # do it for other data.
    for index, value in enumerate(flow_pts[:-1]):
        next_value = flow_pts[index + 1]
        if next_value <= value:
            # Figure out the first instance of a decrease in flow.
            which = PairText(index + 1, index + 2, "flowrates", flow_pts)
            (line_number, discard, line_text) = line_triples[tr_index]
            (source_name, descrip, source_index) = QA_list[:3]
            (line_num2, discard, line2_text) = line_triples[source_index]
            err = ('> The file named "' + file_name + '"\n'
                   '> had a fan characteristic definition in the\n'
                   '> fan named "' + char_name
                     + '".  The flow values must\n'
                   '> increase as you go down the list of entries.\n'
                   '> The ' + which + '\n'
                   "> don't follow that rule.  Please edit the file\n"
                   '> to correct this.\n'
                   '> The line calling up the block of data (the '
                     + gen.Enth(line_number) + '\n'
                   '> line) is:\n'
                   '>     ' + line_text)
            if descrip[:10] == ' datasource':
                err = (err + '\n> The data block began on the '
                       + gen.Enth(line_num2) + ' line of the\n'
                       '> input file.')
            else:
                # Get the column number as an ordinal.
                col_text = gen.Enth(QA_list[3][1])
                err = (err + '\n> The data came from the ' + col_text
                       + ' column of the\n'
                       '>' + descrip + '.')
            gen.WriteError(2503, err, log)
            return(None)
    # Put the fan characteristic into a list.  The flow values are m^3/s and
    # the pressure values are m^2/s^2 (gamma **2 * P_tot / (rho * psi) ).
    fan_char = (flow2_pts, press2_pts)
    # Return a pair of lists in the original values, the pressures
    # in Pa and the flow limits

    # Now decide on the extents of the interpolated curve in case we
    # need to feed them to SES.
    if "limits" in curve_defn:
        values = curve_defn["limits"]
        minflow = values[0]
        maxflow = values[1]
        try:
            limit_units =  values[-2]["units"]
        except:
            pass
        else:
            if limit_units == "us" and units == "si":
                # We need to convert the limits from US units to SI.
                minflow = USc.ConvertToSI("volflow", minflow, False, log)[0]
                maxflow = USc.ConvertToSI("volflow", maxflow, False, log)[0]
    else:
        # Put in reasonable guesses for the extents of the fan curve.
        # These numbers may change once we start modelling with it.
        minflow = -0.2 * (flow2_pts[-1] - flow2_pts[0])
        maxflow = 1.25 * flow2_pts[-1]

    return(fan_char, press3_pts, minflow, maxflow)


def PairText(first, second, descrip, array):
    '''Take two numbers and an array and turn them into a phrase
    describing their ordinal numbers with a description and their
    values, like "3rd and 4th times (1452 and 1200)".  This happens
    often enough in error messages that it is worth having it as a
    separate function.

        Parameters:
            first           int             The first ordinal number.
            second          int             The first ordinal number.
            descrip         str             A description.
            array           []              A list of numbers.  Two are
                                            taken from it for the text.

        Returns:
            text            str             A useful phrase in some contexts.
    '''
    text = gen.Enth(first) + ' and ' + gen.Enth(second) + ' ' + descrip + ' (' \
         + str(array[first - 1]) + ' and ' + str(array[second - 1]) + ')'
    return(text)


def GetTabulated(settings_dict, line_triples, tr_index, log, unbroken,
                 block_name, col1 = "#none", col2 = "#none",
                 col3 = "#none", col4 = "#none"):
    '''Take the name of a source of data (this could be a .csv file or a
    begin data...end data block) and the names or numbers of up to four
    columns in it.  Return a list of lists with the data in them.

    If any of the blocks of data have empty values in them and the
    Boolean 'unbroken' is true, we raise an error.  Data blocks with
    empty values are useful for plotting (gnuplot puts a break in the
    curve at them) but causes problems if we tried to use the same data
    as a fan characteristic or as variable pressure loss factors for
    platform screens when the doors open and close.

        Parameters:
            settings_dict   {}              Dictionary of the run settings.
            line_triples [(int, str, str)]  List of lines in the file.
            tr_index        int             Index in line_triples of the
                                            line that generated this.  Used
                                            in error messages.
            log             handle          The handle of the logfile.
            unbroken        Bool            If True the series must not
                                            have blank entries in it in
                                            any column being returned.
            block name      str             Name of the type of block
                                            being processed.  Used in
                                            error messages.
            col1 to col4    int or str      If an integer, the index of
                                            a column in the data, starting
                                            at 1.
                                            If a string, the name at the
                                            top of a column in the data.

        Returns:
            values_list     []              A list of one to four lists
                                            of data from a datasource.
            QA_data         [str]           A list of descriptive strings.

        Errors:
            Aborts with 2481 if the named data source did not exist.
            Aborts with 2482 if a column number below 1 was given.
            Aborts with 2483 if a column number above the count of columns
            in the data source was given.
            Aborts with 2484 if the name of a column was given and there
            is no column with that name in the datasource.
            Aborts with 2485 if we wanted the data to be all unbroken
            sequences and there was a break.
    '''
    user_data_dict = settings_dict["user_data"]
    data_names = user_data_dict.keys()

    file_name = settings_dict["file_name"]
    if ("csv_" + block_name not in user_data_dict and
                 block_name not in user_data_dict):
        err = ('> The file named "' + file_name + '"\n'
               '> tried to read data from a data source (a .csv\n'
               '> file or "begin data...end data" block in the\n'
               '> file) that does not exist.  The nickname of\n'
               '> the desired data source was "' + block_name + '".\n')
        if len(data_names) == 0:
            err = (err +
                    '> There were no data sources in the file (no\n'
                    '> "begin data...end data" blocks and no data\n'
                    '> taken from .csv files).\n'
                    '> Please edit the file to add a suitable data\n'
                    '> source or remove the line of entry.')
        else:
            err = (err +
                    '> Please edit the file to add a suitable data\n'
                    '> source or remove the line of entry.  For what\n'
                    "> it's worth the following nickname(s) of data\n"
                    '> sources are available:\n'
                     + gen.FormatOnLines(data_names) + '\n')
        gen.WriteError(2481, err, log)
        gen.ErrorOnLine2(tr_index, line_triples, log, False)
        return(None)
    else:
        # Get the block of data using two possible keys (one is for a
        # data block inside the Hobyah input file, one for data in a
        # separate .csv file).  We keep a note of which name was used.
        try:
            data = user_data_dict[block_name]
            source = ' datasource "' + block_name + '".'
        except KeyError:
            used_name = "csv_" + block_name
            data = user_data_dict[used_name]
            csv_name = data["#name"]
            source = ' .csv file "' + csv_name + '"'

        defn_tr_index = data["#tr_index"]
        # Get the names of the keys, but take off the key to tr_index entry
        # so it doesn't appear in lists of valid keys.
        titles = list(data.keys())[:-1]

    # Now get the data out, checking for invalid names/numbers of the columns.
    # We also make a list of traceability data so that later routines that
    # check for things like ever-increasing values can point to column names,
    # column ordinal numbers and the lines defining the datalist or which
    # .csv file the data came from.
    values_list = []
    # Create a list of QA data, with the nickname (block name), a description
    # of the source and the number of the line in the input file.  As we
    # process the columns we add to it.
    QA_data = [block_name, source, data["#tr_index"]]
    for col_name in (col1, col2, col3, col4):
        if col_name == "#none":
            # We don't want this column of data.
            pass
        else:
            try:
                col_num = int(col_name)
                # If we get to here we have an integer column number.
                if col_num < 1:
                    maxcols = str(len(titles))
                    if block_name[:4] == "csv_":
                        # Take off the prefix for the error message.
                        block_name = block_name[4:]
                    err = ('> The file named "' + file_name + '"\n'
                           '> tried to read data from a data source called\n'
                           '> "' + block_name + '".  The block exists, but\n'
                           '> you want to read the column number '
                             + str(col_num) + ', which\n'
                           '> is impossible - the lowest column number is 1.\n'
                           '> The block of data has ' + maxcols
                             + ' columns of data, so a\n'
                           '> number between 1 and '
                             + maxcols + ' is needed.\n'
                           '> Please edit the file to correct it.\n'
                          )
                    gen.WriteError(2482, err, log)
                    gen.ErrorOnLine2(tr_index, line_triples, log, False)
                    return(None)
                elif col_num > len(titles):
                    maxcols = str(len(titles))
                    if block_name[:4] == "csv_":
                        # Take off the prefix.
                        block_name = block_name[4:]
                    err = ('> The file named "' + file_name + '"\n'
                           '> tried to read data from a data source called\n'
                           '> "' + block_name + '".  The block exists, but\n'
                           '> you want to read the column number '
                             + str(col_num) + ', which\n'
                           '> is higher than the count of columns available.\n'
                           '> The block of data has ' + maxcols
                             + ' columns of data, so a\n'
                           '> number between 1 and '
                             + maxcols + ' is needed.\n'
                           '> Please edit the file to correct it.\n'
                          )
                    gen.WriteError(2483, err, log)
                    gen.ErrorOnLine2(tr_index, line_triples, log, False)
                    return(None)
                else:
                    col_name = titles[col_num - 1]
                    values = list(data[col_name])
            except ValueError:
                # The column ID is not an integer, it's a word.
                if col_name.lower() not in titles:
                    # The word is not valid (it is not on the first line
                    # of the block of data).
                    if block_name[:4] == "csv_":
                        # Take off the prefix.
                        block_name = block_name[4:]
                    err = ('> The file named "' + file_name + '"\n'
                           '> tried to read data from a data source called\n'
                           '> "' + block_name + '".  The block exists, but\n'
                           '> you want to read the column named '
                             + col_name + ',\n'
                           '> which does not exist.  For what it is worth\n'
                           '> the data block contained the following\n'
                           '> column names:\n'
                            + gen.FormatOnLines(titles)
                          )
                    gen.WriteError(2484, err, log)
                    gen.ErrorOnLine2(tr_index, line_triples, log, False)
                    return(None)
                else:
                    values = list(data[col_name])
                    col_num = titles.index(col_name) + 1
            values_list.append(values)
            QA_data.append((col_name, col_num))

    if unbroken:
        # We want the lists of data not to have breaks in them.  Check if
        # they have breaks and complain if they do.
        for index, column in enumerate(values_list):
            if '     ' in column:
                # We have a break in the data.
                # print(QA_data)
                defn_tr_index = QA_data[2]
                col_name, col_num = QA_data[index + 3]
                line_number, discard, line_text = line_triples[tr_index]
                line_num2, discard, line2_text = line_triples[defn_tr_index]
                err = ('> The file named "' + file_name + '"\n'
                       '> successfully read data from "' + block_name + '",\n'
                       '> but there was a problem with the data.  The\n'
                       '> data is intended for a purpose that requires\n'
                       '> no missing entries (such as a fan characteristic\n'
                       '> or the behaviour of a portal door).  One of\n'
                       '> the columns of data has at least one missing\n'
                       "> entry, so the data can't be used.  Here is some\n"
                       '> information to help you trace the problem:\n'
                       '>\n'
                       '>   The line calling up the block of data (the '
                         + gen.Enth(line_number) + '\n'
                       '>   line) is:\n'
                       '>     ' + line_text + '\n>')



                if "csv_" + block_name in user_data_dict:
                    # This was from a .csv file, not a block of data in
                    # the Hobyah file.  We give the name of the .csv file
                    # and the ordinal number of the column being used
                    # (1st, 5th etc.).
                    err = (err + '\n'
                       '>   The data came from a .csv file named "'
                         + csv_name + '".\n'
                       '>\n'
                       '>   The column with a missing entry was the '
                         + gen.Enth(int(col_num)) + ' column\n'
                       '>   in the .csv file:')
                else:
                    err = (err + '\n'
                       '>   The data block was named "' + block_name + '" and\n'
                       '>   it began on the '
                         + gen.Enth(line_num2) + ' line of the file:\n'
                       '>     ' + line2_text + '\n'
                       '>\n'
                       '>   The column with a missing entry was the '
                         + gen.Enth(int(col_num)) + ' column')
                    # Add a full stop or the name of the column.
                    if str(col_num) != col_name:
                        err = err + '\n>   (named "' + col_name + '").'
                    else:
                        err = err + '.'
                gen.WriteError(2485, err, log)
                # Add an extract giving the numbers surrounding the
                # missing entry.
                place = column.index('     ')
                low_index = max(place - 2, 0)
                high_index = min(place + 3, len(column))
                sample = list(zip(*values_list))[low_index:high_index]
                for entry in sample:
                    print('>      ' + str(entry)[1:-1])
                return(None)

    return(values_list, QA_data)


def ProcessCalc(line_triples, begin_lines, settings_dict, log):
    '''Read all the blocks that define a Hobyah calculation and run
    either run the calculation or build the skeleton of an SES input
    file from the Hobyah geometry and routes.

        Parameters:
            line_triples [(int, str, str)]  List of lines in the file.  First
                                            entry is the line number in the file
                                            (starting at one, not zero).
                                            Second is the valid data on the line.
                                            Third is the entire line (including
                                            comments) also used in error messages.
            begin_lines     [int]           List of which entries in line_triples
                                            are top-level "begin" blocks.
            settings_dict   {}              The entries in the settings block.
            log             handle          The handle of the logfile.

        Returns:
            success_str     str             A string stating what happened in
                                            the run.

        Errors:
            Aborts with 2081 if the binary file could not be written to.
            Aborts with 2082 if there were no sectypes.
            Aborts with 2083 if the binary file was somehow changed from
            writeable to non-writeable while the calculation was running
            (e.g. we had a race condition in the file system).
            Aborts with 2084 if someone added a new friction factor
            approximation without handling it in this procedure.
            Aborts with 2085 if an optional entry in a line of input
            changed the sectype of the tunnel to a sectype whose name
            does not exist.
            Aborts with 2086 if a tunnel is so short that trying to
            calculate characteristics in it could violate the CFL condition.
            Aborts with 2087 if two features in a tunnel (e.g. area change,
            adit) that trying to calculate characteristics in the stretch
            of tunnel between them could violate the CFL condition.
            Aborts with 2088 if there was one node with only one tunnel
            attached to it.
            Aborts with 2089 if there was more than one node with only
            one tunnel attached to it.
            Aborts with 2090 if the scipy Python module was not available.
    '''
    dir_name = settings_dict["dir_name"]
    file_name = settings_dict["file_name"]
    file_stem = settings_dict["file_stem"]
    dt = settings_dict["aero_step"]
    nofortran = settings_dict["nofortran"]
    time_accuracy = settings_dict["time_accuracy"]
    show_errors = settings_dict["show_errors"]
    min_area = settings_dict["min_area"]

    # Check if we can open the binary file before we spend any time on the
    # calculation.  We open it for appending so that its contents are not
    # destroyed (if we decide to break out of the calculation loop the old
    # binary file remains in the folder, unchanged).
    # First though, we open it for reading so that we can check if it
    # exists.
    bin_name = file_stem + ".hbn"
    try:
        bdat = open(dir_name + bin_name, "r")
    except FileNotFoundError:
        hbn_nonexistent = True
    else:
        hbn_nonexistent = False
    try:
        bdat = open(dir_name + bin_name, "ab")
    except PermissionError:
        err = ('> Skipping "' + file_name + '", because you\n'
               "> do not have permission to write to its binary file.")
        gen.WriteError(2081, err, log)
        return(None)
    else:
        bdat.close()
    # The act of opening a file for appending creates the file if it
    # does not exist.  If the file did not exist before we opened it
    # for appending, we delete the empty file we just created.  This
    # means that we don't have empty .hbn files cluttering up the
    # folders.
    if hbn_nonexistent:
        os.remove(dir_name + bin_name)

    if nofortran is False:
        # Check to see if the Fortran calculation files have been compiled
        # and the routines are available for the calculations. These are
        # worth using, as they speed up calculations significantly.
        try:
            import compressible as ftn # Fortran routines inside f2py
        except:
            # The Fortran routines are not available.  We set a flag that is
            # passed to some routines to tell them to use Python versions of
            # the calculation routines.
            have_ftn = False
            # State that we looked for the Fortran modules but failed to
            # find them.
            gen.WriteMessage2("Couldn't find the Fortran compressible "
                              "flow routines, using Python ones instead",
                              log)
        else:
            # The Fortran calculation routines are available, use them.
            have_ftn = True
    else:
        have_ftn = False


    # Now figure out some fundamental values.  Speed of sound, minimum
    # length of a cell (so that a pressure wave can't cross it in less
    # than one timestep).  All the values we use are in settings_dict
    # so we just feed it that.
    (c_atm, dx, psi, gammapsi) = BaseConstants(settings_dict)
    # We have some new entries for settings_dict.  These are:
    #   "c_atm"  speed of sound in air outside the tunnel (m/s).
    #   "dx"     the minimum distance between grid points (m).
    #   "psi"    2/(gamma - 1).  In most cases this is 2/(1.4 - 1) = 5.
    # "gammapsi" (gamma - 1)/(2 * gamma).

    settings_dict.__setitem__("c_atm", c_atm)
    settings_dict.__setitem__("dx", dx)
    settings_dict.__setitem__("psi", psi)
    settings_dict.__setitem__("gammapsi", gammapsi)


    debug1 = settings_dict["debug1"]
    # First find where all the sectypes blocks begin (there can be more than one).
    result = GetBegins(line_triples, begin_lines, "sectypes",
                       1, math.inf, file_name, debug1, log)
    if result is None:
        return(None)
    # 'result' is a list that indexes where all the "begin sectypes"
    # blocks are.  Iterate over them and add each sectype to a
    # dictionary of sectypes.
    sectypes_dict = {}
    if debug1:
        print("Processing sectypes")
    for tr_index in result:
        result = ProcessSectypes(line_triples, tr_index, settings_dict,
                                 sectypes_dict, log)
        if result is None:
            return(None)
        else:
            sectypes_dict = result
    if len(sectypes_dict.keys()) <= 1:
        # We are running a calculation but we have not defined
        # any sectypes.  If there are no "sectypes" blocks the
        # length will be zero.  If some idiot defines an empty
        # sectypes block it will have one entry (the key is
        # "block_index" and points to the line the block starts
        # at, for error messages).  If a user defines one
        # sectype, the length of the block will be two.
        err = ('> The file named "' + file_name + '" is\n'
               '> running a calculation but has no sectypes.\n'
               '> Please either add one or more sectypes\n'
               '> or edit the settings block to change\n'
               '> "runtype calc" to "runtype plot".'
              )
        gen.WriteError(2082, err, log)
        return(None)

    # Now look for a block defining types of jet fan: thrust, installation
    # efficiency, jet speed.  Unidirectional jet fans need three numbers,
    # reversible jet fans need six numbers.  We do this before we read
    # the tunnels so that we can check the instructions for running
    # jetfans in each tunnel block's "jetfans1" keyword.
    if debug1:
        print("Processing jet fan types")
    result = GetBegins(line_triples, begin_lines, "jetfantypes",
                       0, 1, file_name, debug1, log)
    if result is None:
        return(None)
    if len(result) == 1:
        # There was a "jetfantypes" block.
        tr_index = result[0]
        result = ProcessJFTypes(line_triples, tr_index, settings_dict, log)
        if result is None:
            return(None)
        else:
            # JFblock_dict has all the input in the "jetfantypes" block.
            # JFcalc_dict has all the values needed in the calculation.
            (JFblock_dict, JFcalc_dict) = result
    else:
        # Spoof the jet fan dictionaries, there are no jet fans in the file.
        JFblock_dict = {}
        JFcalc_dict = {}
    # Store the two dictionaries where we can pull them out later,
    # either in the calculation process or in a classHobyah
    # instance.
    settings_dict.__setitem__("JFblock_dict", JFblock_dict)
    settings_dict.__setitem__("JFcalc_dict", JFcalc_dict)


    # Now seek out all the tunnels blocks.
    if debug1:
            print("Processing tunnels")
    result = GetBegins(line_triples, begin_lines, "tunnel",
                       1, math.inf, file_name, debug1, log)
    if result is None:
        return(None)
    tunnels_dict = {}
    for tr_index in result:
        result = ProcessTunnel(line_triples, tr_index, settings_dict,
                               sectypes_dict, log)
        if result is None:
            return(None)
        else:
            (tunnel_name, new_tun_dict) = result
            tunnels_dict.__setitem__(tunnel_name, new_tun_dict)
    # We don't need to check for there being no tunnels, if there
    # were none then error 2063 would already have been raised.

    # Now seek out all the blocks that create multiple tunnels (cloned
    # tunnels).
    if debug1:
            print("Processing clones")
    result = GetBegins(line_triples, begin_lines, "tunnelclones",
                       0, math.inf, file_name, debug1, log)
    if result is None:
        return(None)
    else:
        clonecount = len(result)
        # Make a list to hold lines of tunne definition.
        tunnel_def_lines = []
    for tr_index in result:
        result = ProcessClones(line_triples, tr_index, settings_dict,
                               tunnels_dict, sectypes_dict, log)
        if result is None:
            return(None)
        else:
            (clonedtunnels_dict, tunnel_lines) = result
            # Add the new set of cloned tunnels to the tunnels
            # dictionary.  We have already checked for name clashes.
            tunnels_dict.update(clonedtunnels_dict)
            tunnel_def_lines.extend(tunnel_lines)
    if clonecount != 0:
        # Write the tunnel definitions for all the clones to the log file.
        gen.WriteOut("===Start of commands to generate cloned tunnels===\n"
                     "===Copy these to the input file if you want to ===\n"
                     "===customise their properties.                 ===",
                     log)
        for line in tunnel_def_lines:
            gen.WriteOut(line, log)
        gen.WriteOut("===End of commands to generate cloned tunnels.===\n\n",
                     log)

    # Now seek out all the traintypes blocks.
    if debug1:
        print("Processing traintypes")
    result = GetBegins(line_triples, begin_lines, "traintype",
                       0, math.inf, file_name, debug1, log)
    if result is None:
        return(None)
    trtypes_dict = {}
    for tr_index in result:
        result = ProcessTrType(line_triples, tr_index, settings_dict,
                                  trtypes_dict, log)
        if result is None:
            return(None)
        else:
            (trtype_name, new_trtype_dict) = result
            trtypes_dict.__setitem__(trtype_name, new_trtype_dict)
    # We don't need to check for there being no train types.
    # Figure out what the longest train type is and store it in
    # settings_dict.  If there are no train types we set the maximum
    # train length to 1000 metres, so that if the user is using this
    # to build SES input files that have trains going up the route,
    # they have 1 km to get their trains up to speed.  If there are
    # train types longer than 1 km, we use their length plus 1 m
    # instead.
    max_trlen = 1000.0
    for trtype_name in trtypes_dict:
        length = trtypes_dict[trtype_name]["geometry"][0]
        if length > max_trlen:
            max_trlen = length + 1.0
    settings_dict.__setitem__("max_trlen", max_trlen)

    # Now seek out all the routes blocks.
    if debug1:
            print("Processing routes")
    result = GetBegins(line_triples, begin_lines, "route",
                       0, math.inf, file_name, debug1, log)
    if result is None:
        return(None)
    routes_dict = {}
    for tr_index in result:
        result = ProcessRoute(line_triples, tr_index, settings_dict,
                               tunnels_dict, trtypes_dict, log)
        if result is None:
            return(None)
        else:
            (route_name, new_route_dict) = result
            routes_dict.__setitem__(route_name, new_route_dict)
    # We don't need to check for there being no routes, as simple
    # files may not have them.


    # Now look for road vehicle type definitions.
    if debug1:
        print("Processing vehicle types")
    result = GetBegins(line_triples, begin_lines, "traffictypes",
                       0, 1, file_name, debug1, log)
    if result is None:
        return(None)
    if len(result) == 1:
        # There was a "traffictypes" block.
        tr_index = result[0]
        result = ProcessVehTypes(line_triples, tr_index, settings_dict, log)
        if result is None:
            return(None)
        else:
            # vehicles_dict has all the input in the "traffictypes" block.
            # vehcalc_dict has all the values needed to be used in the
            # calculation.
            (vehicles_dict, vehcalc_dict) = result
    else:
        # Spoof the traffic dictionary, there is no traffic in the file.
        vehicles_dict = {}
        vehcalc_dict = {}
    # Store the two dictionaries where we can pull them out later,
    # either in the calculation process or in a classHobyah
    # instance.
    settings_dict.__setitem__("vehicles_dict", vehicles_dict)
    settings_dict.__setitem__("vehcalc_dict", vehcalc_dict)

    # Now look for blocks that put steady-state traffic into routes.
    # These types of traffic are present in the route from the first
    # timestep to the last.  Transient traffic is handled by
    if debug1:
        print("Processing steady-state traffic in routes")
    result = GetBegins(line_triples, begin_lines, "trafficsteady",
                       0, math.inf, file_name, debug1, log)
    if result is None:
        return(None)
    # Make a dictionary to hold the traffic blocks.  The keys are the
    # line number it started at, the results are all the lines in the
    # block and the traffic data (flow, density, speed) derived from
    # the the lines.
    # We prepend 'r_' to the name because this is a route-centric way
    # of looking at traffic.  The 'r_' distinguishes it from traffic
    # data that is expressed in a tunnel-centric way.
    r_traffic_dict = {}
    # Make dictionaries to hold tunnel-centric traffic data and
    # simplified traffic data.  These are replaced if there are
    # blocks of traffic.
    t_traffic_dict = {}
    calc_traffic = {}
    # Make a record of which routes were defined in which blocks,
    # so we can use the lines in later error messages.
    route_indices = {}
    for tr_index in result:
        # We call a routine that puts traffic into routes in the same
        # way as trains follow routes.  More than one route may run
        # through a tunnel, so it is possible to put more traffic in
        # several routes than the lanes in a tunnel can handle.  Later
        # we may set other ways of putting traffic into tunnels, such
        # as the IDA method (setting inflows at portals and fractional
        # splits at each junction).
        result = ProcessTraffic1(line_triples, tr_index, settings_dict,
                                 routes_dict, route_indices, log)
        if result is None:
            return(None)
        else:
            routes_used, traffic1_dict = result

        r_traffic_dict.__setitem__(tr_index, traffic1_dict)
        for name in routes_used:
            # Store the tr_index of the block where this route was
            # defined in case we need it for an error message inside
            # ProcessTraffic1.
            route_indices.__setitem__(name.lower(), tr_index)

        # Now turn the data for traffic in routes into data for traffic
        # in tunnels.  We couldn't do this while reading the routes because
        # more than one route can go through each tunnel.
        # This routine catches overlapping stationary traffic in routes
        # in the same "traffictype" block, which we don't allow because
        # it's too difficult to resolve conflicts like different densities
        # of stationary vehicles being in shared lanes.
        # It also catches mismatches between the counts of lanes in the
        # same traffictype block and mismatches in traffic speed and speed
        # limits in the same traffictype block.
        result = TrafficInTunnels(line_triples, settings_dict, routes_dict,
                                  route_indices, r_traffic_dict, log)
        if result is None:
            return(None)
        else:
            t_traffic_dict = result
            if debug1:
                print("t_traffic_dict")
                for name in t_traffic_dict:
                    print("  " + name)
                    for entry in t_traffic_dict[name]:
                        print("   ", [round(num, 6) for num in entry])
        # Each entry in t_traffic_dict consists of the following:
        #     [start dist, end dist, traffic speed (km/h),
        #      vehicle density(veh/km), gradient (fraction -1 to +1),
        #      vehicle type index (integer)]
        #
        # These are all needed for pollution calculations but can be
        # simplified for putting traffic drag into the tunnels, where
        # gradient isn't needed.  We call a routine that gives us a
        # simpler list (no gradient) and if the distances and
        # vehicle speeds match, gives a list of vehicle
        # densities for each vehicle type.
        calc_traffic = SimplifyTraffic(t_traffic_dict, vehcalc_dict,
                                       debug1, log)
        # Each entry in the calc_traffic dictionary has a tunnel name
        # as the key and returns a list of lists.  Each sub-list has
        # the following entries:
        #   0  start distance
        #   1  stop distance
        #   2  traffic speed (km/h)
        #   3  density of first vehicle type
        #   4  density of second vehicle type
        #  ...
        #   n  density of last vehicle type
        # If a vehicle type is not present, it has a zero value in
        # the list.

    # Create a dictionary to hold details of named entities like fans,
    # dampers, PSDs, tunnel doors etc. in the input file so that if two
    # entities have the same name we can issue an error message with
    # details of both lines of input.
    entities_dict = {}

    # Check the routes and tunnels for jet fans.  Get a tunnel-centric
    # dictionary of active jet fans.  Get a dictionary for plotting that
    # has the names assigned to all banks of jet fans.  We also populate
    # the dictionary of entities with the names of the banks of jet
    # fans.
    result = GetJFBlocks(routes_dict, tunnels_dict, settings_dict,
                         entities_dict, line_triples, log)
    if result is None:
        return(None)
    else:
        t_JF_dict, JF_plot_dict, entities_dict = result

    if debug1:
            print("Processing plotcontrol")
    # Now add a custom entry for a time that is just after the last timestep.
    # Users can use use the word "duration" as if it was a constant,
    # and in a range, startstopcount or startstepcount entry used to
    # define the plot times.
    runtime = settings_dict["aero_time"]
    duration = runtime + 0.1 * dt
    settings_dict.__setitem__("#duration", [duration, str(duration), -1])
    result = GetBegins(line_triples, begin_lines, "plotcontrol",
                        0, 1, file_name, debug1, log)
    if result is None:
        return(None)
    if len(result) == 0:
        # There was no plotcontrol block in the input file.  Make an empty
        # dictionary so that we can account for that when we recreate input
        # files.
        plotcontrol_dict = {}

        # By default, we want to plot every 1 second.  Users may not
        # choose a timestep that fits exactly, and floating point
        # inaccuracies may also hold here.  Calculate a step that is
        # just below the runtime and is an integer multiple of dt.  Say
        # that the runtime is 1 second the user has set time step 'dt'
        # to 0.075 seconds: 'step' is 0.975 seconds, which is the
        # closest integer multiple of 0.075 that is below 1 second.
        # We also include the last and second last timesteps of the
        # calculation, because we want people to be able to plot at
        # the last timestep even if they've cut out lots of plot
        # timesteps.
    # runtime = settings_dict["aero_time"]
    # duration = runtime + 0.1 * dt
        step = dt * round(1/dt, 0)
        aero_times = list(np.arange(0.0, duration, step))

        # Figure out whichever calculation time is closest to and
        # just below the runtime.
        aero_times.insert(-1, dt * round(duration / dt, 0))

    else:
        tr_index = result[0]
        result = ProcessPlotControl(line_triples, tr_index, settings_dict, log)
        if result is None:
            return(None)
        else:
            (plotcontrol_dict, aero_times) = result


    # The aero times may be generated in a number of ways (directly
    # setting values, range(), startstopcount(), startstepcount() or
    # a combination of the above.  The list of calculation times is
    # generated by one np.arange().  The slight differences caused
    # floating-point calculations mean that two times that ought to
    # be identical may not be considered equal.
    # The 2-equation method of characteristics used in this software
    # has a minimum grid size of 1 m, so a suitable smallest timestep
    # is around 1/(343 m/s + 20 m/s) = 0.002755 seconds.  We'll round
    # the values in aero_times to 8 decimal places then check during the
    # run if the runtime (also rounded to 8 d.p.) appears in aero_times.
    # The list comprehension below also cuts out print times above the
    # duration of the run.  While we're doing that, we also check
    # overlaps and duplicates in the times.  This could be caused by
    # list entries like "range(0, 100, 2) + range(85, 205, 5)"
    # which have overlaps in the range 85 to 100 and duplicates at
    # 90 and 100.  We do this in a subroutine because we want to
    # do the same thing in the time series for of timeloops.
    aero_times = SanitiseTimeSeries(aero_times, time_accuracy, duration)

    # Get the actual time the run ends at.  This may be slightly above
    # the desired run time because it is an integer multiple of the time
    # step.
    run_time = aero_times[-1]


    # Now seek out all the fanchar blocks.
    if debug1:
            print("Processing fan characteristics")
    result = GetBegins(line_triples, begin_lines, "fanchar",
                       0, math.inf, file_name, debug1, log)
    if result is None:
        return(None)
    fanchars_dict = {}
    for tr_index in result:
        result = ProcessFanChar(line_triples, tr_index, settings_dict, log)
        if result is None:
            return(None)
        else:
            (char_name, new_char_dict) = result
            # Check for name clashes between the fans and the tunnels
            # and routes.
            fanchars_dict.__setitem__(char_name, new_char_dict)
    # We don't need to check for there being no fans, as simple
    # files may not have them.

    if debug1:
        print("settings_dict")
        for entry in settings_dict:
            print(entry, ":", settings_dict[entry])
        print("sectypes_dict")
        for entry in sectypes_dict:
            print(entry, ":", sectypes_dict[entry])
        print("tunnels_dict")
        for t_entry in tunnels_dict:
            t_dict = tunnels_dict[t_entry]
            print("Tunnel", t_entry)
            for key in t_dict:
                print("   ", key, ":", t_dict[key])
        print("routes_dict")
        for entry in routes_dict:
            print(entry, ":", routes_dict[entry])

    # Once we get here, we've processed all the blocks of input that
    # control the run.

    # Now we want to split up the tunnels into items of constant cross-
    # section and define their frictional parameters (from the sectypes).
    # We call these items of constant cross-section "segments", because
    # they have many similarities to SES segments.
    #
    # There are three types of tunnel.  They are (in order of increasing
    # complexity):
    #  * "Simple" are air paths that only have friction, fixed losses,
    #    variable losses, changes of sectype, axial fans, jet fans and
    #    traffic.  No trains.
    #
    #  * "Navigable" tunnels have all the capabilities of simple tunnels
    #    but can also have moving trains.
    #
    # Note that these are internal designations.  As far as the input
    # file is concerned, there is no difference between them.
    #
    # The reason for doing this is that simple tunnels can be solved
    # by a simple method of characteristics solver.  We don't need to
    # figure out if there are any train boundaries along their lengths.
    #
    # Navigable tunnels need a very complex solver in which the friction
    # equations account for the perimeter of moving trains.  We have
    # to be able to adjust the size of segments in the tunnel at every
    # timestep (in case a train end passes a gridpoint).  We have to
    # to be ready to adjust the count of segments in the tunnel at each
    # timestep, in case a train end enters or leaves the tunnel during
    # the timestep.

    # Define a list to hold the tunnels that are complex and another
    # to hold the tunnels that may be traversed by trains.  Everything
    # else we take to be a simple tunnel.
    simple = []
    navigables = []
    for route_name in routes_dict:
        route_dict = routes_dict[route_name]
        # Figure out if this route has any trains in it.
        hastrains = False
        for key in route_dict:
            if key == "schedules":
                hastrains = True
                break
        if hastrains:
            navigables.extend(route_dict["tunnel_names"])
        else:
            simple.extend(route_dict["tunnel_names"])
    # Remove all the duplicate tunnels from the lists.
    simple = tuple(set(simple))
    navigables = tuple(set(navigables))


    # Make a list to hold which tunnel the segments are in.  The entries
    # are tunnel names.
    segment_source = []

    # Get some settings that we might need.
    units = settings_dict["units"]
    p_atm = settings_dict["p_atm"]
    rho_atm = settings_dict["rho_atm"]
    gamma = settings_dict["gamma"]
    frictiontype = settings_dict["frictiontype"]

    frictionapprox = settings_dict["frictionapprox"]
    # Turn the friction approximation into an integer so we can do a 'case'
    # test in the Fortran version of the friction calculation routine
    # "Friccalc".  There is a Python version (which is slow) and a Fortran
    # version (much faster).
    if frictionapprox == "colebrook":
        fric_app_num = 1
    elif frictionapprox == "colebrook-white":
        fric_app_num = 2
    elif frictionapprox == "moody":
        fric_app_num = 3
    elif frictionapprox == "ses":
        fric_app_num = 4
    else:
        # We added a new friction factor approximation to PROC ProcessSettings
        # but didn't add code to handle it here.  This is likely to have been
        # done by someone forking the code, so give some help here for the
        # programmer but not a full-blown error message with a faulty line
        # of input to point at.
        err = ('> A new type of friction factor approximation has been\n'
               '> added (' + frictionapprox + ') but code to handle it has\n'
               '> not been added to PROC ProcessCalc and likely also not\n'
               '> added to PROC FricFac, compressible.f95 or the tuple\n'
               '> "fric_approx_list".  Please complain to the programmer.')
        gen.WriteError(2084, err, log)
        return(None)
    # Create a list of the available friction factor approximations.  This
    # is used to print the friction factors at 5 m/s to the logfile in a
    # list alongside one another.
    fric_approx_list = (" Colebrook's 1939 approximation ",
                        " Colebrook-White (exact) ",
                        " Moody's 1947 approximation ",
                        " SES's 1974 approximation ",
                       )


    # Figure out how many timesteps we need, rounding up if needed.
    # We run it through SanitiseTimeList to iron out any slight mismatches
    # between the values in time_list and the values in aero_times.

    time_list = SanitiseTimeSeries(np.arange(dt, duration + 0.2 * dt, dt),
                                   time_accuracy, duration)

    # Define four lists to hold the values of distance, air velocity
    # elevations and celerity (speed of sound).  Distances and elevations
    # are fixed, velocities and celerities vary at each time step.
    # The elevations are to let the calculation account for the variation
    # of density with height.
    dists = []
    elevs = []
    vels = []
    cels = []
    # Create a list to hold a list of the physical properties in each
    # segment.
    segments_consts = []

    # Create a dictionary to hold the pointers to main fans.  These
    # use the fan name as the key and return a list of things that can
    # be used to plot from them.
    tunnelfans_dict = {}

    # Create two dictionaries to hold the pointers to variable loss factors
    # (dampers, PSDs, tunnel doors etc.).  These use the loss name as
    # the key and return a list of things that can be used to plot from
    # them.  Although we call it "timedlosses" this also holds the
    # properties of things like gravity-operated flap dampers, whose
    # resistances vary with pressure.
    #
    timedlosses_dict = {}
    dampertiming_dict = {}

    # Generate a dictionary to hold the details of what happens at nodes
    # and adits.  Nodes and adits have names.  We use the name as the key
    # and each time we find something connected to that name, we add details
    # of which segment(s) are connected to it and which end they are
    # connecting with.  For example:
    # {"XP1a" : [("dopey", 4), ("dopey", -5), ("XP1", -1)]
    # This means that the adit named "XP1a" connects to the forward end
    # of the fourth segment of the tunnel named "dopey", the back end of
    # the fifth segment of the same tunnel and the back end of the first
    # segment in the tunnel named "XP1".
    joins_dict = {}


    # We create a counter of segments, which we use in the aditnodes
    # dictionary.  If the seg_ID is +ve, the forward end of a segment
    # is attached.  If -ve, the back end is attached.  This is also
    # used to track which segment each train boundary is in at each
    # timestep.  If the seg_ID is zero, then a train boundary is
    # in the open air.
    seg_ID = 0

    # Create a dictionary to hold the list of the seg_IDs in each tunnel.
    # The key is the tunnel name, the list starts at the back end and
    # ends at the forward end.  Also create a list that does the reverse
    # i.e., if we treat the seg_ID as a list index it returns the name of
    # the tunnel the seg_ID is in.
    # There is no seg_ID of zero, so we put something in that may help
    # when we try to pick up the tunnel name associated with seg_ID zero
    # (it's going to happen at least once).
    tuns2segs = {}
    segs2tuns = ["#Oops, tried to use a seg_ID of zero in segs2tuns"]

    # Build lists of the distances at the back end and forward end of
    # segments.  We use this to switch between the route chainages and
    # the distances along segments.  The first entry is not used because
    # the index is the seg_ID, which starts at 1, not zero.
    seg_lengths = ["Not used"]

    # Now we iterate over each tunnel, dividing it into segments of constant
    # sectype.  We split at area changes, discrete pressure losses, adits,
    # fans, dampers and tunnel ends.
    gen.WriteOut('\nSplitting up the tunnels into segments with'
                 ' unique seg_IDs.\n',
                 log)
    for tun_key in tunnels_dict:
        tunnel_data = tunnels_dict[tun_key]
        # First we get all the stuff in the tunnel into new lists that
        # hold their key but remove the comment text.
        # We could build the segments from the contents of the dictionary
        # but it is tedious.
        tun_details = []

        for key in tunnel_data:
            # We add all the entities except:
            #  * the pointer to the start of the block,
            #  * any "joins" keywords (because when reading the tunnel
            #    we create a set of "join" commmands from them),
            #  * any "jetfans1" keywords, because their influences are
            #    body forces in the same way that road traffic drag is.
            if ( key != "block_index" and
                 key[:6] != "joins#" and
                 key[:9] != "jetfans1#" and
                 key[:10] != "sespragmat"):
                entity_details = list(tunnel_data[key])
                entity_details.pop(-3)
                # Make a list of the key and the remaining details.
                tun_details.append([key] + entity_details)

        # Sort the list based on the distances of the points (we have
        # no guarantee that the user entered the entities in order of
        # increasing distances).
        tun_details.sort(key=operator.itemgetter(1))


        # Check if the user set the forward end distance below the
        # back end distance.
        if tun_details[0][0] != "back":
            # They did.  Reverse the list.
            tun_details.reverse()
        if debug1:
            print("tun_details")
            for detail in tun_details:
                print(" ", detail)

        # Create a list of the seg_IDs in this tunnel and the distances
        # they start and end at.  This is the result returned by the
        # tunnel name in tunnel_segs.  We use it to figure out the
        # orientation of the tunnels in routes and when plotting
        # fixed data in tunnels against distance.
        #
        tun_segs = []

        # Now loop over the contents of tun_details and populate local
        # arrays for the segment geometries.  First we define the
        # arrays.

        # Distances at the ends of segments.  This is one longer than most
        # of the other lists.
        end_dists = []

        # End types holds a letter that identifies what is at the back end
        # of segments.
        #   "c" is used for changes of sectype and fixed losses.
        #   "d" is used for dampers whose properties vary with runtime.
        #   "f" is used for main fans that fill the cross-section (not jet fans)
        #   "n" is used for adits and nodes.
        #   "p" is used for portals with a fixed pressure at them
        #   "v" is used for portals with a fixed velocity or fixed volume
        #       flow into or out of them.
        back_end_types = []
        fwd_end_types = []
        # End values holds the values associated with the end types.  In
        # keeping with the examples given above we would have the following:
        #   "c" gets the number of the adjacent segment.
        #   "d" gets a tuple of values defining the damper in this segment.
        #       It is only used at forward ends.
        #   "f" gets a tuple of values defining the fan in this segment.
        #       It is only used at forward ends.
        #   "n" gets the name of the node.
        #   "p" gets the gauge pressure at the portal.
        #   "v" gets the air velocity at the portal (either specified
        #       directly or calculated from a volume flow).
        back_end_values = []
        fwd_end_values = []
        # The remainder are just the physical properties of the segments and
        # the pressure losses at each end.
        areas = []
        perims = []
        d_hs = []
        epsilons = []
        relroughs = [] # This is roughness height divided by d_h.
        zetas_back_bf = []
        zetas_back_fb = []
        zetas_fwd_bf = []
        zetas_fwd_fb = []

        # We write details to the log file, and store descriptions
        # at ends.
        end_descrips = []

        # Create a counter that we can use for the node names at
        # changes of section.
        changes = 1
        for index, details in enumerate(tun_details):
            keyword = details[0]
            if keyword != "fwd":
                segment_source.append(tun_key)
            # Get the optional arguments and the line index.
            (optionals_dict, tr_index) = details[-2:]
            # Get the line number and line text, for error messages.
            (line_number, discard, line_text) = line_triples[tr_index]

            # Store the location of this feature (distance along the
            # tunnel).
            end_dists.append(details[1])

            if keyword == "back":
                # This is a back end, get the values in sectype.
                sec_name = details[4]
                new_sectype = True
                (area, perim, roughness) = sectypes_dict[sec_name.lower()][:3]
                # Increment the segment ID when we hit a back end.
                seg_ID += 1
            elif keyword == "fwd":
                new_sectype = False

            if keyword in ("back", "fwd"):
                # Get the conditions at the end.  This processes fixed
                # pressures, fixed velocities, fixed volume flow, and nodes,
                # returns a letter and a value (like "p" and a pressure in Pa).
                letter, value, descrip = PortalConstraints(details, area, p_atm,
                                                           c_atm, gamma,
                                                           gammapsi, have_ftn)
                if letter == "n":
                    # The end of this tunnel is a node at which two or more
                    # tunnels meet.  Add it to the dictionary of tunnels
                    # with the node name.

                    # Add pointers to the appropriate segment end.
                    if keyword == "back":
                        joins_dict = AddToJoin(joins_dict, value, -seg_ID)
                    else:
                        joins_dict = AddToJoin(joins_dict, value, seg_ID)
                    # Check if there were any optional arguments setting the
                    # pressure loss factors at the join.  If there were none,
                    # default to zero for both flow directions.
                    result = PortalZetas(keyword, area, 0.0, 0.0, optionals_dict,
                                         settings_dict, file_name, tunnel_name,
                                         line_number, line_text, log)
                    if result is None:
                        return(None)
                    else:
                        zeta_bf, zeta_fb = result
                elif letter in ("v", "q"):
                    # We are forcing a fixed velocity boundary condition
                    # so the pressure loss coefficients don't matter, the
                    # program will generate whatever celerity is needed to
                    # get to that velocity (within reason; it may just crash).
                    zeta_bf = 0.0
                    zeta_fb = 0.0
                    if keyword == "fwd":
                        # We need to multiply the fixed velocity by -1 at
                        # the forward end, because the convention is that
                        # positive inflow values at the forward end are
                        # negative air velocities in the calculation grid.
                        value = -value
                elif letter == "p":
                    # Check if there were any optional arguments setting the
                    # pressure loss factors at the portal to atmosphere.  If
                    # there were none, use 0.5 for inflow and 1.0 for outflow.
                    result = PortalZetas(keyword, area, 0.5, 1.0, optionals_dict,
                                         settings_dict, file_name, tunnel_name,
                                         line_number, line_text, log)
                    if result is None:
                        return(None)
                    else:
                        zeta_bf, zeta_fb = result
                if keyword == "back":
                    back_end_types.append(letter)
                    back_end_values.append(value)
                    zetas_back_bf.append(zeta_bf)
                    zetas_back_fb.append(zeta_fb)
                else:
                    fwd_end_types.append(letter)
                    fwd_end_values.append(value)
                    zetas_fwd_bf.append(zeta_bf)
                    zetas_fwd_fb.append(zeta_fb)
            elif keyword[:6] == "change":
                sec_name = details[2]
                if sec_name == "same":
                    # The user wants to use the same sectype as in the
                    # previous segment (setting a change to the same
                    # sectype can be useful for forcing a plot point at
                    # a specific location).  We keep the section properties
                    # unchanged, but we still make the call to GetChangeZetas
                    # in case there are optional entries setting pressure
                    # loss factors here.  I don't know why anyone would
                    # do that, but it is not forbidden.
                    pass
                else:
                    (area, perim, roughness) = sectypes_dict[sec_name.lower()][:3]
                # Call a routine that sets the four pressure loss factors
                # on either side of the change.  This will either be from
                # expressions for abrupt contractions / expansions, from
                # optional entries on the line that set them directly or
                # from a mixture of the two.  Two of these values will
                # always be zero, but we don't know which.
                result = GetChangeZetas(area, areas[-1], optionals_dict,
                                        file_name, tunnel_name,
                                        line_number, line_text, log)
                (zeta_1, zeta_2, zeta_3, zeta_4) = result
                zetas_fwd_bf.append(zeta_1)
                zetas_fwd_fb.append(zeta_2)
                zetas_back_bf.append(zeta_3)
                zetas_back_fb.append(zeta_4)

                # Make a note that the forward end of the segment is an
                # area change.  We solve the values at the gridpoint at
                # the end by tying together a forward characteristic at
                # the last cell in this segment and a backwards char-
                # acteristic in the first cell in the next segment.
                fwd_end_types.append("c")
                fwd_end_values.append(index + 1)

                # When we are processing the next segment we can ignore
                # the first cell.
                back_end_types.append("c")
                back_end_values.append(index)
                descrip = "change of sectype"

                # Create a name for the node and update the counter
                # of node changes.  We include a hash character so
                # that it can't clash with a user's node name.
                node_name = tun_key + '-#' + str(changes)
                changes += 1
                # Add two node connections at the change of sectype, one
                # pointing to the forward end of the current segment and
                # the other pointing to the back end of the next segment.
                #
                joins_dict = AddToJoin(joins_dict, node_name, seg_ID)
                seg_ID += 1
                joins_dict = AddToJoin(joins_dict, node_name, -seg_ID)
            elif keyword[:3] == "fan":
                # This is either "fan1" or "fan2".  "Fan1" is a fan that
                # is turned on and off by optional arguments (if there
                # are no optional arguments that set times, then the fan
                # starts at zero seconds and runs for the whole simulation).
                # "Fan2" is a fan with the nickname of a data block/csv
                # file that defines a time series setting the fan speed.
                #
                # The mandatory inputs are the name of the fan (so that
                # we can plot at the fan) and the name of the fan
                # characteristic definition.
                fan_name, char_name = details[2:4]
                # Get the fan data (a list of pairs of times and fan speeds
                # and the modified fan characteristic at the given speed).
                # The modified fan characteristic is a list of pairs of air
                # velocities and pressure rise (P_tot / rho in m^2/s^2),
                # which looks weird but is sensible because it means fewer
                # arithmetic operations inside the calculation loop.
                result = GetCharData(seg_ID, area, char_name, optionals_dict,
                                    fanchars_dict, settings_dict, file_name,
                                    tunnel_name, line_number, line_text,
                                    debug1, log)
                if result is None:
                    return(None)
                else:
                    fan_details, lines1 = result
                    # Fan_details is a list of four numpy arrays that
                    # are used to calculate the fan performance in this
                    # segment.
                    # [np.array of air velocities on the fan curve for
                    #    forwards rotation (air velocities in this
                    #    segment's area at fan speed ratio 1.0),
                    #  np.array of pressures on the forwards fan curve
                    #    (expressed as stagnation pressure coefficients
                    #     at fan speed ratio 1.0),
                    #  np.array of air velocities on the fan curve for
                    #    reverse rotation (air velocities in this
                    #    segment's area),
                    #  np.array of pressures on the reverse fan curve
                    #    (expressed as stagnation pressure coefficients),
                    # ]
                    # 'lines1' lists the fan performance in this tunnel
                    # (air velocity and dimensionless total pressure
                    # rise) that will be printed to the log file.


                if keyword[:4] == "fan1":
                    # "fan1" is a fan that is turned on and off by optional
                    # arguments (if there are no optional arguments that set
                    # times then the fan starts at zero seconds and runs
                    # for the entire simulation).
                    fan_speed = details[4]
                    fan_timing, lines2 = Fan1Timing(fan_speed, optionals_dict,
                                                    char_name, fanchars_dict,
                                                    settings_dict)
                    # 'fan_timing' is a list of two np.arrays, one of
                    # times that set the fan operation, another of fan
                    # speeds at each time.
                    # 'lines2' lists the times and speeds in a way
                    #  that can be printed to the log file later.
                elif keyword[:4] == "fan2":
                    # "fan2" is a fan that follows a time-speed curve set
                    # in a data block or .csv file.  We get the nickname
                    # and the names/numbers of the columns from the line
                    # of input and feed them to a routine that gets the
                    # lists of numbers back.
                    result = Fan2Timing(details, char_name, fanchars_dict,
                                        settings_dict, file_name,
                                        line_triples, log)
                    if result is None:
                        return(None)
                    else:
                        fan_timing, lines2 = result
                    # 'fan_timing' is a list of two np.arrays, one of
                    # times that set the fan operation, another of fan
                    # speeds at each time.
                    # 'lines2' lists the times and speeds in a way
                    #  that can be printed to the log file later.
                # Set the fan pressure losses at the forward end of this
                # segment, set set zero pressure losses at the back end
                # of the adjacent segment.  These are all zero at fans.
                zetas_fwd_bf.append(0.0)
                zetas_fwd_fb.append(0.0)
                zetas_back_bf.append(0.0)
                zetas_back_fb.append(0.0)


                # Make a note that the forward end of the segment is a fan
                # and store the key to it in the fans dictionary.
                fwd_end_types.append("f")
                fwd_end_values.append(fan_name)

                # When we are processing the next segment we can ignore the
                # first cell, so we pretend it is a change and put in 'c'.
                back_end_types.append("c")
                back_end_values.append(index)
                descrip = "axial/centrifugal fan"

                # Create a name for the node and update the counter
                # of node changes.  We include a hash character so
                # that it can't clash with a user's node name.
                node_name = tun_key + '-#' + str(changes)
                changes += 1
                # Add two node connections at the change of sectype, one
                # pointing to the forward end of the current segment and
                # the other pointing to the back end of the next segment.
                #
                joins_dict = AddToJoin(joins_dict, node_name, seg_ID)
                seg_ID += 1
                joins_dict = AddToJoin(joins_dict, node_name, -seg_ID)
                new_sectype = False

                # Check if the name has already been used for something
                # else.
                result = CheckEntityName(fan_name, "a fan", tun_key,
                                         line_number, line_text,
                                         entities_dict, file_name, log)
                if result is None:
                    return(None)
                else:
                    entities_dict = result
                    # Build a list of the fan's details and store them
                    # in a dictionary specific to each fan in each tunnel.
                    #
                    #
                    # 'fan_details' and 'fan_timing' are used in the
                    # calculation.
                    #
                    # Fan_name is used in warning messages when the method
                    # of characteristics doesn't converge and in the
                    # plotting.
                    #
                    # Char_name and the segment numbers are used in plotting.
                    # We subtract 1 & 2 from the seg_IDs so they are the
                    # segment indices of the segments on both sides of the
                    # fan rather than the seg_IDs.
                    #
                    # The lists of lines of text is written to the log
                    # file later.
                    #
                    # We store the tunnel name, line number and line text
                    # for use in the error message in CheckEntityName.
                    fan_stuff = fan_details + fan_timing + [fan_name,
                                keyword[:4], char_name, seg_ID - 2, seg_ID - 1,
                                lines1 + lines2, tun_key,
                                line_number, line_text]

                    tunnelfans_dict.__setitem__(fan_name, fan_stuff)
            elif keyword[:4] == "loss":
                # These keywords are both fixed pressure loss factors.
                if keyword[:5] == "loss1":
                    # "loss1" is a fixed pressure loss that has an area and
                    # two k-factors, one for +ve airflow, the other for -ve.
                    area_loss = float(CheckForConstant(details[2], False,
                                                       settings_dict))
                    zeta_bf = float(CheckForConstant(details[3], False,
                                                     settings_dict))
                    zeta_fb = float(CheckForConstant(details[4], False,
                                                     settings_dict))

                    if units == "us":
                        # Convert the area from ft^2.
                        area_loss = area_loss * 0.09290304
                    descrip = "pressure loss: area = "  \
                              + gen.RoundText(area, 3)  \
                              + " m^2, zeta_bf = " + gen.RoundText(zeta_bf, 5) \
                              + ", zeta_fb = " + gen.RoundText(zeta_fb, 5)
                    # Adjust the k-factor values so that they apply to the
                    # current area of the segment.
                    if not math.isclose(area, area_loss, abs_tol = 1e-9):
                        adjustment =  (area / area_loss) **2
                        zeta_bf = zeta_bf * adjustment
                        zeta_fb = zeta_fb * adjustment
                elif keyword[:5] == "loss2":
                    # "loss2" is two values of Atkinson resistance, one for
                    # +ve airflow, one for -ve airflow.
                    R_bf = float(CheckForConstant(details[2], False,
                                                  settings_dict))
                    R_fb = float(CheckForConstant(details[3], False,
                                                  settings_dict))
                    if units == "us":
                        # Convert from lb-s^2/ft^6 to Pa-s^2/m^6.
                        R_bf = USc.ConvertToSI("atk", R_bf, debug1, log)[0]
                        R_fb = USc.ConvertToSI("atk", R_fb, debug1, log)[0]

                    # Convert the Atkinson resistances R to pressure loss
                    # factors zeta.  The relationships between zeta (which
                    # is dimensionless) and R (N-s^2/m^8) are:
                    #
                    #       1                      1              Q^2
                    # DP =  - * rho * zeta * v^2 = - * rho * zeta --- = R * Q^2
                    #       2                      2              A^2
                    #
                    #         rho * zeta
                    # => R =  ----------   and
                    #           2 * A^2
                    #
                    #           2 * A^2 * R
                    # => zeta = -----------
                    #              rho
                    #
                    # where:
                    #  * rho is air density 1.2 kg/m^3
                    #  * A is segment area (m^2)
                    #  * Q is volume flow (m^3/s)
                    #  * v is air velocity (m/s)
                    #
                    adjustment =  2 * area**2 / 1.2
                    zeta_bf = R_bf * adjustment
                    zeta_fb = R_fb * adjustment

                    descrip = "fixed Atkinson resistance: "  \
                              + "R_bf = " + str(R_bf) + " gauls, "\
                              + "R_fb = " + str(R_fb) + " gauls"
                # Set the fixed pressure losses at the forward end of this
                # segment, set set zero pressure losses at the back end
                # of the adjacent segment.
                zetas_fwd_bf.append(zeta_bf)
                zetas_fwd_fb.append(zeta_fb)
                zetas_back_bf.append(0.0)
                zetas_back_fb.append(0.0)
                # Make a note that the forward end of the segment is an
                # area change.  We solve the values at the gridpoint at
                # the end using the same routine that we use for changes
                # of sectype (hence the "c").
                fwd_end_types.append("c")
                fwd_end_values.append(index + 1)

                # When we are processing the next segment we can ignore
                # the first cell.
                back_end_types.append("c")
                back_end_values.append(index)

                # Create a name for the node and update the counter
                # of node changes.  We include a hash character so
                # that it can't clash with a user's node name.
                node_name = tun_key + '-#' + str(changes)
                changes += 1
                # Add two node connections at the loss, one pointing
                # to the forward end of the current segment and the
                # other pointing to the back end of the next segment.
                joins_dict = AddToJoin(joins_dict, node_name, seg_ID)
                seg_ID += 1
                joins_dict = AddToJoin(joins_dict, node_name, -seg_ID)
                new_sectype = False
            elif keyword[:6] == "damper":
                # "damper1" has area and two pressure losses zeta that
                # vary with time, "damper2" is two Atkinson resistances
                # that vary with time.
                #
                # The input for "damper1" has a name (so that the loss
                # can be plotted at), the nickname of a datasource and
                # the following four column numbers or column names in
                # the datasource:
                #  * time,
                #  * area,
                #  * loss factor for +ve flow,
                #  * loss factor for -ve flow.
                #
                # The input for "damper2" also has a name, the nickname
                # of a datasource and the following three column numbers
                # or column names:
                #  * time,
                #  * Atkinson resistance for +ve flow,
                #  * Atkinson resistance for -ve flow.
                #
                # The entries are turned into lists of time-varying
                # stagnation pressure loss coefficients.  There is one
                # for entry for each of time in 'time_list'.  This
                # routine also returns a list of lists of the properties
                # at each timestep, which we write to the binary file for
                # plotting.
                result = TransientLoss1(keyword, details, area, settings_dict,
                                        file_name, time_list, aero_times,
                                        line_triples,  log)
                if result is None:
                    return(None)
                else:
                    damper_name, damper_details, lines = result
                    # 'damper_details' has the following:
                    #   * one np.array of the stagnation pressure loss
                    #     coefficients at each timestep for +ve flow,
                    #   * one np.array of the stagnation pressure loss
                    #     coefficients at each timestep for -ve flow,
                    #   * a tuple of five np.arrays (for plotting at
                    #     type 1 dampers) or two np.arrays (for plotting)
                    #     at type 2 dampers.  These are converted into
                    #     pandas databases after the calculation finishes.
                    #  'lines' lists the details in a way that
                    #   can be printed to the log file later.
                    # Store these details in the runtime damper dictionary.
                    # We don't store it in timedlosses_dict because we
                    # write that to the binary file and the contents of
                    # damper_details are large and not needed.
                # Set the fixed pressure losses at the forward end of this
                # segment, set set zero pressure losses at the back end
                # of the adjacent segment.
                zetas_fwd_bf.append(0.0)
                zetas_fwd_fb.append(0.0)
                zetas_back_bf.append(0.0)
                zetas_back_fb.append(0.0)
                # Make a note that the forward end of the segment is an
                # damper.  We solve the values at the gridpoint at
                # the end using the same routine that we use for changes
                # of sectype (hence the "c").
                fwd_end_types.append("d")
                fwd_end_values.append(damper_name)

                # When we are processing the next segment we can ignore
                # the first cell.
                back_end_types.append("c")
                back_end_values.append(index)
                descrip = 'time-varying damper "' + damper_name + '"'

                # Create a name for the node and update the counter
                # of node changes.  We include a hash character so
                # that it can't clash with a user's node name.
                node_name = tun_key + '-#' + str(changes)
                changes += 1
                # Add two node connections at the loss, one pointing
                # to the forward end of the current segment and the
                # other pointing to the back end of the next segment.
                joins_dict = AddToJoin(joins_dict, node_name, seg_ID)
                seg_ID += 1
                joins_dict = AddToJoin(joins_dict, node_name, -seg_ID)
                new_sectype = False

                # Check if the name has already been used for something
                # else.
                result = CheckEntityName(damper_name, "a damper", tun_key,
                                         line_number, line_text,
                                         entities_dict, file_name, log)
                if result is None:
                    return(None)
                else:
                    entities_dict = result
                # Build a list of the damper's details and store them
                # them in two dictionaries specific to each fan in each
                # tunnel.  One is used in errors and plotting, one
                # is used during the run.
                #
                # Damper_name is used in warning messages when the
                # method of characteristics doesn't converge and in
                # the plotting.
                #
                # We subtract 1 & 2 from the seg_IDs so they are the
                # segment indices of the segments on both sides of the
                # damper rather than the seg_IDs.
                #
                # The lists of lines of text is written to the log
                # file later.
                #
                # We store the tunnel name, line number and line text
                # for error messages about duplicate damper names.
                damper_stuff = [keyword, seg_ID - 2, seg_ID - 1, lines,
                                tun_key, line_number, line_text]
                timedlosses_dict.__setitem__(damper_name, damper_stuff)
                dampertiming_dict.__setitem__(damper_name, damper_details)
            elif keyword[:4] == "join":
                # Check if there is an optional argument setting a new
                # sectype at the adit.
                if "sectype" in details[-2]:
                    sec_name = details[-2]["sectype"]
                    # Now check if this sectype exists in the file
                    # and complain if it does not.
                    if sec_name not in sectypes_dict:
                        # Oops.
                        line_index = details[-1]
                        err = ('> In the file named "' + file_name + '"\n'
                               '> an optional argument in tunnel "'
                                 + tunnel_name + '"\n'
                               '> set a new sectype ("' + sec_name + '") for '
                                 + 'the tunnel.\n'
                               '> There is no sectype named "' + sec_name + '"\n'
                               '> Please edit the file to correct it.  For\n'
                               "> what it's worth, here are the names of\n"
                               "> the available sectype(s):\n"
                              )
                        err = err + gen.FormatOnLines(sectypes_dict.keys())
                        gen.WriteError(2085, err, log)
                        gen.ErrorOnLine2(line_index, line_triples, log, False)
                        return(None)
                    else:
                        (area, perim, roughness) = sectypes_dict[sec_name.lower()][:3]
                        new_sectype = True
                else:
                    new_sectype = False


                # Set zero values for the k-factors on both sides of the adit.
                # We will replace these when we figure out how we want to
                # process the adit.
                zetas_fwd_bf.append(0.0)
                zetas_fwd_fb.append(0.0)
                zetas_back_bf.append(0.0)
                zetas_back_fb.append(0.0)
                node_name = details[2].lower()
                # Check if the name has already been used for something
                # else.
                result = CheckEntityName(node_name, "a join", tun_key,
                                         line_number, line_text,
                                         entities_dict, file_name, log)
                if result is None:
                    return(None)
                else:
                    entities_dict = result
                fwd_end_types.append("n")
                fwd_end_values.append(node_name)
                back_end_types.append("n")
                back_end_values.append(node_name)
                descrip = 'adit with a node named "' + details[2] + '"'

                # Add pointers to the two segments on either side of this
                # adit to the joins dictionary.
                joins_dict = AddToJoin(joins_dict, node_name, seg_ID)
                seg_ID += 1
                joins_dict = AddToJoin(joins_dict, node_name, -seg_ID)

            # Once we get to here we have processed the entity at the
            # end of the segment.  We may have a new sectype, we may not.
            # Add the sectype of this segment to the lists.
            end_descrips.append(descrip)
            areas.append(area)
            perims.append(perim)
            if new_sectype:
                d_h = 4 * area / perim
                d_hs.append(d_h)
                calc_rough = GetRoughness(roughness, frictiontype)
                epsilons.append(calc_rough)
                if calc_rough > 0.0:
                    relroughs.append(calc_rough / d_h)
                else:
                    # This segment has fixed friction factor.  In the
                    # event that we start using the relative roughness
                    # accidentally in calculations, we want them to fail.
                    relroughs.append(math.nan)
            else:
                # Same as the sectype in the previous segment.
                d_hs.append(d_hs[-1])
                epsilons.append(epsilons[-1])
                relroughs.append(relroughs[-1])
            # Add the segment ID to the list of segments in this
            # tunnel.
            tun_segs.append(seg_ID)



        # Now write the details of these segments to the log file for
        # error checking.  These are always in SI units because I reckon
        # anyone that wants to delve this deep into what the program is
        # up to will be familiar with them.
        # While we're doing this, we build a list of segment lengths.

        for index, (back_dist, fwd_dist, back_descrip, fwd_descrip,
                    area, perim, d_h, roughness, rr, zeta_back_bf,
                    zeta_back_fb, zeta_fwd_bf, zeta_fwd_fb,
                    back_type, back_value, fwd_type, fwd_value) in \
                    enumerate(
                      zip(end_dists[:-1], end_dists[1:],
                          end_descrips[:-1], end_descrips[1:],
                          areas, perims, d_hs, epsilons, relroughs,
                          zetas_back_bf, zetas_back_fb,
                          zetas_fwd_bf, zetas_fwd_fb,
                          back_end_types, back_end_values,
                          fwd_end_types, fwd_end_values)):

            # Put a descriptive line above the list of properties of each
            # segment in the log file.  First figure out what the segment
            # ID of this segment is.
            area_count = len(areas)
            print_ID = seg_ID - area_count + index + 2
            if area_count == 2:
                # There is only one segment in this tunnel.
                gen.WriteOut(' Seg_ID ' + str(print_ID)
                              + ', 1st (and only) segment of tunnel "'
                              + tun_key + '":', log)
            elif index == area_count - 2:
                # There was more than one segment and this is the last one.
                gen.WriteOut(' Seg_ID ' + str(print_ID)
                              + ', last segment of tunnel "'
                              + tun_key + '":', log)
            else:
                # It's a segment in the middle somewhere.
                gen.WriteOut(' Seg_ID ' + str(print_ID) + ', '
                              + gen.Enth(index + 1) + ' segment of tunnel "'
                              + tun_key + '":', log)
            length = abs(fwd_dist - back_dist)
            # Check for descriptions at the back end that have fixed
            # losses.  If we find them we remove the numbers because
            # the losses are at the forward end of the previous segment.
            if "pressure loss: area" in back_descrip:
                gen.WriteOut('              Back end: ' + gen.FloatText(back_dist)
                               + ' m (fixed pressure loss)', log)
            elif "fixed Atkinson resistance:" in back_descrip:
                gen.WriteOut('              Back end: ' + gen.FloatText(back_dist)
                               + ' m (fixed Atkinson resistance)', log)
            else:
                gen.WriteOut('              Back end: ' + gen.FloatText(back_dist)
                               + ' m (' + back_descrip + ')', log)
            gen.WriteOut('           Forward end: ' + gen.FloatText(fwd_dist)
                           + ' m (' + fwd_descrip + ')\n' +
                         '        Segment length: ' + gen.FloatText(length)
                           + ' m\n' +
                         '          Segment area: ' + gen.FloatText(area)
                           + ' m^2\n' +
                         '             Perimeter: ' + gen.FloatText(perim)
                           + ' m\n' +
                         '    Hydraulic diameter: ' + gen.RoundText(d_h, 4)
                           + ' m', log)
            if roughness < 0.0:
                # It's a fixed friction factor.
                gen.WriteOut('     Fixed Fanning c_f: '
                              + gen.FloatText(-roughness) +
                           '\n    Fixed Darcy lambda: '
                              + gen.FloatText(-4 * roughness) +
                           '\n      Fixed Atkinson k: '
                              + gen.FloatText(-0.5 * 1.2 * roughness)
                              + ' kg/m^3', log)
            else:
                gen.WriteOut('      Roughness height: '
                              + str(roughness) + ' m\n'
                           + '    Relative roughness: '
                              + str(round(rr, 7)), log)
                # Put two list of friction factors at airspeed 5 m/s into the
                # log file.  First we write lines giving the Fanning friction
                # factors (tagging the one that we are using), then we write
                # one line with the equivalent Darcy friction factor that we
                # are using.
                for fric_index, descrip in enumerate(fric_approx_list, start=1):
                    if have_ftn:
                        ftfric = ftn.fricfac(d_h, roughness, rr, rr/3.7,
                                             5.0, fric_index)
                    else:
                        ftfric = FricFac(d_h, roughness, rr, rr/3.7,
                                         5.0, fric_index)
                    # Round to six decimal places because we only use this
                    # for printing (seven for Atkinson friction factor
                    # because we are multiplying by 0.6).
                    if fric_index == fric_app_num:
                        text_to_use = descrip + "(used)"
                        DarcyText = (' '*10+'>>> Darcy lambda at 5 m/s: '
                                     + str(round(4 * ftfric, 6)) + text_to_use)
                        AtkinsonText = (' '*10+'>>> Atkinson k at 5 m/s:   '
                                        + str(round(0.5 * 1.2 * ftfric, 7))
                                        + ' kg/m^3')
                    else:
                        text_to_use = descrip
                    gen.WriteOut(' '*10 + '>>> Fanning c_f at 5 m/s:  '
                                   + str(round(ftfric, 6)) + text_to_use, log)
                gen.WriteOut(DarcyText, log)
                gen.WriteOut(AtkinsonText, log)

            gen.WriteOut('      Back inflow zeta: '
                            + gen.FloatText(zeta_back_bf) + '    (zeta_bf)\n' +
                         '     Back outflow zeta: '
                            + gen.FloatText(zeta_back_fb) + '    (zeta_fb)', log)

            if fwd_type in ("d", ):
                # This is a damper
                gen.WriteOut('  Forward outflow zeta: varies (zeta_bf)\n' +
                             '   Forward inflow zeta: varies (zeta_fb)', log)
            else:
                gen.WriteOut('  Forward outflow zeta: '
                                + gen.FloatText(zeta_fwd_bf) + '    (zeta_bf)\n' +
                             '   Forward inflow zeta: '
                                + gen.FloatText(zeta_fwd_fb) + '    (zeta_fb)', log)


            # While we are in this loop we might as well build a set of
            # gridpoints within this segment and store the values we need
            # for the calculation.
            seg_lengths.append((back_dist, fwd_dist))
            # Check if the segment is too short.
            if length < dx:
                # Two things are two close together.  We need a complex
                # error message complaining about two lines and offering
                # possible solutions (use a shorter timestep or widen
                # the spacing).
                #
                # Figure out what timestep would be suitable.
                # We have already imposed a limit on things being
                # too close (1 m), so the shortest timestep we could
                # recommend is 1/(343+20) or 0.00276 sec (five digits).
                # As a general rule I wouldn't use a timestep higher
                # than 0.1 seconds (computers are just too fast these
                # days), so we'll use 0.1 as an upper limit in the
                # suggested changes.
                if length <= 2.:
                    # The segment is under 2 metres, we need four decimal
                    # places in the aero timestep.
                    digits = 4
                    subtract = 0.0001
                elif length <= 10.:
                    # It's under 10 metres, we can go with three digits
                    digits = 3
                    subtract = 0.001
                elif length <= 36.3:
                    # Note: 36.3 m cells is what you get from c_atm = 343 m/s
                    # and u_max = 20 m/s.  It leads to a timestep of 0.1 sec.
                    digits = 2
                    subtract = 0.005
                else:
                    # They used a timestep over 0.1 seconds, which is
                    # a bit weird.  Suggest 0.1 seconds.
                    digits = 2
                    subtract = 0.0

                suggested = min(0.1, round(length / dx * dt - subtract, digits))
                min_tstep = gen.FloatText(suggested)
                if units == "si":
                   min_len = str(round(dx, 2)) + " m"
                   act_len = str(round(length, 2)) + " m"
                   ch1 = str(round(back_dist, 2)) + " m"
                   ch2 = str(round(fwd_dist, 2)) + " m"
                else:
                   min_len = str(round(dx/0.3048, 2)) + " ft"
                   act_len = str(round(length/0.3048, 2)) + " ft"
                   ch1 = str(round(back_dist/0.3048, 2)) + " ft"
                   ch2 = str(round(fwd_dist/0.3048, 2)) + " ft"
                #
                # Get the line numbers and line text of the lines we
                # want to complain about.
                (line1_num, line_data1, line1_text) =  \
                                line_triples[tun_details[index][-1]]
                (line2_num, line_data2, line2_text) =  \
                                line_triples[tun_details[index + 1][-1]]

                if len(tun_details) == 2:
                    # We complain about the two tunnel ends being
                    # too close together.
                    err = ('> In the file named "' + file_name + '"\n'
                           '> tunnel "' + tunnel_name
                             + '" is too short to calculate\n'
                           '> in with the aero timestep you set.\n'
                           '> The tunnel is ' + act_len
                             + ' long and the aero\n'
                           '> timestep is ' + str(dt) + ' seconds.\n'
                           '> Please either reduce the aero timestep in\n'
                           '> the "settings" block or make the tunnel\n'
                           '> longer.\n'
                           '> If you must keep the tunnel that length,\n'
                           '> a suitable timestep would be '
                           + min_tstep + ' seconds\n'
                           '> or less.\n'
                           '> If you want to keep the same timestep, a\n'
                           '> suitable tunnel length would be '
                           + min_len + '.\n'
                           '> Please edit the file to correct one or\n'
                           '> the other.'
                          )
                    gen.WriteError(2086, err, log)
                    gen.ErrorOnTwoLines(line1_num, line1_text,
                                        line2_num, line2_text, log)
                else:
                    # We complain about two unnamed features being
                    # too close together.
                    err = ('> In the file named "' + file_name + '"\n'
                           '> tunnel "' + tunnel_name
                             + '" had two entities\n'
                           '> that were too close together to comfortably\n'
                           '> calculate with (they are '
                                + act_len + ' apart, at\n'
                           '> ' + ch1 + ' and ' + ch2 + ' respectively.\n'
                           '> Please either reduce the aero timestep in\n'
                           '> the "settings" block or move the features\n'
                           '> farther apart.\n'
                           '> If you must keep that spacing, a suitable\n'
                           '> timestep would be '
                           + min_tstep + ' seconds or less.\n'
                           '> If you want to keep the same timestep, a\n'
                           '> suitable spacing would be '
                           + min_len + ' or more.\n'
                           '> Please edit the file to correct one or\n'
                           '> the other.'
                          )
                    gen.WriteError(2087, err, log)
                    gen.ErrorOnTwoLines(line1_num, line1_text,
                                        line2_num, line2_text, log)
                # raise()
                return(None)

            # If we get to here we have a valid segment length for the
            # timestep we chose.
            # Figure out how many elements we need.  We use math.floor
            # to make sure that the cells are longer than or equal to
            # dx.
            cells = math.floor(length / dx)
            dx_local = length / cells
            # Get the ratio of timestep to grid length, we use this
            # a lot when interpolating for values at the base of the
            # characteristics.
            dtdx = dt/dx_local
            gen.WriteOut('        Count of cells: ' + str(cells) + '\n' +
                         '           Cell length: '
                          + gen.FloatText(round(dx_local, 4)) + ' m\n' +
                         '   Count of gridpoints: ' + str(cells + 1),
                         log)

            # Check if the forward end of the segment is a fan or damper
            # and write the segment-specific fan data if it is.
            if fwd_type == "f":
                fan_descrip = tunnelfans_dict[fwd_value][-4]
                for line in fan_descrip:
                    gen.WriteOut(line, log)
            elif fwd_type == "d":
                damper_descrip = timedlosses_dict[fwd_value][-4]
                for line in damper_descrip:
                    gen.WriteOut(line, log)

            # Add a blank line after the segment definition.
            gen.WriteOut("", log)

            # Only c_f and v will vary (this may not be true when we
            # get to handling train ends).  But we can pre-calculate a
            # friction term for the constant stuff.
            fric_const = 0.5 * perim / area * dt

            # Set the initial celerities to c_atm.
            gridpoints = cells + 1
            cels.append(np.array([c_atm] * gridpoints))
            # Set the initial velocities at the gridpoints to 0 m/s.
            vels.append(np.array([0.0] * gridpoints))

            # Save the details of where this segment's gridpoints are.
            if fwd_dist > back_dist:
                finish = fwd_dist + 0.5 * dx_local
                dists.append(np.arange(back_dist, finish, dx_local))
            else:
                finish = fwd_dist - 0.5 * dx_local
                dists.append(np.arange(back_dist, finish, -dx_local))
            if debug1:
                print("Segment gridpoints:", tun_key, dists[-1])

            # Create something to hold the calculated properties of
            # the segment.  We do this here because we would otherwise
            # have to calculate them many times inside the main loop.
            if tun_key in navigables:
                navigable = True
            else:
                navigable = False

            segments_consts.append((area, perim, d_h, roughness, rr, rr/3.7,
                                    zeta_back_bf, zeta_back_fb,
                                    zeta_fwd_bf, zeta_fwd_fb,
                                    back_type, back_value,
                                    fwd_type, fwd_value,
                                    fric_const, dtdx, dx_local, gridpoints,
                                    gridpoints - 1, navigable))

        # Now that we've processed all the entities along the length of
        # the tunnel, save the list of segments in this tunnel and their
        # distances in the tunnel.
        # We added a segment number to the list for every entity
        # including the forward end, so we need to knock off the last
        # entry in tun_segs.
        #
        tun_segs.pop()
        tuns2segs.__setitem__(tun_key, {"segs": tun_segs, "dists": end_dists} )
        # Add the tunnel name to the list of tunnel names until we've
        # filled up the count of segments in this tunnel.
        segs2tuns.extend([tun_key]*len(tun_segs))

    # Go through the dictionary of the joins and add the geometric data
    # at the ends connected to the joins.  We make a list of all the nodes
    # that have only one tunnel connected to them and fault after we finish.
    lonely_nodes = []
    if len(joins_dict) == 0:
        gen.WriteOut('There are no joins in this file.\n', log)
    else:
        gen.WriteOut('Generating the lists of joins.\n', log)
    for key in joins_dict:
        legs = joins_dict[key]
        gen.WriteOut('\nSeg_IDs connected at join "' + key
                       + '": ' + str(legs),log)
        leg_data = []
        for seg_ID in legs:
            properties = JoinProperty(segments_consts, tunnelfans_dict,
                                      dampertiming_dict, psi, seg_ID)
            # "properties" starts with a list of the following:
            #   [back_end, area, d_h, roughness, rr, rr/3.7, fric_const,
            #    dtdx, stag_away, stag_twd, zeta_away, zeta_twd, seg_ID,
            #    seg_index]
            #
            # If there is a fan at the join, it will continue with the
            # letter "f" and a list of the features needed to calculate
            # the fan.
            #
            # If there is a variable loss at the join, it will continue
            # with the letter "d" and a list of the features needed to
            # calculate it, which could be:
            #  * dampers - lists of stagnation losses for +ve and -ve flow.
            #    These are the same length as the list of calculation
            #    times.
            #  * PSDs & tunnel doors - times, stagnation losses & triggers.
            #  * flap dampers - pressure difference vs. stagnation losses.
            #
            leg_data.append(properties)
            if seg_ID < 0:
                gen.WriteOut('  * Back end of seg_ID ' + str(-seg_ID)
                             + ' attached, details:', log)
            else:
                gen.WriteOut('  * Forward end of seg_ID ' + str(seg_ID)
                             + ' attached, details:', log)

            # if properties[0]:
            #     gen.WriteOut('  * Back end of ' + str(-seg_ID) + ' attached,    area: '
            #                  + str(properties[1]) + ' m^2', log)
            # else:
            #     gen.WriteOut('  * Forward end ' + str(seg_ID) + ' attached, area: '
            #                  + str(properties[1]) + ' m^2', log)
            stag_away = gen.FloatText(properties[8])
            stag_twd = gen.FloatText(properties[9])

            # gen.WriteOut('  * Back end of ' + str(-leg) + ' attached,    area: '
            #                  + str(properties[1]) + ' m^2', log)
            gen.WriteOut(' '*26 + 'Area: ' + str(round(properties[1],4))
                          + ' m^2\n' +
                         ' '*27 + 'D_h: ' + str(round(properties[2],4))
                          + ' m\n' +
                         ' '*9 + 'Roughness height/-c_f: '
                          + str(properties[3]), log)

            if properties[0]:
                gen.WriteOut(' '*18  + 'Zeta towards: ' + str(properties[11])
                              + '    (bf)\n'
                            + ' '*21 + 'Zeta away: ' + str(properties[10])
                              + '    (fb)\n'
                            + ' '*7  + 'Stagnation term towards: '
                              + stag_twd + '\n'
                            + ' '*10 + 'Stagnation term away: ' + stag_away, log)
            else:
                pass
                gen.WriteOut(' '*18  + 'Zeta towards: ' + str(properties[11])
                              + '    (fb)\n'
                           + ' '*21 + 'Zeta away: ' + str(properties[10])
                              + '    (bf)\n'
                           + ' '*7  + 'Stagnation term towards: ' + stag_twd + '\n'
                           + ' '*10 + 'Stagnation term away: ' + stag_away, log)
        joins_dict.__setitem__(key, leg_data)

        # Now check that there are at least two branches at each node/join.
        # Joins will always have at least two branches (because joins are
        # socketed into the side of a tunnel partway along its length) but
        # nodes are at tunnel ends and could have only one connection.
        # When we do this check we make a list of all the entries that
        # only had one connection and put them all in the error message,
        # because the most likely cause of lonely nodes is that one of
        # the entries for a node name is mis-spelled.
        if len(leg_data) == 1:
            lonely_nodes.append(key)

    # We've processed all the nodes, now fault if any of them only have
    # one tunnel attached to them.
    if lonely_nodes != []:
        # Figure out the names of the tunnels that have a lonely node
        # at one end.  We make a list of the tunnel names so we can pick
        # out the relevant lines of input to complain about.
        tunnel_names = []
        tr_indices = []
        for join_name in lonely_nodes:
            seg_index = abs(joins_dict[join_name][0][12]) - 1
            tunnel_name = segment_source[seg_index]
            tunnel_names.append(tunnel_name)
            # Now get the definition of this tunnel and figure out
            # if the node was set at the back end or the forward end.
            # Get the tr_index of the relevant line of input.
            tunnel_defn = tunnels_dict[tunnel_name]
            for keyword in ("back", "fwd"):
                line_defn = tunnel_defn[keyword]
                if line_defn[1] == "node" and line_defn[2].lower() == join_name:
                    tr_indices.append(line_defn[-1])
        # Now check how many we have and choose an appropriate message.
        if len(lonely_nodes) == 1:
            err = ('> In the file named "' + file_name + '",    \n'
                   '> the tunnel named "' + tunnel_names[0]
                     + '" had a node called \n'
                   '> "' + lonely_nodes[0]
                     + '" at one end that no other tunnels were\n'
                   '> connected to.  Nodes must have between two and\n'
                   '> six tunnels connected to them.\n'
                   '> Please edit the file to correct this, either by\n'
                   '> correcting the node name, changing the end to a\n'
                   '> portal or connecting at least two tunnel ends to\n'
                   '> the node.'
                  )
            gen.WriteError(2088, err, log)
            gen.ErrorOnLine2(tr_indices[0], line_triples, log, False)
            return(None)
        else:
            # We make a generic error message suitable for any number
            # of nodes.  It gives a list of all the lonely nodes.
            err = ('> In the file named "' + file_name + '",    \n'
                   '> there were ' + str(len(lonely_nodes))
                     + ' tunnels that ended in nodes that\n'
                   '> no other tunnels were connected to.  Nodes must\n'
                   '> have between two and six tunnels connected to\n'
                   '> them. These are the names of those nodes:\n'
                     + gen.FormatOnLines(lonely_nodes) + '\n'
                   '> Here is a list of the lines that defined them:\n')
            for tr_index in tr_indices:
                (line_number, line_data, line_text) = line_triples[tr_index]
                err = (err + '>   ' + gen.Enth(line_number) + ' line:   '
                       + line_text.strip() + '\n')
            err = err + ('> Please edit the file to correct this, either by\n'
                         '> correcting the node names, changing the ends to\n'
                         '> portals or connecting at least two tunnel ends to\n'
                         '> the nodes.')
            gen.WriteError(2089, err, log)
            return(None)

    if False:
        for name, calc_dens in calc_traffic.items():
            print("calc_traffic\n", name,
                  [round(dens, 4) for dens in calc_dens[0]])
            for dens in calc_dens[1:]:
                print(' ' * len(name), [round(dens, 4) for dens in dens])

    # Now make the Python and Fortran lists for the vehicle drag
    # calculation.
    TR_drag, TR_mess = TrafficDragArrays(vehicles_dict, vehcalc_dict,
                                         calc_traffic, tuns2segs, segs2tuns,
                                         segments_consts, dists, debug1, log)

    # Now make the lists for the jet fan thrust calculation.  This
    # returns None if an active jet fan outlet is exactly on the
    # boundary of a segment (because it's unclear which side of the
    # boundary to take the air velocity that derates the jet fan
    # thrust from).
    result = JetFanDragArrays(t_JF_dict, tuns2segs, segs2tuns,
                              segments_consts, dists, TR_mess,
                              settings_dict, line_triples, log)
    if result is None:
        return(None)
    else:
        (JF_drag, JF_outlets, JFTR_mess) = result
    # Write the segment-based data for the traffic and jet fans to the
    # screen and to the log file.
    header_written = False
    for messages in JFTR_mess:
        if messages != []:
            if not header_written:
                mess = "Segment-based traffic drag and jet fan thrust terms:"
                gen.WriteMessage2(mess, log)
                header_written = True
            for message in messages:
                gen.WriteMessage2(message, log)

    # List of lists of the segments in each route.
    route_segs = []
    # Now make two lists of lists of numbers that are used to convert the
    # chainage of a train end into the distances along the segments.
    # The offset turns the chainage of the up end of a segment in a route
    # to into tunnel distance (this could be of the back end or the down
    # end).
    # If a segment's multiplier is +1 the segment has its back end at the
    # up end in the route.
    # If a segment's multiplier is -1 the segment has its forward end at the
    # up end in the route.
    route_offsets = []
    route_mults = []   # These will be either +1 or -1.

    # This is a list of the route chainages of each end of each segment
    # in the route.  It is used to figure out which segment a train end
    # is in, or if the train end is outside the tunnels. It has one more
    # entry than route_mults and route_offsets because it has the chainage
    # of both portals in it.
    route_seg_chs = []

    # This is a list of segment chainages, for checking.
    route_tun_chs = []

    # Build lists that hold sub-lists of four important chainages in each
    # route: the origin, the end of the route, the chainage of the up
    # portal and the chainage of the down portal.
    route_main_chs = []

    # Build a dictionary that holds the above four lists.  The dictionary
    # is stored in the binary file and is used in classHobyah.py.  But the
    # lists above are used during the calculation (it's faster).
    route2segs_dict = {}

    for r_index, (route_name, route_dict) in enumerate(routes_dict.items()):
        # Figure out which segments are in this route, which order they
        # are in and what route chainage the up end of each segment is.
        tunnels = route_dict["signed_names"]
        up_rte_ch = route_dict["portal"][0]
        # Add a list to the lists of segments, offsets, multipliers and
        # chainages.
        loc_segs = []
        loc_offsets = []
        loc_mults = []
        loc_chs = [up_rte_ch]
        for tun_name in tunnels:
            if tun_name[0] == "-":
                # The forward end of the tunnel is the up end in the
                # route (the tunnel is reversed in the route).
                segs = tuns2segs[tun_name[1:]]["segs"].copy()
                segs.reverse()
                tun_dict = tunnels_dict[tun_name[1:]]
                up_tun_ch = tun_dict["fwd"][0]
                down_tun_ch = tun_dict["back"][0]
            else:
                # The back end of the tunnel is the up end in the route,
                # (the tunnel is the way we expect it to be).
                segs = tuns2segs[tun_name]["segs"]
                tun_dict = tunnels_dict[tun_name]
                up_tun_ch = tun_dict["back"][0]
                down_tun_ch = tun_dict["fwd"][0]
            # Check to see if we need to use a +ve or -ve multiplier to
            # turn a route chainage into a tunnel distance for the segments
            # in this tunnel.
            # A -ve multiplier ends up being used in two situations:
            #  1) A tunnel that is reversed in the route and has its back
            #     chainage below its forward chainage.
            #  2) A tunnel that is not reversed in the route and has its back
            #     chainage above its forward chainage.
            if up_tun_ch > down_tun_ch:
                seg_mult = -1
            else:
                seg_mult = +1

            # Add two tuples that let us convert segment chainages into
            # route chainages.  These are the segment multiplier and
            # an offset.  They are used as follows:
            #
            #   route_ch =   seg_ch * seg_mult - offset
            #
            #     seg_ch = route_ch * seg_mult + offset
            #
            # The algebra here looks a little odd.  That's because
            # seg_mult is only ever +1 or -1, and multiplying by +1 or -1
            # has the same effect as dividing by +1 or -1.
            #
            seg_count = len(segs)
            loc_mults.extend((seg_mult,)*seg_count)

            # Figure out the offset to apply.  We only need to do this
            # for the up end of the first segment in the tunnel, because
            # the offset is the same for all the other route chainages
            # in the segments of this tunnel.
            # print("HH1", tun_name, up_rte_ch, seg_mult, up_tun_ch)
            offset = seg_mult * up_tun_ch - up_rte_ch
            loc_offsets.extend((offset,)*seg_count)

            for seg_ID in segs:
                # Build a list of route chainages for the up and down
                # ends of the segments (up and down in the route, which
                # may be back or forward in the tunnels).  This is really
                # confusing.
                (back_ch, fwd_ch) = seg_lengths[seg_ID]

                up_rte_ch += abs(back_ch - fwd_ch)
                loc_chs.append(up_rte_ch)
                loc_segs.append(seg_ID)
        # Turn the lists into tuples so they can be accessed faster during
        # the calculation.
        loc_segs = tuple(loc_segs)
        loc_offsets = tuple(loc_offsets)
        loc_mults = tuple(loc_mults)
        loc_chs = tuple(loc_chs)

        # Add the tuples to lists of tuples that we use during the
        # calculation.
        route_segs.append(loc_segs)
        route_offsets.append(loc_offsets)
        route_mults.append(loc_mults)
        route_seg_chs.append(loc_chs)

        # Add a tuple of four important chainages for the trains.  This
        # lets us avoid accessing dictionaries at every timestep.
        origin = route_dict["origin"][0]
        runout = route_dict["elevgrad_chs"][-1]
        route_main_chs.append((origin, loc_chs[0], loc_chs[-1], runout))
        # Create a dictionary with sub-dictionaries.  We can use this in
        # classHobyah.py to insert conditions at train ends into the
        # correct segments when plotting profiles.  Note that we could
        # not create this in ProcessRoute because the segments had
        # not been assigned then.
        frames = tuple([seg - 1 for seg in loc_segs])
        route2segs_dict.__setitem__(route_name, {"segs": frames,
                                                 "mults": loc_mults,
                                                 "offsets": loc_offsets,
                                                 "seg_chs": loc_chs}
                                   )

    # Set a value that we use to assess convergence.  If successive
    # approximations of c_N differ by less than this, we treat it as
    # converged.  0.000001 m/s is a difference in absolute static
    # pressure of approximately 0.0006 Pa (at 1 atm).
    converge = 0.000001 # m/s of celerity
    # Set a limit on the count of iterations in the MoC calculations.
    max_iter = 100

    # Get the time over which fixed values of air velocity/volume flow
    # ramp up.
    rise_time = settings_dict["rise_time"]
    # Create lists to hold the values at the print timesteps, and
    # put in the values at zero seconds.
    c_plot = [cels]
    v_plot = [vels]

    # Check if it is worthwhile trying to calculate in the individual
    # segments in parallel.
    # serial = settings_dict["serial"]
    # coresavailable = multiprocessing.cpu_count()
    # if serial:
        # We have forced the run to be on one core, we can use them
        # all here.
        # corestouse = coresavailable
    # else:
        # We could be running multiple files in parallel in main().
        # We may be able to use some cores in the calculation, depending
        # on how many input files were passed.
        # print(type(1), type(divmod(coresavailable, file_count)[0]))
        # corestouse, discard = max(1, divmod(coresavailable, file_count)[0])
        # print(type(1), type(divmod(coresavailable, file_count)[0]))
        # file_count = settings_dict["file_count"]
        # corestouse = max(1, divmod(coresavailable, settings_dict["file_count"])[0])

    # Run them all in parallel, using as many cores as are available.
    # my_pool = multiprocessing.Pool(processes = corestouse)
    # my_pool.map(ProcessFile,runargs)


    # Check if scipy is installed and fault if it is not.
    try:
        import scipy.optimize
    except ModuleNotFoundError:
        err = ("> Ugh, can't process this run because Python's\n"
               '> scipy module is not installed.\n'
               '> If you are fortunate enough to have an IT\n'
               '> department, please ask them to install it for\n'
               '> you then try again.  If you do not have an IT\n'
               '> department and have to do it for yourself, good\n'
               '> luck!\n'
               '> Note that there are other programs you may\n'
               '> want to install: gnuplot and ImageMagick.\n'
               '> See section 1.11 of the Hobyah User Guide for\n'
               '> more details.'
              )
        gen.WriteError(2090, err, log)
        return(None)

    # Build a list of lists of locators, like "# westbound@1200.0"
    # giving the name of the tunnel and the distance of each gridpoint
    # in that tunnel.  This is just to save us from building them
    # from scratch every time in classHobyah.
    locators = []
    for seg_index, dists2 in enumerate(dists):
        name = segs2tuns[seg_index + 1]
        sub_list = ["# " + name + " @ " + gen.RoundText(dist, 3)
                             for dist in dists2]
        locators.append(sub_list)

    # Figure out how many decimal places to round run times to in
    # error messages issued during the calculation.
    dt_text = str(dt)
    if '.' not in dt_text:
        # I'm not expecting to have timesteps greater than 0.1 seconds
        # but someone might modify the code for that some day and if
        # this if clause wasn't present they'll get an odd error.
        time_round = 1
    else:
        time_round = len(str(dt).split(sep = '.')[1])

    # Create a variable to catch when calculations don't converge.
    # This is increased whenever scipy.optimize.fsolve raises a warning
    # or the final values of c_N and c_N_old do not match in OpenEnd,
    # FixedVel or any of the CelVel1 calculations.
    dud_calc = 0
    # The main loop of the calculation.  Loop over each time.
    print("> Starting a method of characteristics calculation.")
    for index, time in enumerate(time_list):
        # print("Time", time)
        # Create empty lists to hold the velocities and celerities
        # at the next timestep.  Once we have built these (and
        # optionally written them to the binary file) we overwrite
        # 'cels' and 'vels' with them and use them as the initial
        # conditions for the next timestep.
        cels_newtime = []
        vels_newtime = []

        # Figure out if this time is later than the rise time we set
        # for fixed velocity/volume flow values.
        if time >= rise_time:
            vel_frac = 1.0
        else:
            vel_frac = time / rise_time
        if index == 0:
            last_time = 0.0
        else:
            last_time = time_list[index - 1]

        # Figure out the air velocities to use in the jet fan thrust
        # calculations in this timestep.
        # Calculate the traffic drag in each cell using:
        #  * the tunnel air velocity next to the fan outlet at the
        #    previous timestep,
        #  * the jet velocity multiplied by the fan rotational speed
        #    at the current time, and
        #  * the jet fan static thrust multiplied by the square of
        #    the fan rotational speed at the current time.
        # Likewise, calculate the traffic drag in each cell, using the
        # mean of the velocity at the gridpoints at each end of the cell
        # at the previous timestep.
        # Each of these returns a list of np.arrays (one array for
        # each segment in the simulation, each array containing one
        # drag/thrust value for each cell in that segment).  The values
        # are expressed as resistances in m^2/s^2 per metre length of
        # the cell and they can be added to the body force term 'E' in
        # the characteristic equations.
        #
        # One criticism of the above that is easy to raise and hard
        # to dismiss is as follows: the velocity values at the current
        # timestep should be used, not those at the previous timestep.
        # With short timesteps and low impelling forces (which we would
        # expect to find in tunnel ventilation systems) the velocity
        # values at the previous timestep are acceptable.
        #
        JFTR_array = TRTimeAdjust(TR_drag, dt, last_time, dists, vels, have_ftn)
        # Get the jet fan thrust terms and add them to the traffic
        # drag terms.
        jetfanthrusts = JFTimeAdjust(JF_drag, dt, last_time, vels)
        for seg_index in range(len(JFTR_array)):
            # print(seg_index, JFTR_array[seg_index])
            # print(jetfanthrusts[seg_index])
            JFTR_array[seg_index] += jetfanthrusts[seg_index]

        if False:
            for seg_index in range(len(JFTR_array)):
                print("Traffic drags & jet fan thrusts per cell:")
                for seg_index, drag in enumerate(JFTR_array):
                    print(" ", seg_index + 1, drag)

        # Loop over each segment and get the new values at all gridpoints
        # that are not at entities like fans, dampers and junctions.
        for seg_index, seg_constants in enumerate(segments_consts):
            # Get the values for celerity and velocity at the current
            # timestep, starting from the values at the previous timestep.

            # Gridpoints whose values must be calculated in conjunction
            # with values in the cells of other segments are not covered
            # in this loop.  Those gridpoints are given NaN values for
            # celerity and velocity here, which are overwritten when
            # we loop over all the boundaries.

            JFTR_drag = JFTR_array[seg_index]

            if have_ftn:
                # Unpack the segment constants for the call to the Fortran
                # version of CelVel1.
                (area, perim, d_h, roughness, rr, rr37,
                 zeta_back_bf, zeta_back_fb,
                 zeta_fwd_bf, zeta_fwd_fb,
                 back_type, back_value, fwd_type, fwd_value,
                 fric_const, dtdx, dx_local, gridpoints,
                 end_test, navigable) = seg_constants

                # Check the values assigned to the back and forward ends
                # of the segments.  These could be numbers (pressure,
                # velocity) but they could also be the names of nodes,
                # fan data or damper data.
                # I don't know how to get Fortran to handle polymorphic
                # variables (and I can't be bothered to learn).  So we
                # put in a spoof number for those cases when we call the
                # Fortran version of CelVel1.
                if back_type == "n":
                    back_value_alt = 1.0
                else:
                    back_value_alt = back_value
                if fwd_type in ("n", "f", "d"):
                    fwd_value_alt = 1.0
                else:
                    fwd_value_alt = fwd_value
                (c_new, u_new, success) = ftn.celvel1(
                                       cels[seg_index], vels[seg_index],
                                       d_h, roughness, rr, rr37,
                                       zeta_back_bf, zeta_back_fb,
                                       zeta_fwd_bf, zeta_fwd_fb,
                                       back_type, back_value_alt,
                                       fwd_type, fwd_value_alt,
                                       fric_const, dtdx,
                                       converge, max_iter,
                                       fric_app_num, last_time, psi,
                                       vel_frac, JFTR_drag, gridpoints,
                                       end_test)
                # Turn the fixed-length Fortran bytes result into a
                # string that we can print.
                success = bytes.decode(success)
            else:
                # Use a solver routine that handles traffic drag and
                # jet fans.
                (c_new, u_new, success) = CelVel1(
                                       cels[seg_index], vels[seg_index],
                                       seg_constants, converge, max_iter,
                                       fric_app_num, psi, debug1,
                                       last_time, vel_frac, JFTR_drag)

            if success.lstrip() != "The solution converged.":
                dud_calc = NotConverged(dud_calc, success, seg_index, time, log)

            # Once we get to here we have processed all the gridpoints in
            # one segment at this timestep.  Add the entries to the lists
            # of values that we will use in the next timestep.
            cels_newtime.append(c_new)
            vels_newtime.append(u_new)


        # Now handle all the gridpoints at junctions, area changes and
        # the like.  We take the values at two gridpoints nearest to the
        # junction in all the cells connected to it.
        for join in joins_dict:
            connections = joins_dict[join]
            if len(connections) == 2:
                # This node only has two branches connecting to it.  It
                # may be an area change with losses, it may be an axial
                # or centrifugal fan part-way along a tunnel, it may be
                # a loss whose resistance changes with time (like a
                # damper).
                # Get what we need to solve the values at the next
                # time step.
                (left_props, right_props) = connections
                # Get the segment IDs and segment indices.  The segment
                # IDs are +ve if the forward end is attached and -ve if
                # the back end is attached and are offset by one from
                # the indices of the segments in cels, vels etc.
                seg_ID1, seg_index1 = left_props[12:14]
                seg_ID2, seg_index2 = right_props[12:14]

                # Get the values of celerity, velocity, traffic drag
                # and jet fan thrust at the gridpoints at the ends
                # of the cells attached to the join.
                left_vals = TwoPoints(cels, vels, JFTR_array,
                                      seg_ID1, seg_index1)
                right_vals = TwoPoints(cels, vels, JFTR_array,
                                       seg_ID2, seg_index2)

                if len(left_props) == 14:
                    # We have a simple feature here, like a join or node.
                    result = CelVel2(left_vals, right_vals,
                                     left_props, right_props, time,
                                     fric_app_num, psi, debug1, have_ftn)
                else:
                    # We have a complex feature here, like a fan, a
                    # damper or a tunnel door.  The feature has
                    # transient properties, which we need to handle
                    # properly.
                    if left_props[14] == "f":
                        # We have a fan at this point.
                        result = CelVelFan(left_vals, right_vals,
                                           left_props, right_props, time,
                                           fric_app_num, psi, debug1, have_ftn)
                    elif left_props[14] == "d":
                        # We have damper (a resistance that varies with time)
                        # at this location.  Spoof the properties with the
                        # precalculated values of the damper resistance
                        # at the current timestep and call the routine
                        # that handles two-way junctions.
                        left_mod = left_props[:8]                \
                                      + [left_props[16][index]]  \
                                      + [left_props[15][index]]  \
                                      + left_props[10:15]         \
                                      + [left_props[17]]
                        # print("Setting damper", left_props[16],
                        #       "at time", gen.FloatText(time),
                        #        gen.FloatText(left_mod[7]),
                        #        gen.FloatText(left_mod[8]),
                        #        gen.FloatText(left_props[14][index]),
                        #        gen.FloatText(left_props[15][index]))
                        result = CelVel2(left_vals, right_vals,
                                         left_mod, right_props, time,
                                         fric_app_num, psi, debug1, have_ftn)
                    else:
                        print("Need to add code to handle a transient feature\n"
                              'in PROC ProcessCalc.  The feature letter that\n'
                              'needs to be handled is "' + left_props[14] + '".')
                        sys.exit()
                if result is None:
                    return(result)
                else:
                    (c_N1, u_N1, c_N2, u_N2, success) = result
                    if success != "The solution converged.":
                        # Complain in the log file and to the screen.
                        dud_calc += 1
                        print(success)
                        gen.WriteOut(success, log)

                # Now overwrite the NaN values in the celerity and
                # velocity arrays with the values calculated from
                # the conditions in the segments at the join.
                if seg_ID1 > 0:
                    # The forward end of the first segment is at the join.
                    # This is what we expect at area changes.
                    cels_newtime[seg_index1][-1] = c_N1
                    vels_newtime[seg_index1][-1] = u_N1
                else:
                    # The back end of the first segment is at the join.
                    cels_newtime[seg_index1][0] = c_N1
                    vels_newtime[seg_index1][0] = u_N1

                if seg_ID2 > 0:
                    cels_newtime[seg_index2][-1] = c_N2
                    vels_newtime[seg_index2][-1] = u_N2
                else:
                    cels_newtime[seg_index2][0] = c_N2
                    vels_newtime[seg_index2][0] = u_N2

            elif len(connections) == 3:
                # This join has three branches connecting to it.  Get
                # what we need to solve the values at the next time step.
                (props1, props2, props3) = connections

                seg_ID1, seg_index1 = props1[12:14]
                seg_ID2, seg_index2 = props2[12:14]
                seg_ID3, seg_index3 = props3[12:14]

                vals1 = TwoPoints(cels, vels, JFTR_array, seg_ID1, seg_index1)
                vals2 = TwoPoints(cels, vels, JFTR_array, seg_ID2, seg_index2)
                vals3 = TwoPoints(cels, vels, JFTR_array, seg_ID3, seg_index3)

                result = CelVel3(vals1, vals2, vals3, props1, props2, props3,
                                 fric_app_num, psi, debug1, have_ftn)
                if result is None:
                    return(result)
                else:
                    (c_N1, u_N1, c_N2, u_N2, c_N3, u_N3) = result
                if seg_ID1 > 0:
                    cels_newtime[seg_index1][-1] = c_N1
                    vels_newtime[seg_index1][-1] = u_N1
                else:
                    cels_newtime[seg_index1][0] = c_N1
                    vels_newtime[seg_index1][0] = u_N1

                if seg_ID2 > 0:
                    cels_newtime[seg_index2][-1] = c_N2
                    vels_newtime[seg_index2][-1] = u_N2
                else:
                    cels_newtime[seg_index2][0] = c_N2
                    vels_newtime[seg_index2][0] = u_N2

                if seg_ID3 > 0:
                    cels_newtime[seg_index3][-1] = c_N3
                    vels_newtime[seg_index3][-1] = u_N3
                else:
                    cels_newtime[seg_index3][0] = c_N3
                    vels_newtime[seg_index3][0] = u_N3
            elif len(connections) == 4:
                # This join has four branches connecting to it.  Get
                # what we need to solve the values at the next time step.
                (props1, props2, props3, props4) = connections

                seg_ID1, seg_index1 = props1[12:14]
                seg_ID2, seg_index2 = props2[12:14]
                seg_ID3, seg_index3 = props3[12:14]
                seg_ID4, seg_index4 = props4[12:14]

                vals1 = TwoPoints(cels, vels, JFTR_array, seg_ID1, seg_index1)
                vals2 = TwoPoints(cels, vels, JFTR_array, seg_ID2, seg_index2)
                vals3 = TwoPoints(cels, vels, JFTR_array, seg_ID3, seg_index3)
                vals4 = TwoPoints(cels, vels, JFTR_array, seg_ID4, seg_index4)

                result = CelVel4(vals1, vals2, vals3, vals4,
                                 props1, props2, props3, props4,
                                 fric_app_num, psi, debug1, have_ftn)
                if result is None:
                    return(result)
                else:
                    (c_N1, u_N1, c_N2, u_N2, c_N3, u_N3, c_N4, u_N4) = result
                if seg_ID1 > 0:
                    cels_newtime[seg_index1][-1] = c_N1
                    vels_newtime[seg_index1][-1] = u_N1
                else:
                    cels_newtime[seg_index1][0] = c_N1
                    vels_newtime[seg_index1][0] = u_N1

                if seg_ID2 > 0:
                    cels_newtime[seg_index2][-1] = c_N2
                    vels_newtime[seg_index2][-1] = u_N2
                else:
                    cels_newtime[seg_index2][0] = c_N2
                    vels_newtime[seg_index2][0] = u_N2

                if seg_ID3 > 0:
                    cels_newtime[seg_index3][-1] = c_N3
                    vels_newtime[seg_index3][-1] = u_N3
                else:
                    cels_newtime[seg_index3][0] = c_N3
                    vels_newtime[seg_index3][0] = u_N3

                if seg_ID4 > 0:
                    cels_newtime[seg_index4][-1] = c_N4
                    vels_newtime[seg_index4][-1] = u_N4
                else:
                    cels_newtime[seg_index4][0] = c_N4
                    vels_newtime[seg_index4][0] = u_N4
            elif len(connections) == 5:
                # This join has five branches connecting to it.  Get
                # what we need to solve the values at the next time step.
                (props1, props2, props3, props4, props5) = connections

                seg_ID1, seg_index1 = props1[12:14]
                seg_ID2, seg_index2 = props2[12:14]
                seg_ID3, seg_index3 = props3[12:14]
                seg_ID4, seg_index4 = props4[12:14]
                seg_ID5, seg_index5 = props5[12:14]

                vals1 = TwoPoints(cels, vels, JFTR_array, seg_ID1, seg_index1)
                vals2 = TwoPoints(cels, vels, JFTR_array, seg_ID2, seg_index2)
                vals3 = TwoPoints(cels, vels, JFTR_array, seg_ID3, seg_index3)
                vals4 = TwoPoints(cels, vels, JFTR_array, seg_ID4, seg_index4)
                vals5 = TwoPoints(cels, vels, JFTR_array, seg_ID5, seg_index5)

                result = CelVel5(vals1, vals2, vals3, vals4, vals5,
                                 props1, props2, props3, props4, props5,
                                 fric_app_num, psi, debug1, have_ftn)
                if result is None:
                    return(result)
                else:
                    (c_N1, u_N1, c_N2, u_N2, c_N3, u_N3,
                                 c_N4, u_N4, c_N5, u_N5) = result
                if seg_ID1 > 0:
                    cels_newtime[seg_index1][-1] = c_N1
                    vels_newtime[seg_index1][-1] = u_N1
                else:
                    cels_newtime[seg_index1][0] = c_N1
                    vels_newtime[seg_index1][0] = u_N1

                if seg_ID2 > 0:
                    cels_newtime[seg_index2][-1] = c_N2
                    vels_newtime[seg_index2][-1] = u_N2
                else:
                    cels_newtime[seg_index2][0] = c_N2
                    vels_newtime[seg_index2][0] = u_N2

                if seg_ID3 > 0:
                    cels_newtime[seg_index3][-1] = c_N3
                    vels_newtime[seg_index3][-1] = u_N3
                else:
                    cels_newtime[seg_index3][0] = c_N3
                    vels_newtime[seg_index3][0] = u_N3

                if seg_ID4 > 0:
                    cels_newtime[seg_index4][-1] = c_N4
                    vels_newtime[seg_index4][-1] = u_N4
                else:
                    cels_newtime[seg_index4][0] = c_N4
                    vels_newtime[seg_index4][0] = u_N4

                if seg_ID5 > 0:
                    cels_newtime[seg_index5][-1] = c_N5
                    vels_newtime[seg_index5][-1] = u_N5
                else:
                    cels_newtime[seg_index5][0] = c_N5
                    vels_newtime[seg_index5][0] = u_N5
            elif len(connections) == 6:
                # This join has six branches connecting to it.  Get
                # what we need to solve the values at the next time step.
                (props1, props2, props3, props4, props5, props6) = connections

                seg_ID1, seg_index1 = props1[12:14]
                seg_ID2, seg_index2 = props2[12:14]
                seg_ID3, seg_index3 = props3[12:14]
                seg_ID4, seg_index4 = props4[12:14]
                seg_ID5, seg_index5 = props5[12:14]
                seg_ID6, seg_index6 = props6[12:14]

                vals1 = TwoPoints(cels, vels, JFTR_array, seg_ID1, seg_index1)
                vals2 = TwoPoints(cels, vels, JFTR_array, seg_ID2, seg_index2)
                vals3 = TwoPoints(cels, vels, JFTR_array, seg_ID3, seg_index3)
                vals4 = TwoPoints(cels, vels, JFTR_array, seg_ID4, seg_index4)
                vals5 = TwoPoints(cels, vels, JFTR_array, seg_ID5, seg_index5)
                vals6 = TwoPoints(cels, vels, JFTR_array, seg_ID6, seg_index6)

                result = CelVel6(vals1, vals2, vals3, vals4, vals5, vals6,
                                 props1, props2, props3, props4, props5, props6,
                                 fric_app_num, psi, debug1, have_ftn)
                if result is None:
                    return(result)
                else:
                    (c_N1, u_N1, c_N2, u_N2, c_N3, u_N3,
                                 c_N4, u_N4, c_N5, u_N5, c_N6, u_N6) = result
                if seg_ID1 > 0:
                    cels_newtime[seg_index1][-1] = c_N1
                    vels_newtime[seg_index1][-1] = u_N1
                else:
                    cels_newtime[seg_index1][0] = c_N1
                    vels_newtime[seg_index1][0] = u_N1

                if seg_ID2 > 0:
                    cels_newtime[seg_index2][-1] = c_N2
                    vels_newtime[seg_index2][-1] = u_N2
                else:
                    cels_newtime[seg_index2][0] = c_N2
                    vels_newtime[seg_index2][0] = u_N2

                if seg_ID3 > 0:
                    cels_newtime[seg_index3][-1] = c_N3
                    vels_newtime[seg_index3][-1] = u_N3
                else:
                    cels_newtime[seg_index3][0] = c_N3
                    vels_newtime[seg_index3][0] = u_N3

                if seg_ID4 > 0:
                    cels_newtime[seg_index4][-1] = c_N4
                    vels_newtime[seg_index4][-1] = u_N4
                else:
                    cels_newtime[seg_index4][0] = c_N4
                    vels_newtime[seg_index4][0] = u_N4

                if seg_ID5 > 0:
                    cels_newtime[seg_index5][-1] = c_N5
                    vels_newtime[seg_index5][-1] = u_N5
                else:
                    cels_newtime[seg_index5][0] = c_N5
                    vels_newtime[seg_index5][0] = u_N5

                if seg_ID6 > 0:
                    cels_newtime[seg_index6][-1] = c_N6
                    vels_newtime[seg_index6][-1] = u_N6
                else:
                    cels_newtime[seg_index6][0] = c_N6
                    vels_newtime[seg_index6][0] = u_N6

        # We have processed all the values at this timestep.  Check if we
        # need to save the current set of values for plotting.
        if time in aero_times:
            # Save the celerities and velocities in all segments at this
            # timestep.
            c_plot.append(cels_newtime)
            v_plot.append(vels_newtime)

            # tr_speeds.append(np.copy(tr_speed_newtime) * 3.6)  # This is km/h
            # tr_accels.append(np.copy(tr_accel_newtime))        # This is m/s**2
            # tr_down_chs.append(np.copy(tr_down_newtime))
            # tr_up_chs.append(np.copy(tr_up_newtime))

            # This next block prints the state of the system at the last
            # time step.  Changing the Boolean to turn it on or off.
            if False and (not show_errors) and time == aero_times[-1]:
                print("Last plot timestep", str(time)
                      + ".  Values in gridpoints at the ends of cells.")
                for (seg_index, tunnel) in enumerate(segment_source):
                    loc_cels = cels_newtime[seg_index]
                    loc_vels = vels_newtime[seg_index]
                    print('Tunnel "' + tunnel
                           + '" (seg_ID '+str(seg_index + 1)+'):')
                    print("   Distances:", [round(d, 3) for
                                            d in dists[seg_index]])
                    print("  Celerities:", [round(c, 5) for c in loc_cels])
                    print("  Velocities:", [round(v, 5) for b in loc_vels])
                    loc_dens = [gen.GetDensity(cel, c_atm, rho_atm, psi)
                             for cel in loc_cels]
                    p_stat = [rho / gamma * cel**2
                           for rho, cel in zip(loc_dens, loc_cels)]
                    p_dyn = [0.5 * dens[index] * vel**2
                          for rho, vel in zip(loc_dens, loc_vels)]
                    p_tot = [p_s + p_d for p_s, p_d in zip(p_stat, p_dyn)]
                    print("     Density:", [round(d, 7) for d in dens])
                    print("    P_static:", [round(p, 3) for p in p_stat])
                    print("   P_dynamic:", [round(p, 4) for p in p_dyn])
                    print("       P_tot:", [round(p, 3) for p in p_tot])

            gen.WriteOut("Print time: " + gen.FloatText(time) + ' seconds', log)
            old_name = "#unused"
            t_index = 0
            for index, c_vals in enumerate(cels):
                gp_dists = dists[index]
                v_vals = vels[index]
                old_name, t_index = WriteConditions(segment_source, segments_consts,
                                                    gp_dists, c_vals, v_vals,
                                                    index, c_atm, rho_atm,
                                                    psi, gamma, old_name, time,
                                                    t_index, log)
        # Update the arrays that hold the values in the previous timestep.
        cels = cels_newtime
        vels = vels_newtime
        # tr_speed_prev = tr_speed_newtime
        # tr_down_prev = tr_down_newtime
        # tr_up_prev = tr_up_newtime
        # At this point, we loop around to process the next timestep.


    # When we get to here the transient calculation has finished.  Check
    # if any red flags were raised.
    if dud_calc is True:
        # We had some problems with convergence in the calculation.  Either
        # scipy.optimize.fsolve didn't converge, or we had a problem with
        # high pressure loss factors at pressure portals (PROC OpenEnd).
        #
        # Sometimes the stability problems in the calculation are not a
        # problem in the real world, such as when the calculation of pressure
        # rise across a fan doesn't converge when the duty point is crossing
        # the stall hump while the fan is changing speed (see the test file
        # named "ok-031-fan-test.txt", which raises three "didn't converge"
        # error messages when the duty point of fan 2 crosses its stall hump).
        # This is a transient feature (you can just see a wiggle in the air
        # velocity caused by it at 115 seconds in "ok-031-fan-test-lp1.pdf").
        # The calculations at that time are incorrect, but once the duty
        # point moves away from the stall hump the calculation gets back
        # to normal.
        #
        # But sometimes they are a something that must be changed because
        # the calculation is inaccurate.  For example, when there are high
        # pressure losses at portals open to atmosphere, sometimes the
        # MoC calculation hunts as it tries to get to a stable solution.
        # In the calculation "kb-001-portal-zetas.txt", the solver hops
        # back and forwards between two possible solutions at every time
        # step as soon as the air velocity gets high enough.  It writes
        # over 900 red flags to the terminal and to the log file, and
        # the velocity, mass flow and pressure profiles in the file
        # "kb-001-portal-zetas.pdf" are deeply suspect (especially the
        # mass flow profile).
        #
        # So what should you take from this?  Four things:
        #
        # 1) If Hobyah writes a message to the screen saying "The
        #    calculation suffered from stability problems", then you
        #    should not ignore it.
        #
        # 2) You should plot output that you can use to gauge whether
        #    this is a genuine problem.
        #
        # 3) If it turns out that a particular entity in a tunnel wrote
        #    a screed of messages at multiple time steps, then you have
        #    a problem.  The calculation is not going to converge.
        #
        # 4) A good test is to plot mass flows along the segment in which
        #    the calculation did not converge.  If the calculation can't
        #    conserve mass, then it's probably wrong.  Don't go overboard
        #    with this though: pressure waves can cause fluctuations in
        #    mass flow as the cells in high pressure regions have higher
        #    density and store more mass than the same size cells in low
        #    pressure regions.
        pass


    # Write a new binary file that we can associate with the name "calc".
    try:
        bdat = open(dir_name + bin_name, "wb")
    except PermissionError:
        err = ('> Skipping "' + file_name + '", because you\n'
               "> somehow managed to make its binary file unwriteable\n"
               '> while the calculation was running.  Congratulations!')
        gen.WriteError(2083, err, log)
        return(None)
    else:
        # Write the binary file.  We keep this in an else clause so we
        # can close the handle "bdat" at the end.

        # Generate a version number for the binary file.  Each time
        # we modify the format, the binary file version is increased.
        binversion = 7

        # Identify the input file version, in this version 1.
        prog_type = "Hobyah " + str(settings_dict["version"])

        # We want some of the entries in settings_dict, but not others.
        settings_dict2 = copy.deepcopy(settings_dict)
        # Delete some stuff from a copy of settings_dict that we don't
        # need when using the binary file for plotting or creating new Hobyah
        # input files.
        settings_dict2.__delitem__("debug1")
        settings_dict2.__delitem__("script_name")
        settings_dict2.__delitem__("script_date")
        settings_dict2.__delitem__("show_errors")
        settings_dict2.__delitem__("when_who")
        settings_dict2.__delitem__("named")
        settings_dict2.__delitem__("unnamed")
        settings_dict2.__delitem__("reserved")
        settings_dict2.__delitem__("duplicables")
        settings_dict2.__delitem__("vehicles_dict")
        settings_dict2.__delitem__("vehcalc_dict")

        # Create pandas databases of the properties of dampers at the
        # print timesteps.  We have five for type 1 dampers (the user
        # has set the variations of area and two k-factors with time and
        # we calculate Atkinson resistances).
        # Type 2 dampers only have the variation of two Atkinson
        # resistances with time.
        # Build an index of names that we use for the column names.  We
        # use the plot time as the row names.
        dampers1_names = []
        dampers2_names = []
        d_areas = []
        d_zetas_bf = []
        d_zetas_fb = []
        d_Rs_fb = []
        d_Rs_bf = []
        for damper_name in dampertiming_dict:
            d_props = timedlosses_dict[damper_name]
            d_type = d_props[0]
            d_timing = dampertiming_dict[damper_name]
            if d_type == "damper1":
                dampers1_names.append(damper_name)
                dampers2_names.append(damper_name)
                areas, zetas_bf, zetas_fb, Rs_bf, Rs_fb = d_timing[2]
                d_areas.append(areas)
                d_zetas_fb.append(zetas_fb)
                d_zetas_bf.append(zetas_bf)
                d_Rs_bf.append(Rs_bf)
                d_Rs_fb.append(Rs_fb)
            elif d_type == "damper2":
                dampers2_names.append(damper_name)
                Rs_bf, Rs_fb = d_timing[2]
                d_Rs_bf.append(Rs_bf)
                d_Rs_fb.append(Rs_fb)
            else:
                # A new type of damper has been added.
                print('Need to add more code to PROC ProcessCalc to handle "'
                      + keyword + '" for "' + damper_name + '".')
                gen.OopsIDidItAgain(log, file_name)

        # Add three dataframes for each type 1 damper (area, +ve k-factor,
        # -ve k-factor).
        areas_bin = pd.DataFrame(d_areas, columns = aero_times,
                                   index = dampers1_names)
        zetas_bf_bin = pd.DataFrame(d_zetas_bf, columns = aero_times,
                                      index = dampers1_names)
        zetas_fb_bin = pd.DataFrame(d_zetas_fb, columns = aero_times,
                                      index = dampers1_names)
        # Add two dataframes for each type 1 and type 2 damper.  These
        # hold the Atkinson resistances.
        Rs_bf_bin = pd.DataFrame(d_Rs_bf, columns = aero_times,
                                   index = dampers2_names)
        Rs_fb_bin = pd.DataFrame(d_Rs_fb, columns = aero_times,
                                   index = dampers2_names)
        # Transpose the dataframes so that the indices are time and the
        # columns are the name of the damper.
        areas_bin = areas_bin.transpose()
        zetas_bf_bin = zetas_bf_bin.transpose()
        zetas_fb_bin = zetas_fb_bin.transpose()
        Rs_bf_bin = Rs_bf_bin.transpose()
        Rs_fb_bin = Rs_fb_bin.transpose()

        # Build four dataframes for the the train speeds and locations
        # and transpose them so that the index is the print times and
        # the columns are the train number (starting at 1).  The
        # train speeds are stored in the dataframe in km/h.
        # train_nums = range(1, tr_count + 1)
        # tr_speeds_bin = pd.DataFrame(tr_speeds, columns = train_nums,
        #                                index = aero_times)
        # tr_accels_bin = pd.DataFrame(tr_accels, columns = train_nums,
        #                                index = aero_times)
        # tr_dnch_bin = pd.DataFrame(tr_down_chs, columns = train_nums,
        #                                index = aero_times)
        # tr_upch_bin = pd.DataFrame(tr_up_chs, columns = train_nums,
        #                              index = aero_times)

        # Now build lists to let us handle the navigable segments.  We
        # need to know:
        #  * which train boundaries are in which segment.
        #  * what distance each train boundaries are from the segment's back end
        #  * what are the tunnel area, perimeter and friction factor at each
        #    fixed gridpoint in the segments.
        #  * which train boundaries are treated as coinciding with
        #  * what are the tunnel area, perimeter and friction factor on each
        #    side of the train boundaries.
        #
        # We arrange things so that we can plot from the
        nav_areas_array = []
        nav_perims_array = []
        nav_fanning_array = []

        # Write the input data to the binary file.
        if debug1:
            print("Dictionary of Hobyah settings:", settings_dict2)
        # Put a string with the binary file version number as the
        # first entry.  When we read a binary file we check for
        # this string and don't read anything else until we've
        # checked that it is valid.
        pickle.dump( "Hobyah.py binary version " + str(binversion), bdat)

        # Write out some QA and the various input dictionaries.
        when_who = settings_dict["when_who"]
        script_name = settings_dict["script_name"]
        script_date = settings_dict["script_date"]

        # Turn the runtime data into lists containing pandas arrays.  Each
        # pandas array in the list contains the data for celerity or velocity
        # at all the gridpoints in one segment at all print times.  We don't
        # do this as a 3D numpy array because each segment has a different
        # count of gridpoints and it would involve a lot of padding.
        c_bin = []
        v_bin = []
        for (index, loc_dists) in enumerate(dists):
            # Add a dataframe for each segment.
            c_bin.append(pd.DataFrame([c_list[index] for c_list in c_plot],
                                       columns =loc_dists,
                                       index = aero_times)
                        )
            v_bin.append(pd.DataFrame([v_list[index] for v_list in v_plot],
                                       columns =loc_dists,
                                       index = aero_times)
                        )

        fixed = (prog_type,      # Program and file version ("Hobyah 1")
                 when_who,       # QA data (user, date and time)
                 file_name,      # Name of the file (.txt)
                 script_name,    # QA data (name of this script)
                 script_date,    # QA data (date this script was edited)
                 settings_dict2, # This contains a selection of settings.
                 sectypes_dict,
                 tunnels_dict,
                 routes_dict,
                 fanchars_dict,
                 tunnelfans_dict,
                 JFblock_dict,
                 JFcalc_dict,
                 timedlosses_dict,
                 plotcontrol_dict,
                 tuple(aero_times),
                 tuple(segment_source),
                 tuns2segs,
                 tuple(segments_consts),
                 joins_dict,
                 dists,
                 locators,
                 route2segs_dict,
                 vehicles_dict,
                 vehcalc_dict,
                 r_traffic_dict,
                 t_traffic_dict,
                )
        pickle.dump(fixed, bdat)

        if settings_dict["dudbins"]:
            # Generate new binary files to raise error messages in
            # classHobyah.py.  We assume that we have write access
            # to them.
            print("> Generating four new dud binary files.")
            with open("H-7005-version-too-low.hbn", "bw") as b1:
                pickle.dump("Hobyah.py binary version 1", b1)
            with open("H-7006-version-too-high.hbn", "bw") as b1:
                pickle.dump("Hobyah.py binary version 9999", b1)
            with open("H-7022-raise-EOF.hbn", "bw") as b1:
                pickle.dump("Hobyah.py binary version "
                            + str(binversion), b1)
                pickle.dump(fixed, b1)
            with open("H-7024-ends-early.hbn", "bw") as b1:
                pickle.dump("Hobyah.py binary version "
                            + str(binversion), b1)
                pickle.dump(fixed[:5], b1)



        transients = (# Variation of celerity and velocity at fixed
                      # gridpoints.
                      tuple(c_bin),
                      tuple(v_bin),
                      # Variation of five damper properties.
                      areas_bin,
                      zetas_bf_bin,
                      zetas_fb_bin,
                      Rs_bf_bin,    # These are Atkinson resistances, i.e.
                      Rs_fb_bin,    # 0.5 * 1.2 * zeta / (area**2)
                      # Variation of four train intrinsic properties
                      # tr_speeds_bin,
                      # tr_accels_bin,
                      # tr_dnch_bin,
                      # tr_upch_bin,
                     )
        pickle.dump(transients, bdat)
        bdat.close()

    print("> Finished calculating, created a new binary file.")

    return("Finished successfully")


def WriteConditions(segment_source, segments_consts, gp_dists, c_vals, v_vals,
                    index, c_atm, rho_atm, psi, gamma, old_name, time, t_index,
                    log):
    tunnel_name = segment_source[index]
    area = segments_consts[index][0]
    if tunnel_name != old_name:
        # Reset the tunnel index.
        t_index = 1
        old_name = tunnel_name
    else:
        t_index += 1
    tunnel_term = '"' + tunnel_name + '" (' + gen.Enth(t_index)  \
                    + ' segment in the tunnel, seg_ID ' + str(index + 1)  \
                    + ') at ' + str(time) + ' seconds'

    dens = [gen.GetDensity(cel, c_atm, rho_atm, psi) for cel in c_vals]
    p_stat = [dens[index] / gamma * cel**2 for index, cel in enumerate(c_vals)]
    p_dyn = [0.5 * dens[index] * vel**2 for index, vel in enumerate(v_vals)]
    p_tot = [p_dyn[index] + p_s for (index, p_s) in enumerate(p_stat)]

    gen.WriteOut(tunnel_term, log)
    header1 = "  grid-  distance   velocity   P_static   P_dyn    P_total   celerity   density   volume flow"
    header2 = "  point    (m)       (m/s)       (Pa)     (Pa)      (Pa)      (m/s)    (kg/m^3)     (m^3/s)"
    gen.WriteOut(header1, log)
    gen.WriteOut(header2, log)
    for idx, dist in enumerate(gp_dists):
        # line = (gen.RoundText(dist, 2).rjust(8) +
        #         gen.RoundText(c_vals.loc[dist], 4).center(10) +
        #         gen.RoundText(v_vals.loc[dist], 3).center(10) +
        #         gen.RoundText(dens[idx], 4).center(10) +
        #         gen.RoundText(p_stat[idx], 1).center(10) +
        #         gen.RoundText(p_dyn[idx], 2).center(10) +
        #         gen.RoundText(p_tot[idx], 1).center(10))
        line = (str(idx).rjust(5) + '  ' +
                "{:>10.2f}".format(dist) +
                "{:>9.2f}".format(v_vals[idx]) +
                "{:>13.1f}".format(p_stat[idx]) +
                "{:>8.2f}".format(p_dyn[idx]) +
                "{:>11.1f}".format(p_tot[idx]) +
                "{:>11.4f}".format(c_vals[idx]) +
                "{:>10.4f}".format(dens[idx]) +
                "{:>12.3f}".format(v_vals[idx] * area))
        gen.WriteOut(line, log)
    gen.WriteOut("", log)
    return(old_name, t_index)


def AddTrPerims(seg_constants, fillers, tr_areas, tr_perims, tr_roughs,
                    tr_speeds, seg_ID, debug1, log):
    '''Take the fixed properties of a segment (seg_constants), a list of
    trains that fill the segment and some properties of trains.  Figure out
    the annulus area and the total perimeter, calculate a new hydraulic
    diameter.  Return a list of segment constants with the area, perimeter
    roughness and speed of the train added and the area and hydraulic
    diameter of the annulus.  Note that if the area becomes too small
    or negative, an error is raised in the calling routine, not this
    one.


        Parameters:
            seg_constants   []              A list of lists.  Each sub-list
                                            holds the fixed properties of
                                            a segment (area, perimeter etc.)
            fillers         []              A list of the numbers of trains
                                            that fill this segment end to
                                            end.
            tr_areas        []              A list of train areas (m^2)
            tr_perims       []              A list of train perimeters (m)
            tr_roughs       []              A list of train roughness
                                            heights (m) or friction factors
            tr_speeds       []              A list of train speeds (m/s)
            seg_ID          int             The ID of the segment being
                                            processed.
            debug1          bool            The debug Boolean set by the user.
            log             handle          The handle of the logfile.

        Returns:
            new_consts     []               A list of constants including
                                            the frictional properties of
                                            the trains in the segments
                                            and the reduced are and
                                            hydraulic diameter.
    '''
    # Get the area and perimeter of the open tunnel.
    (new_area, perim) = seg_constants[:2]
    # Note that seg_constants is:
    #  (area, perim, d_h, roughness, rr, rr37,
    #   zeta_back_bf, zeta_back_fb,
    #   zeta_fwd_bf, zeta_fwd_fb,
    #   back_type, back_value, fwd_type, fwd_value,
    #   fric_const, dtdx, dx_local, gridpoints,
    #   end_test, navigable)
    unchanged = list(seg_constants[3:19]) # This is roughness to end_test


    tr_props = []
    new_perim = perim
    new_consts = []
    for index, train in enumerate(fillers):
        tr_area = tr_areas[train]
        tr_perim = tr_perims[train]
        new_area -=  tr_area
        new_perim += tr_perim
        tr_props.extend([tr_area, tr_perim,
                         tr_roughs[train], tr_speeds[train]])
    new_d_h = 4 * new_area / new_perim
    tr_count = len(fillers)
    # Create a new list of constants.  We use the new area and hydraulic
    # diameter but use the original perimeter.  This because the perimeter
    # of stationary tunnel wall is one portion of the segment perimeter,
    # which may also have the perimeter of moving trains.  See Figure 1
    # of Woods & Pope 1979 (ASME-CSME conference) or Woods & Pope 1980
    # (a slightly-updated version of the 1979 conference paper published
    # in the Journal of Wind Engineering and Industrial Aerodynamics).
    new_consts = [new_area, perim, new_d_h] + unchanged + tr_props + [ tr_count]
    message = ("Seg_ID " + str(seg_ID) + " is filled end to end by "
                + str(tr_count) + " train" + gen.Plural(tr_count) + ": "
                + str(fillers))
    gen.WriteOut(message, log)
    if debug1:
        print(message)
    return(tuple(new_consts))


def GetRoughness(roughness, frictiontype):
    '''Take a roughness height (+ve value) or a friction factor (-ve value).
    If it is +ve, return it unchanged.  If it is -ve convert it to Fanning
    friction factor.

        Parameters:
            roughness       float           If -ve, a Fanning friction factor.
                                            Otherwise a roughness height in
                                            metres.
            frictiontype    str             Either "Fanning", "Darcy" or
                                            "Atkinson" (these are different
                                            types of friction factor).

        Returns:
            roughness       str             If +ve, a rougnmess height.  If
                                            -ve, a Fanning friction factor.
    '''
    if roughness < 0.0:
        # Negative roughness means constant friction factor.
        if frictiontype == "darcy":
            # It is not a roughness, it is Darcy friction
            # factor lambda.  Divide it by 4 to get Fanning
            # friction factor c_f.
            roughness = roughness / 4
        elif frictiontype == "atkinson":
            # It is not a roughness, it is Atkinson friction
            # factor k (from the mine vent field).
            # Divide it by 4 and by half of rho (rho =1.2 kg/m^3)
            # to get Fanning friction factor c_f.
            roughness = roughness / (4 * 0.5 * 1.2)
    return(roughness)


def TrainSpeedAccel(speed_spec, time, log):
    '''Take a train speed definition.  If it is a float, return the
    number for the velocity and zero for the acceleration.
    If it is a three-element tuple of np.arrays of time, speed and
    acceleration, figure out what the train speed and acceleration
    are at the current time.

        Parameters:
            speed_spec      float or ()     If a float, a constant speed.
                                            If a tuple, use linear
                                            interpolation to the time
                                            given.
            time            float           The time to interpolate at.
            log             handle          The handle of the logfile.

        Returns:
            speed           float           The speed of the train (m/s)
            accel           float           The acceleration of the
                                            train (m/s^2)
    '''
    if type(speed_spec) is tuple:
        # This train is following a speed-time curve.  The train speed
        # specification is a tuple of numpy arrays (time, speed and
        # acceleration) extending to just beyond the end of the run.
        speed = np.interp(time, speed_spec[0], speed_spec[1])
        accel = np.interp(time, speed_spec[0], speed_spec[2])
    else:
        # This train is at constant speed.
        speed = speed_spec
        accel = 0.0
    return(speed, accel)


def JetFanDragArrays(t_JF_dict, tuns2segs, segs2tuns, segments_consts,
                     dists, TR_mess, settings_dict, line_triples, log):
    '''Build segment-based entries for jet fan thrust.  The data of
    the operational banks of jet fans is in 't_JF_dict', with the
    names of the tunnel as keys.
    We also write the details of which parts of the plumes from jet
    fans are in which segments.
    We make a list of empty sub_lists.  Each time a segment has part
    of a jet fan plume in it the corresponding sub-list is filled out
    with data that can be used to handle the drag in each cell.  Note
    that overlapping jet fan plumes are allowed, because it is possible
    to have a turnout cavern that has jet fans above the main traffic
    lanes and above the slip lanes.
    If a segment has no jet fan plumes in it the string "not here"
    appears in place of the sub-lists.
    The values in the entries apply to segments (which have constant
    area).
    We build one view of the data for the Python routines (a list of
    lists for each segment) and a different view of it for the
    Fortran routines (a flattened list for each segment, because we
    can't seem to pass more than one list of constants via f2py).

        Parameters:
            t_JF_dict       {}              Dictionary of the jet fans
                                            thrusts.  The keys are the
                                            names of tunnels.  The
                                            results are lists of parts
                                            of the jet fan plumes that
                                            enter or fill the segments
                                            in that tunnel, along with
                                            the index of the line that
                                            defines the jet fans
            tuns2segs       {}              A dictionary.  The keys are
                                            tunnel names, the result is
                                            a dictionary of seg_IDs and
                                            distances in that tunnel.
            segments_consts [()]            A list of tuples of the fixed
                                            values in segments.  We need
                                            the first value, segment area.
            dists           [float]         A list of distances in the
                                            tunnel that correlate to the
                                            route chainages above.
            TR_mess         [str]           A list of strings giving the
                                            user data for the segments
                                            that contain traffic drag.
                                            We add to it here.
            settings_dict   {}              Dictionary of the run settings.
            line_triples    [int, str, str] Data for all the lines.
            log             handle          The handle of the logfile.

        Returns:
            JF_drag         [[]]            A list of sub-lists (one per
                                            segment).  Each sub-list may
                                            be empty (no jet fan plumes
                                            in the segment) or have
                                            sub-sub lists, one for each
                                            jet fan plume that enters
                                            the segment.  The sub-sub
                                            lists are combined with the
                                            air velocity at the jet fan
                                            outlet at each timestep to
                                            give the traffic drag in
                                            each cell at that timestep.
    '''
    dt = settings_dict["aero_step"]
    JF_drag = [[] for discard in range(len(segments_consts))]

    # First, figure out which segments the jet fan outlets are in
    # and store them in a dictionary (JF_outlets.  The keys are the
    # names of the jet fan banks.  The values yielded are lists
    # with six numbers, as follows:
    #
    #  * index in v_array of the segment that holds the outlet when
    #    the jet fan is running in forwards mode.
    #
    #  * index in a sub-list in v_array of the gridpoint at the back
    #    end of the cell that contains the outlet when the jet fan
    #    is running in forwards mode.
    #
    #  * location of the discharge point when the jet fan is running
    #    in forwards mode.  It is expressed as a fraction of the
    #    cell length, measured from the back end.
    #
    #  * area of the segment at the jet fan outlet when running in
    #    forwards mode.
    #
    #  * index in v_array of the segment that holds the outlet when
    #    the jet fan is running in reverse mode.
    #
    #  * index in a sub-list in v_array of the gridpoint at the back
    #    end of the cell that contains the outlet when the jet fan
    #    is running in reverse mode.
    #
    #  * location of the discharge point when the jet fan is running
    #    in reverse mode.  It is expressed as a fraction of the
    #    cell length, measured from the back end.
    #
    #  * area of the segment at the jet fan outlet when running in
    #    reverse mode.
    #
    # In most situations the indices and areas will be the same, but
    # on a few long, fake "jet fans" like Epping Services Facility
    # the jet fan outlets in forwards and reverse mode may be in
    # different segments or different cells.
    #
    JF_outlets = {}
    for tun_key, JFbank_specs in t_JF_dict.items():
        # Get the list of segments in this tunnel.
        segments = tuns2segs[tun_key]["segs"]
        # Get the data for each jet fan that could send a jet plume
        # into the segments of this tunnel.
        for JFcalcdata in JFbank_specs:
            (JF_name, fwd_dist1, fwd_dist2, E_fwd, U_f,
                      rev_dist1, rev_dist2, E_rev, U_r,
                      times, speeds, tr_index) = JFcalcdata

            # fwd_dist1 is the location of the jet fan discharge when
            # the jet fans are running in forwards mode, rev_dist2 is
            # the location when the jet fans are running in reverse
            # mode.  These are the locations we take the segment area
            # to divide the E values from and where we take the
            # background air velocity approaching the jet fan at.
            # We can't allow either of these locations to be at
            # segment boundaries because there could be two or more
            # values of air velocity at the boundary and we can't
            # figure out which one to use.
            #
            for seg_ID in segments:
                seg_index = seg_ID - 1
                gp_dists = dists[seg_index]
                fwd_here = False
                rev_here = False

                # Check if the outlet in forwards mode is in this
                # segment.
                result = CheckDist(fwd_dist1, gp_dists, False, log)
                if result == "not here":
                    # The jet fan discharge when in forwards mode is
                    # not in this location.  We'll look in other
                    # segments for it.
                    pass
                else:
                    # Get the segment index, the gridpoint index, the
                    # fractional distance along the cell and the area
                    # at the jet fan outlet.
                    fwd_seg = seg_index
                    fwd_gp, fwd_frac = result
                    fwd_area = segments_consts[seg_index][0]
                    fwd_here = True
                    # Check if the jet fan outlet coincides with the back
                    # end or forward end of a segment and complain if it
                    # does. The problem is that the segments at the
                    # boundary may have different air velocities in them.
                    # In those cases, it is unclear which air velocity
                    # should be used in the jet fan thrust calculation
                    # so we can't proceed with the calculation.
                    result2 = JFOutletCheck(fwd_dist1, "forwards", gp_dists,
                                           settings_dict, tun_key,
                                           line_triples, tr_index, log)
                    if result2 is None:
                        return(None)

                # Now do the outlet when the jet fan is running in
                # reverse.
                if math.isclose(fwd_dist1, rev_dist2):
                    if result != "not here":
                        # The jet fan has a zero length carcass (this
                        # should be the case most of the time) and is
                        # in this cell.  Copy the data to the outlet
                        # when running in reverse.
                        rev_seg = seg_index
                        rev_gp, rev_frac = result
                        rev_area = fwd_area
                        rev_here = True
                else:
                    # Check if the outlet in reverse mode is in this
                    # segment.
                    result = CheckDist(rev_dist2, gp_dists, False, log)
                    if result != "not here":
                        rev_seg = seg_ID - 1
                        rev_gp, rev_frac = result
                        rev_area = segments_consts[seg_index][0]
                        rev_here = True
                        # Check if the outlet in reverse mode is too
                        # close to a boundary.
                        result = JFOutletCheck(rev_dist2, "reverse", gp_dists,
                                               settings_dict, tun_key,
                                               line_triples, tr_index, log)
                        if result is None:
                            return(None)
                # Build a value or pair of values to store in the
                # dictionary.
                if fwd_here and rev_here:
                    yields = [fwd_seg, fwd_gp, fwd_frac, fwd_area,
                              rev_seg, rev_gp, rev_frac, rev_area]
                    JF_outlets.__setitem__(JF_name, yields)
                elif fwd_here:
                    # Only the outlet in forwards mode is in this
                    # segment. Check if the reverse values have already
                    # been found.
                    try:
                        rev = JF_outlets[JF_name]
                    except:
                        # Make an entry in the dictionary with the
                        # four values we know.  We use the segment
                        # index instead of the seg_ID.
                        yields = [fwd_seg, fwd_gp, fwd_frac, fwd_area]
                    else:
                        # Add the four entries to the existing entries.
                        yields = [fwd_seg, fwd_gp, fwd_frac, fwd_area] + rev
                    JF_outlets.__setitem__(JF_name, yields)
                elif rev_here:
                    # Check if the forwards values have already been
                    # found.
                    try:
                        fwd = JF_outlets[JF_name]
                    except:
                        yields = [rev_seg, rev_gp, rev_frac, rev_area]
                    else:
                        yields = fwd + [rev_seg, rev_gp, rev_frac, rev_area]
                    JF_outlets.__setitem__(JF_name, tuple(yields))


    # Now that we know which segments the ends of the jet fans are in,
    # we can process the same data again and build the arrays we need
    # for each part of the plume in each segment.  We need a second loop
    # because the segment we are processing may be processed before the
    # segment with the jet fan outlet in it: we need to determine those
    # first (in the loop above).
    for tun_key, JFbank_specs in t_JF_dict.items():
        # Get the list of segments in this tunnel.
        segments = tuns2segs[tun_key]["segs"]
        # Get the data for each jet fan that could send a jet plume
        # into the segments of this tunnel.
        for JFcalcdata in JFbank_specs:
            (JF_name, fwd_dist1, fwd_dist2, E_fwd, U_f,
                      rev_dist1, rev_dist2, E_rev, U_r,
                      times, speeds, tr_index) = JFcalcdata
            # Get the location data for the two ends of the jet fan.
            (fwd_seg, fwd_sub, fwd_frac, fwd_area,
             rev_seg, rev_sub, rev_frac, rev_area) = JF_outlets[JF_name]

            # Get the E values into m^2/s^2 per metre, the units
            # used in the calculation routines.  At each timestep these
            # are multiplied by:
            #   * (1 - tunnel velocity / jet velocity),
            #   * the square of fan rotational speed at each timestep, and
            #   * the timestep dt.
            E_fwd = E_fwd / fwd_area
            E_rev = E_rev / rev_area

            for seg_ID in segments:
                seg_index = seg_ID - 1
                seg_dists = dists[seg_index]
                # Get an array of fractional occupancies of the plume
                # in each cell in the segment.  If the plume does not
                # cross a cell, its value is zero.  If it fills the
                # cell completely, its value is one.  If no part of
                # the plume is in this segment, it returns the phrase
                # "not here" instead of a list of fractional occupancies.
                # We do this twice, once for the fan in forwards mode
                # and once for the fan in reverse mode.
                fwd_res, mess1 = BuildJFCells(seg_index, seg_dists,
                                             fwd_dist1, fwd_dist2,
                                             segs2tuns, JF_name,
                                             E_fwd, U_f, log)
                if fwd_res != "not here":
                    # The plume of this jet fan crosses this segment
                    # when it is running in forwards mode.
                    # Check if this fan ever runs in forwards mode
                    # and add it to the list if it does.
                    if max(speeds) > 0.0:
                        # Combine the jet fan thrust term with the
                        # fractional occupancies.
                        fwd_drags = np.array([frac * E_fwd for frac in fwd_res])
                        # We store the indicators to the background
                        # velocity, the jet velocity, zero for the
                        # background velocity (which we will adjust
                        # at each timestep), the array of cell traffic
                        # drag values in m^2/s^2 per metre and the
                        # arrays of times and fan speeds.
                        JFcalc_fwd = [fwd_seg, fwd_sub, fwd_frac, U_f,
                                      fwd_drags, times, speeds, tr_index,
                                      JF_name, True] # True for forwards mode
                        JF_drag[seg_index].append(JFcalc_fwd)
                rev_res, mess2 = BuildJFCells(seg_index, seg_dists,
                                              rev_dist1, rev_dist2,
                                              segs2tuns, JF_name,
                                              E_rev, U_r, log)
                if rev_res != "not here":
                   if min(speeds) < 0.0:
                        # The plume of this jet fan does run in reverse
                        # and it crosses this segment.
                        rev_drags = np.array([frac * E_rev for frac in rev_res])
                        JFcalc_rev = [rev_seg, rev_sub, rev_frac, U_r,
                                      rev_drags, times, speeds, tr_index,
                                      JF_name, False] # False for reverse mode
                        JF_drag[seg_index].append(JFcalc_rev)
                if mess1 != "" and mess2 != "":
                    # The jet fan plume in forwards mode and in reverse
                    # mode are in this cell.
                    mess = mess2 + [mess1[1]]
                elif mess1 != "":
                    mess = mess1
                elif mess2 != "":
                    mess = mess2
                else:
                    mess = []
                if mess != []:
                    # Check if this segment already has traffic in it.
                    # If it does, we add it to the segment's existing
                    # entries.  If it doesn't exist we add it to the
                    # end.
                    if len(TR_mess[seg_index]) == 0:
                        TR_mess[seg_index] =  mess
                    else:
                        TR_mess[seg_index].extend(mess[1:])

    # Each entry in JF_outlets now has four numbers for forwards
    # mode and four for reverse mode.  Key is the jet fan name,
    # e.g. "eb_jf08".  Result is a tuple of eight numbers, as below:
    #
    #    (6,            seg_index pointing to the np.array inside v_cell
    #                   that holds the velocity at the jet fan outlet
    #                   when running in forwards mode.
    #     0,            gp_index in the np.array at the back end of
    #                   the cell that the jet fan outlet is in.
    #     0.220588,     fractional distance from the back end of the
    #                   cell to the jet fan outlet.
    #     92.0,         area of the segment that holds the jet fan outlet
    #                   when the jet fan is running in forwards mode.
    #
    #     5,            seg_index pointing to the np.array inside v_cell
    #                   that holds the velocity at the jet fan outlet
    #                   when running in reverse mode.
    #     2,            gp_index in the np.array at the back end of
    #                   the cell that the jet fan outlet is inside.
    #     0.977941,     fractional distance from the back end of the
    #                   cell to the jet fan outlet.
    #     78.0)         area of the segment that holds the jet fan outlet
    #                   when the jet fan is running in reverse mode.
    return(JF_drag, JF_outlets, TR_mess)


def JFOutletCheck(out_dist, text1, gp_dists, settings_dict,
                  tun_name, line_triples, tr_index, log):
    '''Check if the outlet of a jet fan coincides with the end of a
    segment.  If it does, raise an error and return None.

        Parameters:
            out_dist        float           Distance along the tunnel
                                            of the jet fan outlet.
            text1           str             Either "forwards" or
                                            "reverse".
            gp_dists        np.array        Distances of the gridpoints
                                            in the segment.
            settings_dict   {}              Dictionary of the run settings.
            tun_name        str             Name of the tunnel, for the
                                            error message.
            line_triples  [(int, str, str)] List of lines in the file.
            tr_index        int             Pointer to where the jet fan
                                            is defined in line_triples.
            log             handle          The handle of the logfile.

    '''
    # Set a Boolean to False then check if we need to set it True.
    dud = False
    if math.isclose(out_dist, gp_dists[0]):
        dud = True
        text2 = "back end of a\n"
    elif math.isclose(out_dist, gp_dists[-1]):
        dud = True
        text2 = "forward end of a\n"

    if dud:
        # Raise an error.
        line_num, line_data, line_text = line_triples[tr_index]
        file_name = settings_dict["file_name"]
        mid_dist, JFbank_name, discard, JFtype = line_data.split()[1:5]

        err = ('> In the file named "' + file_name + '"\n'
               '> there was a bank of jet fans (named "'
                 + JFbank_name + '") in\n'
               '> tunnel ' + gen.OptQuotes(tun_name) + ' at distance '
                 + gen.RoundText(mid_dist, 5) + ' m.\n'
               '> When running in ' + text1 + ' mode the outlet of the\n'
               '> jet fan coincides with the ' + text2 +
               '> segment (at distance ' + gen.RoundText(out_dist, 5)
                 + ' in tunnel ' + gen.OptQuotes(tun_name) + ').\n'
               '> Please edit the file to adjust the location of\n'
               '> the jet fan outlet so that it does not coincide\n'
               '> with a boundary between segments.  That is not\n'
               '> allowed because there may be between two and\n'
               "> six tunnel air velocities at boundaries and it\n"
               '> is unclear which should be used.'
              )
        # Check if the jet fan has a non-zero length and add a
        # paragraph that may help out if the reader doesn't realize
        # that a length was set for the jet fan type.
        dist = float(mid_dist)
        if not math.isclose(out_dist, dist):
            # Get the length of the fan carcass, then add something
            # telling the user they can either move the bank of fans
            # or adjust the length in the jet fan type definition.
            fanlength = gen.RoundText(abs(2 * (out_dist - dist)), 2)
            err = err + '\n>\n' + (
                  '> Note that the type of jet fan (called "'
                    + JFtype+ '")\n'
                  '> has a length of ' + fanlength
                    + ' m, which you should factor\n'
                  '> into whether to move the bank of jet fans or\n'
                  '> alter the length of the jet fan type in the\n'
                  '> "jetfantypes" block.')
        gen.WriteError(2961, err, log)
        gen.ErrorOnLine(line_num, line_text, log)
        return(None)
    return("all good")


def BuildJFCells(seg_index, gp_dists, dist1, dist2, segs2tuns, JF_name,
                 E_JF, JF_speed, log):
    '''Take a segment index, the list of segment gridpoint distances
    and two distances that may or may not be in the current segment.
    The distances are the start and end of a plume from a jet fan.
    If the distances are not in the segment, return the string "not here".
    If they are, figure out how much of the plume is in this segment,
    expressed as the index of the gridpoint immediately back from the
    end and a fractional occupancy of the cell (0 means the cell has
    no part of the plume in it, 1 means it is filled end to end).
    Return a list of fractional occupancies the same length as the
    count of cells in the segment (not the count of gridpoints, the
    count of cells).

        Parameters:
            seg_index       int             seg_index of the current
                                            segment
            gp_dists        [float]         A list of distances of the
                                            gridpoints in the segment.
            dist1           float           Distance along a tunnel
                                            of the back end of a jet
                                            fan plume.
            dist2           float           Distance along a tunnel
                                            of the forward end of a jet
                                            fan plume.
            log             handle          The handle of the logfile.

        Returns:
            occupancies   [float] or str    If any of the plume is in
                                            this  cell, a list of
                                            fractional occupancies of
                                            the jet fan plume in the
                                            cells of this segment.
                                            If none of it is, the
                                            string "not here" is
                                            returned instead.
    '''
    gp_count = len(gp_dists)
    back = gp_dists[0]
    fwd = gp_dists[-1]
    # Figure out if the plume from the jet fan is in this segment
    # or if it blows into this segment from a segment attached
    # to the back end of this segment.
    if (fwd < dist1) or (back > dist2):
        # This segment appears back from the jet fan outlet
        # or forward of the end of the plume (or vise-versa
        # if the fan is running in reverse mode.
        # There cannot be any of the plume in this segment.
        empty = True
    elif back <= dist1 < fwd:
        # This segment has the back end of the plume in it.
        seg1_idx = seg_index
        result = CheckDist(dist1, gp_dists, True, log)
        gp1_idx, seg1_frac = result
        empty = False
        JF_start = dist1
        JF_stop = fwd
    else:
        # The plume must cross the back end of this
        # segment (because it must have started in a
        # different segment and extend into this one).
        seg1_idx = seg_index
        gp1_idx = 0
        seg1_frac = 0
        empty = False
        JF_start = back

    # Figure out if the plume from the jet fan ends in
    # this segment or carries into another.
    if empty:
        # This segment does not overlap the plume of the
        # jet fan, no need to do anything else.
        pass
    elif back < dist2 < fwd:
        # The plume ends in this segment.
        seg2_idx = seg_index
        # We pass True in the call to CheckDist() because
        # the test in the above elif means we won't have
        # to trap the "not here" returnee.
        result = CheckDist(dist2, gp_dists, True, log)
        gp2_idx, seg2_frac = result
        JF_stop = dist2
    else:
        # The plume fills up to the forward end of the
        # segment.  We subtract two from the count of
        # gridpoints because we want this to be an index
        # to an array that is cell-based rather than
        # gridpoint based, and there is one fewer entry
        # in a cell-based array than in a gridpoint-based
        # array.  This will definitely confuse me when I
        # look at it again in a few years.
        seg2_idx = seg_index
        gp2_idx = gp_count - 2
        seg2_frac = 1.0
        JF_start = dist1
        JF_stop = fwd

    if empty:
        occupancies = "not here"
        mess = ""
    else:
        # Part of the plume from the jet fan is in this
        # segment.  Make a set of cell-based fractional
        # occupancies for the calculation.

        # Start with a list of zeros (one for each cell). These will
        # be populated with values of fractional plume occupancy.
        occupancies = [0.0] * (gp_count - 1)

        if gp2_idx == gp1_idx:
            # The entire plume is in one cell.  This can happen when
            # a plume passes through a segment that has one cell and
            # when the plume is set to be so short that it is shorter
            # than a cell.
            occupancies[gp1_idx] = seg2_frac - seg1_frac
        else:
            occupancies[gp1_idx] = (1 - seg1_frac)
            occupancies[gp2_idx] = seg2_frac
            if gp2_idx - gp1_idx > 1:
                # There are some cells filled end to end with
                # this jet fan plume.
                for index in range(gp1_idx, gp2_idx):
                    # We don't want to overwrite an existing nonzero
                    # value of fractional cell drag, so we check for
                    # zero values.  Unsure if this is wrong in some
                    # corner case, though.
                    if math.isclose(occupancies[index],0.0):
                        occupancies[index] = 1
        seg_ID = seg_index + 1
        name = segs2tuns[seg_ID]
        if math.isclose(E_JF, 0.0):
            mess = ""
        else:
            dist_text = gen.RoundText(JF_stop - JF_start, 4) + ' m plume)'
            if JF_speed < 0.:
                text1 = gen.RoundText(JF_speed, 4)
                text2 =  ' in reverse mode, ' + dist_text
            else:
                text1 = '+' + gen.RoundText(JF_speed, 4)
                text2 =  ' in forwards mode, ' + dist_text
            mess = ['  Seg_ID ' + str(seg_ID) + ' ('
                    + gen.RoundText(back, 4) + ' m to '
                    + gen.RoundText(fwd, 4) + ' m in tunnel "'
                    + name + '"):',
                    "    Jet fans: " + gen.RoundText(JF_start, 4)
                    + ' m to ' + gen.RoundText(JF_stop, 4)
                    + ' m, ' + text1 + ' m/s, static thrust '
                    + gen.RoundText(abs(E_JF), 10) + ' N/m ('
                    + JF_name + text2]
    return(occupancies, mess)


def CheckDist(candidate, distances, fail, log):
    '''Take a distance along a tunnel and a list of distances of
    gridpoints in a segment.  If the distance is not in this segment,
    either return the string "not here" or fail miserably, depending
    on the value of the Boolean 'fail'.  If the distance is in this
    segment, return the index of the gridpoint at the back end of
    the cell and the fractional distance along the cell.
    If we think we know for sure that the candidate lies inside the
    cell, we set 'fail' to True in the call.  If we were wrong, the
    routine prints some useful information for the programmer before
    stopping.
    If we don't know for sure we pass False instead so that the
    routine that called it can deal with that case.

        Parameters:
            candidate       float           A distance along a tunnel
                                            that may or may not be in
                                            the current segment.
            distances       [float]         A list of the distances of
                                            gridpoints in the current
                                            segment.
            fail            bool            A Boolean to catch mistakes
                                            by the programmer instead
                                            of by the user.  If False,
                                            return the string "not here"
                                            if the candidate distance
                                            is not in the segment.  If
                                            True, raise an error if the
                                            candidate distance is not
                                            in the segment.
            log             handle          The handle of the logfile,
                                            for the error message.

        Returns:
            occupancies   [float] or str    If any of the plume is in
                                            this  cell, a list of
                                            fractional occupancies of
                                            the jet fan plume in the
                                            cells of this segment.
                                            If none of it is, the
                                            string "not here".
    '''

    # In the logic check below, we use math.isclose() in addition
    # to '<=' to minimise floating point mismatches.
    if (distances[0] <= candidate <= distances[-1] or
        math.isclose(candidate, distances[0]) or
        math.isclose(candidate, distances[-1])):
        for index, back in enumerate(distances[:-1]):
            fwd = distances[index + 1]
            if (back <= candidate <= fwd or
                math.isclose(candidate, fwd) or
                math.isclose(candidate, back)):
                # It is in the cell that has this gridpoint at the
                # back end.
                break
        fraction = (candidate - back) / (fwd - back)
        returnee = (index, fraction)
    elif fail is True:
        # We were sure this candidate was in this segment, but we
        # were wrong.  Stop the run with the details of the values
        # we passed to this routine.  This message is for the
        # programmer rather than the user.
        err = ('> Whoops, the programmer missed a corner case in\n'
               '> PROC CheckDist.  Details are:\n'
               '>   Candidate distance: ' + gen.RoundText(candidate, 2) + '\n'
               '>   Gridpoint distances: ' + str(distances) + '\n'
               '> '
              )
        gen.WriteMessage(err, log)
        gen.OopsIDidItAgain(log)
    else:
        returnee = "not here"
    return(returnee)


def TrafficDragArrays(vehicles_dict, vehcalc_dict, calc_traffic,
                      tuns2segs, segs2tuns, segments_consts, dists,
                      debug1, log):

    '''Build segment-based entries for traffic drag.  The traffic
    data is in calc_traffic, with the name of the tunnel as keys.
    We make a list of empty sub_lists.  Each time a cell in a segment
    has traffic in it the corresponding sub-list is filled out with
    data that can be used to handle the drag in each cell.  Note
    that there may be traffic at multiple speeds in the cells (from
    bidirectional traffic and heavy vehicles slowing on upgrades).
    The values in the entries apply to segments (which have constant
    area) so we apply the blockage correction (if the user wants it).
    We can also merge the C_d * A_veh of vehicle types that are at
    the same speed.
    We build one view of the data for the Python routines (a list of
    lists for each segment) and a different view of it for the
    Fortran routines (a flattened list for each segment, because we
    can't seem to pass more than one list of constants via f2py).


        Parameters:
            vehicles_dict   {}              Dictionary defining the vehicle
                                            type definitions.
            vehcalc_dict    {}              Dictionary defining the vehicles
                                            for the calculation.
            calc_traffic    {}              Dictionary defining the vehicles
            log             handle          The handle of the logfile,
                                            for the error message.

        Returns:
            TR_drag       [[]]              A list of sub-lists, one for
                                            each segment.  Each sub-list
                                            is either empty (no traffic
                                            in that segment) or has
                                            sub-sub-lists of four numbers
                                            giving the extents of
                                            traffic, the traffic speed
                                            and the traffic drag term
                                            used in the calculation.
                                            There is one sub-sub-list
                                            for each block of traffic
                                            in the segment.
            TR_mess       [[str]]           A list of sub-lists, one for
                                            each segment, giving the
                                            strings
    '''
    TR_drag = [ [] ]* len(segments_consts)
    TR_mess = TR_drag.copy()

    if vehicles_dict != {}:
        veh_names = tuple(vehcalc_dict.keys())
        # Make a list of the name, area and (c_d * area) of each vehicle
        # type so we can use them in the loops below.  This is more
        # convenient than calling vehcalc_dict each time and we need
        # to calculate c_d * area anyway, so best to do them all at
        # once.
        veh_drag = []
        for name, veh_props in vehcalc_dict.items():
            veh_area, c_d = vehcalc_dict[name][1:3]
            veh_drag.append((name, veh_area, c_d * veh_area))

        # Set a Boolean to tell if the user wanted to apply the blockage
        # correction.  The test below is for the line in the input file
        # that has "calculate with blockage correction" or "calculate
        # without blockage correction" on it.
        if "with blockage" in vehicles_dict["calculate"][0]:
            block_corr = True
        else:
            block_corr = False

        for tun_key, traffic_spec in calc_traffic.items():
            # Get the segments in this tunnel and look at its traffic.
            segs = tuns2segs[tun_key]['segs']
            for seg_ID in segs:
                seg_index = seg_ID - 1
                # Get the segment's area.
                area = segments_consts[seg_index][0]
                # Get the gridpoint distances in this segment.
                seg_start = dists[seg_index][0]
                seg_stop = dists[seg_index][-1]
                # Loop over the traffic in this tunnel and check if
                # any of it overlaps this segment.  If it does,
                # store its properties in a way that can be used by
                # the calculation.
                for veh_spec in traffic_spec:
                    # Unpack the values for the current block of traffic
                    # in this tunnel.
                    start, stop, speed = veh_spec[:3]
                    # Check if this block of traffic overlaps this segment.
                    if start < seg_stop and stop > seg_start:
                        # It does.  Get the extents in this segment.
                        tr_start = max(start, seg_start)
                        tr_stop = min(stop, seg_stop)
                        # Now add up the drag contributions of all the
                        # different vehicles, accounting for blockage
                        # if necessary.  These vehicles are all moving
                        # at the same speed, so summing up their drag
                        # areas is valid.
                        drag_term = 0.0
                        for index in range(len(vehcalc_dict)):
                            name, veh_area, veh_ACd = veh_drag[index]
                            if block_corr:
                                # Adjust the product of vehicle area for
                                # blockage using the area of this segment.
                                veh_ACd = veh_ACd / (1 - veh_area/area)**2
                            # Now multiply by the vehicle density in veh/km,
                            # divide by 1000 to get a per-metre figure, and
                            # divide by the segment area so we have a
                            # dimensionless pressure loss factor per metre
                            # of segment length.
                            # We also include the 1/2 term from the dynamic
                            # pressure, so that we don't have to multiply
                            # by 0.5 hundreds of times each timestep.
                            # The units calc is as follows, for future
                            # reference and verification:
                            #
                            #  area *  c_d   * density  * 0.001  / segment / 2
                            #                                       area
                            #
                            #   m^2   dimen      veh        km         1
                            #   --- *  sion  *   ---    *   --   *    ---  * 0.5
                            #   veh    less       km         m        m^2
                            #
                            # Once the dimensions are cancelled the units are
                            # m^-1.  This can be thought of as a pressure loss
                            # factor (k-factor, or zeta in Hobyah parlance) per
                            # metre length of segment, although keep in mind
                            # that it includes the 0.5 term that is usually
                            # excluded from zeta!
                            # When we are calculating the body force terms in
                            # the characteristics (E_b for forward characteristics
                            # and E_f for backwards characteristics, see the
                            # description in MoC.pdf) we can multiply the
                            # zeta/m value by whatever length of the queue
                            # overlaps the characteristic.  There will be
                            # situations in which a queue of stationary
                            # traffic stops in the middle of a cell, so
                            # only part of the length of the characteristic
                            # has a traffic drag term.  This is a painful
                            # way to program, but has benefits for the
                            # users because queues of traffic don't have
                            # to start and end at gridpoints.
                            drag_term += veh_ACd * veh_spec[index + 3]\
                                         / (2000. * area)
                            # The above term can be multiplied by
                            #   (v_v - v_t) * |v_v - v_t| * dx_traf * dt
                            # to give the traffic drag component of E_b
                            # or E_f.  dx_traff is the length of traffic
                            # block that overlaps the length of a cell.
                            # Note that we don't include the 'dt' term
                            # here.  This is in case some calculations
                            # need to use lower values of 'dt' for
                            # calculations at intermediate timesteps.
                        result = [tr_start, tr_stop, speed / 3.6, drag_term]
                        # result = np.array([tr_start, tr_stop, speed, drag_term])
                        # TR_drag[seg_index].append(result)
                        TR_drag[seg_index] = TR_drag[seg_index] + [result]
                TR_drag[seg_index].sort(key=operator.itemgetter(0))

                # Build a list of messages to write to the screen and
                # log file giving details of the traffic drag.  We pass
                # this to the calling routine so that the details of
                # the jet fan thrusts can be added and the information
                # printed out and stored in the log file.
                name = segs2tuns[seg_ID]
                mess = ['  Seg_ID ' + str(seg_ID) + ' ('
                        + gen.RoundText(seg_start, 4) + ' m to '
                        + gen.RoundText(seg_stop, 4) + ' m in tunnel "'
                        + name + '"):']
                for entries in TR_drag[seg_index]:
                    # Get the details of the file for the log file and
                    # screen.  Note that we multiply by two so that the
                    # value we call 'drag (in zeta/m) is the printed as
                    # a traditional k-factor, even though we are storing
                    # half its value for use in the calculation loop.
                    mess.append("    Traffic: " + gen.RoundText(entries[0], 4)
                                + ' m to ' + gen.RoundText(entries[1], 4)
                                + ' m, ' + gen.RoundText(entries[2] * 3.6, 4)
                                + ' km/h with drag '
                                + gen.RoundText(entries[3] * 2, 15) + ' zeta/m')
                TR_mess[seg_index] = mess
    return(TR_drag, TR_mess)


def CheckEntityName(entity2_name, entity2_type, tunnel2_name, line2_num,
                    line2_text, entities_dict, file_name, log):
    '''Check for name clashes in the various named entities (fans, dampers,
    PSDs and whatnot).  If the name is unique, add its properties to the
    entities dictionary so we can catch future name clashes.

        Parameters:
            entity2_name    str             Name of an entity
            entity2_type    str             Type of entity, e.g. "fan".
                                            Used in the error message.
            tunnel2_name    str             Name of the tunnel the
                                            entity is in.  Used in
                                            the error message.
            line2_num       int             The line number being processed.
            line2_text      str             The text of the line being processed.
            log             handle          The handle of the logfile.
            entities_dict   {}              Dictionary of the named entities.
                                            Used to catch name clashes.
            file_name       str             The file name, for error messages.
            log             handle          The handle of the logfile.

        Returns:
            entities_dict   {}              Updated dictionary.
        Errors:
            Aborts with 2561 if the name of the new entity clashes with
            the name of an existing entity.
    '''
    if entity2_name in entities_dict:
        # This is a duplicate entity name, complain about it.
        entity1 = entities_dict[entity2_name]
        (entity1_type, entity1_name,
         tunnel1_name, line1_num, line1_text) = entities_dict[entity2_name]
        err = ('> In the file named "' + file_name + '"\n'
               '> there were two entities with the same name ('
                 + entity2_name + ').\n'
               '> The first is ' + entity1_type + ' in tunnel "'
                 + tunnel1_name + '", the\n'
               '> second is ' + entity2_type + ' in tunnel "'
                 + tunnel2_name + '".  Please\n'
               '> edit the file and rename one of them.'
              )
        gen.WriteError(2561, err, log)
        gen.ErrorOnTwoLines(line1_num, line1_text,
                            line2_num, line2_text, log)
        return(None)
    else:
        # Add it to the dictionary and return it.
        new_props = (entity2_type, entity2_name,
                     tunnel2_name, line2_num, line2_text)
        entities_dict.__setitem__(entity2_name, new_props)
    return(entities_dict)


def NotConverged(dud_calc, success, seg_index, time, log):
    '''A calculation in a cell or at a node failed to converge.  Write
    the details of the failure to the screen and to the log file and
    update the counter of failures by one.

        Parameters:
            dud_calc        int             Count of how many dud
                                            calculations have occurred
            success         str             Description of the failure
            seg_ID          int             Segment number, for the
                                            error message.
            time            float           Time the error occurred.
            log             handle          The handle of the logfile.

        Returns:
            dud_calc        int             An updated count
    '''
    dud_calc += 1
    # Add some detail of the time and the segment that the calculation
    # failed at.  Write these to the log file.  The most likely
    # candidate or failures is open portals with large pressure loss
    # factors, but fans can glitch when their system characteristics
    # cross stall humps in the fan characteristics and gravity-operated
    # flap dampers (whose resistance varies with pressure difference)
    # are notoriously unstable.
    success = (success + ' of seg_ID ' + str(seg_index + 1)
                + ' at ' + gen.FloatText(time) + ' s.')
    print(success)
    gen.WriteOut(success, log)
    return(dud_calc)


def GetCharData(seg_ID, area, char_name, optionals_dict,
               fanchars_dict, settings_dict, file_name, tunnel_name,
               line_number, line_text, debug1, log):
    '''Take the details of a tunnel, the name of a fanchar, the dictionary
    of fan characteristics and the dictionary of optional entries in the
    line.  Check if the name of the fan characteristic we want has been
    defined in a "begin fanchar"..."end fanchar" block and fault if it
    has not.
    Otherwise, take the fan characteristic and turn its volume flows into
    air velocities in this tunnel's cross-sectional area.  Keep the
    pressure rise as celerity^2 (m^2/s^2).  We add points on the
    characteristic that correspond to +/-100,000,000,000 m^3/s so that
    in the runtime code (which uses np.interp to interpolate to get the
    fan celerity^2 based on the tunnel air velocity) is unlikely to
    try to end up trying to extrapolate.  Extrapolating causes np.interp
    to fault, so having the characteristics extend out to 100 trillion
    m^3/s is a sure-fire way of avoiding the faulting.

        Parameters:
            seg_ID          int             Segment number, for the
                                            error message.
            area            float           Area in the segment.
            char_name       str             Name by which the plot
                                            routines refer to the fan
            optionals_dict  {}              Dictionary of the optional
                                            entries on the line that
                                            defined the fan
            fanchars_dict   {}              Dictionary of fan chars.
                                            The fan will use one.
            settings_dict   {}              Dictionary of the run settings.
            file_name       str             The file name, for error messages.
            tunnel_name     str             The tunnel name, for error messages.
            line_number     int             The line number being processed.
            line_text       str             The text of the line.
            debug1          bool            The debug Boolean set by the user.
            log             handle          The handle of the logfile.

        Returns:
            fan_details     []              A list of four np.arrays (see
                                            description below).
            lines           [str]           A list of lines describing
                                            the fan.  The routine that
                                            called it can print them
                                            to the log file.
        Errors:
            Aborts with 2521 if the named fan characteristic does
            not exist.
    '''
    aero_time = settings_dict["aero_time"]

    # First check that the name of the fan exists.
    if char_name not in fanchars_dict:
        err = ('> In the file named "' + file_name + '"\n'
               '> you tried to use a fan characteristic in\n'
               '> tunnel "' + tunnel_name
                  + '", but the fan characteristic\n'
               '> (named "' + char_name + '") does not exist.\n'
               '> Please either add a "fanchar" block with this\n'
               '> name or change its name to one of the fanchars\n'
               '> defined in the file, which are:\n'
                 + gen.FormatOnLines(tuple(fanchars_dict.keys()))
              )
        gen.WriteError(2521, err, log)
        gen.ErrorOnLine(line_number, line_text, log, False)
        return(None)

    char_dict = fanchars_dict[char_name]
    fwd_char = char_dict["forwards"]
    rev_char = char_dict["reverse"]
    gamma = settings_dict["gamma"]

    # Add points at +/-100 trillion cubic metres per second to the
    # fan characteristics.  This should make runtime interpolation
    # faster.  This routine also divides the volume flow by the
    # area of the tunnel the fan is in to get air velocities (this
    # is good because the program calculates air velocities at each
    # timestep, not volume flows.
    fwd_vels, fwd_presses = NumpifyFanChar(fwd_char, area)
    rev_vels, rev_presses = NumpifyFanChar(fwd_char, area)


    # Build a list of lines giving the forwards and reverse fan chars.
    # We return this to the calling routine and it is printed to the
    # log file after printing the segment definition.
    lines = ['  Fan curve at fwd end is from fan char. "' + char_name + '":',
             "  Internal forwards fan char  |  Internal reverse fan char",
             "     Tunnel    gamma^2 P_tot  |     Tunnel    gamma^2 P_tot",
             "       air     -------------  |       air     -------------",
             "    velocity     rho * psi    |    velocity     rho * psi",
             "      (m/s)      (m^2/s^2)    |      (m/s)      (m^2/s^2)"
            ]
             #     0.32609       13.587     |     0.32609       13.587

    for (fwd_vel, fwd_press) in zip(fwd_vels[1:-1], fwd_presses[1:-1]):
        lines.append("{:>12.5f}".format(fwd_vel) + " "
                "{:>12.3f}".format(fwd_press) + " ")

    # Pad out the lines with spaces if we need them so that the numbers
    # in the columns of the reverse characteristic line up.
    if len(rev_vels) > len(fwd_vels):
        lines.extend([' ' * 26] * (len(rev_vels) - len(fwd_vels)) )

    for index, (rev_vel, rev_press) in enumerate(zip(rev_vels[1:-1],
                                                     rev_presses[1:-1]),
                                                 start=6):
        left_side = lines[index]
        lines[index] = left_side + ("    |" + "{:>12.5f}".format(rev_vel)
                                 + " " "{:>12.3f}".format(rev_press))

    # Now reverse the order of the elements in the reverse
    # characteristic for numpy.interp: if you don't do this
    # numpy's interpolate function can't handle reverse flow.
    rev_vels.reverse()
    rev_presses.reverse()
    # Put the data we have from the fan definition into a list (flows
    # and pressures in forward mode, then the same in reverse mode).
    # This list will be extended by the calling routine to add a list
    # of times and speeds.
    fan_details = [np.array(fwd_vels), np.array(fwd_presses),
                   np.array(rev_vels), np.array(rev_presses)]
    return(fan_details, lines)


def NumpifyFanChar(fan_char, area):
    '''Take a fan characteristic (a list of volume flows and a list
    of their corresponding fan total pressure rises expressed in
    m^2/s^2) and turn them into lists of air velocities in a tunnel
    and values of m^2/s^2.
    Extend the characteristics by linear interpolation back to
    -100,000,000,000 m^3/s and forward to +100,000,000,000 m^3/s,
    so that the runtime code can call np.interp most of the time
    instead of having to use the (much slower) Python function
    'interp()' in the generics.py file.

    It is possible to craft a fan characteristic in which the
    pressure value at +/-100,000,000,000 causes a floating point
    overflow in the calculated pressure.  If any of you craft such
    a cursed fan curve deliberately and submit a bug report about it,
    you can go fuck yourselves, because I haven't trapped that
    particular corner case and am telling you about it here.  Breaking
    my code in that particular way is unnecessary and uncool.  Shame
    on you!
    If you manage to do it accidentally, get in touch and I promise
    not to be mad.
    '''
    flowlim = 100000000000
    flows, presses = fan_char
    mod_flows = [-flowlim] + flows + [flowlim]
    low_Ptot = gen.Interpolate(flows[0], flows[1],
                               presses[0], presses[1],
                               -flowlim, True)
    high_Ptot = gen.Interpolate(flows[-2], flows[-1],
                               presses[-2], presses[-1],
                               flowlim, True)
    mod_presses = [low_Ptot] + presses + [high_Ptot]

    # Divide the volume flows by segment area.
    mod_vels = [flow / area for flow in mod_flows]
    return(mod_vels, mod_presses)



def Fan1Timing(fan_speed, optionals_dict, char_name, fanchars_dict,
               settings_dict):
    '''Build the list of times to set the operation of a type 1 fan.
    (a fan that starts at a time, runs at a constant speed, then stops:
    much like fans in SES).  See also Fan2Timing (more flexible).

    Turn the optional entries (which set a start and finish time if they
    are present) and the startup and rundown times in the fan dictionary
    into a list of fan operation speeds.  If there are no optional
    entries, make the fan start up at time zero and stop after the run
    ends.

        Parameters:
            fan_speed       float           A constant fan speed.  +1
                                            is full speed in forwards
                                            mode, -1 is reverse.
            optionals_dict  {}              Dictionary of the optional
                                            entries on the line that
                                            defined the fan
            char_name       str             Name by which the plot
                                            routines refer to the fan
            fanchars_dict   {}              Dictionary of fan chars.
                                            The fan will use one.
            settings_dict   {}              Dictionary of the run settings.

        Returns:
            fan_details     []              A list of four np.arrays (see
                                            description below).
            lines           [str]           A list of lines describing
                                            the fan.  The routine that
                                            called it can print them
                                            to the log file.
    '''
    aero_time = settings_dict["aero_time"]
    aero_step = settings_dict["aero_step"]

    # Check for four optional arguments that set the start time, stop
    # time, time to get to speed and time to stop after switching off.
    #
    # If there are none, the fan will start at time zero, take 15
    # seconds to get to speed, and continue running until the end
    # of the run.
    try:
        value = optionals_dict["runup"]
        runup = float(CheckForConstant(value, False, settings_dict))
    except:
        # Default is 15 seconds for an axial/centrifugal fan to go
        # from stationary to full speed.
        runup = 15.0
    try:
        value = optionals_dict["rundown"]
        rundown = float(CheckForConstant(value, False, settings_dict))
    except:
        # Default is 20 seconds for an axial/centrifugal fan to slow down.
        rundown = 20.0

    try:
        value = optionals_dict["start"]
        start = float(CheckForConstant(value, False, settings_dict))
    except:
        start = 0.0

    at_speed = start + runup

    try:
        value = optionals_dict["stop"]
        stop = float(CheckForConstant(value, False, settings_dict))
    except:
        # We want the fan to continue running at the end of the run.
        stop = max(at_speed, aero_time) + aero_step

    # Build a list of times.  If necessary extend the time to just after
    # the run ends.
    times = [start,  at_speed,     stop,   stop + rundown]
    speeds = [0.0,  fan_speed,  fan_speed,       0.0]
    if not(math.isclose(start, 0.0)):
        times.insert(0, 0.0)
        speeds.insert(0, 0.0)
    if times[-1] <= aero_time:
        times.append(aero_time + aero_step)
        speeds.append(speeds[-1])
    elif times[-2] > aero_time:
        # We don't need the rundown portion of the time-speed curve, as
        # it starts beyond the end of the run.
        times.pop(-1)
        speeds.pop(-1)

    text1, text2 = gen.AlignListPrint((times, speeds))
    lines = ['"Fan1" keyword operating rules:',
             " * Operating times (s): " + text1,
             " * Fractional speeds:   " + text2]
    times = np.array(times)
    speeds = np.array(speeds)
    return([times, speeds], lines)


def Fan2Timing(details, char_name, fanchars_dict, settings_dict,
               file_name, line_triples, log):
    '''Build the list of times to set the operation of a type 2 fan
    (a fan that can change speed any number of times, set by data in
    the columns of a datablock).  See also Fan1Timing (simpler).

    Get the name of the datablock, the nicknames/numbers of the
    columns to use for time and fan speed, get the data and ensure
    it has no gaps.  If the data stops before the end of the run,
    set constant speed from the last time defined to the end of the
    run.

        Parameters:
            details         []              List of words on the line
                                            defining the fan, and the
                                            optional entries on the line.
            char_name       str             Name by which the plot
                                            routines refer to the fan.
            fanchars_dict   {}              Dictionary of fan chars.
                                            The fan will use one.
            settings_dict   {}              Dictionary of the run settings.
            line_triples    [int, str, str] Data for all the lines.
            log             handle          The handle of the logfile.

        Returns:
            unnamed         []              A list of two np.arrays of
                                            times and speeds, that
                                            the calculation will
                                            interpolate with at each
                                            timestep.
            lines           [str]           A list of lines describing
                                            the fan operating times
                                            and corresponding speeds.
    '''
    # Get the nickname of the datasource, the column numbers/nicknames
    # and the index of where the line is in line_triples (we need to
    # send it to other routines).
    nickname = details[4]
    t_col = details[5]
    spd_col = details[6]
    tr_index = details[-1]

    curve_defn = fanchars_dict[char_name]
    # Call a routine that extracts up to three columns from a datasource
    # and faults if it can't find them.  We want the sequences to have
    # no missing entries (i.e. be unbroken).
    unbroken = True
    result = GetTabulated(settings_dict, line_triples, tr_index, log,
                          unbroken, nickname, t_col, spd_col)
    if result is None:
        return(None)
    else:
        # We get a list of two lists and a list of QA data back.
        ((times, speeds), QA_data) = result

    # Check that the times start at zero and increase as we go through
    # the sequence.
    zerostart = True
    result = CheckTimesIncrease(t_col, times, QA_data, zerostart, nickname,
                                "a fan operating sequence", "fan",
                                file_name, line_triples, tr_index, log)
    if result is None:
        return(None)


    # If the sequence ends before the run does, extend the sequence
    # to just beyond the run time.  We don't need to check for
    # non-zero start times, CheckTimesIncrease() did that.
    aero_time = settings_dict["aero_time"]
    if times[-1] <= aero_time:
        aero_step = settings_dict["aero_step"]
        times.append(aero_time + aero_step)
        speeds.append(speeds[-1])

    # Build a set of lines describing the datasource and the columns
    # used in it, for the log file.  This lets engineers check things,
    # e.g. check that the column they think they are using for time
    # is the column that the program to use for time.  These are
    # written to the log file by the calling routine.
    descriptors = ['"Fan2" keyword operating rules',
                   "Operating times",
                   "Fractional speeds"]
    lines = DatasourceDescrip(descriptors, (times, speeds), QA_data)
    times = np.array(times)
    speeds = np.array(speeds)
    return([times, speeds], lines)


def JetFan1Timing(JF_speed, optionals_dict, runup, rundown, settings_dict):
    '''Build the list of times to set the operation of a type 1 jet fan.
    (a fan that starts at a time, runs at a constant speed, then stops:
    much like jet fans in SES form 7C).

    Turn the optional entries (which set a start and finish time if they
    are present) and the startup and rundown times in the fan dictionary
    into a list of fan operation speeds.  If there are no optional
    entries, make the fan start up at time zero and stop after the run
    ends.

    The time taken for the jet fans to run up and run down may be set
    by optional entries in the "jetfans1" line of entry, optional
    entries in the "jetfantypes" or the default values (5 seconds to
    run up, 10 seconds to run down).

        Parameters:
            JF_speed        float           A constant fan speed.  +1
                                            is full speed in forwards
                                            mode, -1 is reverse.
            optionals_dict  {}              Dictionary of the optional
                                            entries on the line that
                                            defined the bank of jet
                                            fans.
            runup           float           The runup time set for
                                            the jet fan type in the
                                            "jetfantypes" block.
            rundown         float           The rundown time set for
                                            the jet fan type in the
                                            "jetfantypes" block.
            settings_dict   {}              Dictionary of the run settings.

        Returns:
            fan_details     []              A list of four np.arrays (see
                                            description below).
            lines           [str]           A list of lines describing
                                            the fan.  The routine that
                                            called it can print them
                                            to the log file.
    '''
    aero_time = settings_dict["aero_time"]
    aero_step = settings_dict["aero_step"]

    # Check for four optional arguments that set the start time, stop
    # time, time to get to speed and time to stop after switching off.
    #
    # If there are none, the fan will start at time zero, take 'runup'
    # seconds (from the jet fan type definition)) to get to speed,
    # and continue running until the end of the run.
    if runup in optionals_dict:
        # Overwrite the value from the jet fan type.
        value = optionals_dict["runup"]
        runup = float(CheckForConstant(value, False, settings_dict))

    if rundown in optionals_dict:
        # Overwrite the value from the jet fan type.
        value = optionals_dict["rundown"]
        rundown = float(CheckForConstant(value, False, settings_dict))

    try:
        value = optionals_dict["start"]
        start = float(CheckForConstant(value, False, settings_dict))
    except:
        start = 0.0

    at_speed = start + runup
    try:
        value = optionals_dict["stop"]
        stop = float(CheckForConstant(value, False, settings_dict))
    except:
        # We want the fan to continue running at the end of the run.
        stop = max(at_speed, aero_time) + aero_step

    # Build a list of times.  If necessary extend the time to just after
    # the run ends.
    times = [start,  at_speed,     stop,   stop + rundown]
    speeds = [0.0,  JF_speed,  JF_speed,       0.0]
    if not(math.isclose(start, 0.0)):
        times.insert(0, 0.0)
        speeds.insert(0, 0.0)
    if times[-1] <= aero_time:
        times.append(aero_time + aero_step)
        speeds.append(speeds[-1])
    elif times[-2] > aero_time:
        # We don't need the rundown portion of the time-speed curve, as
        # it starts beyond the end of the run.
        times.pop(-1)
        speeds.pop(-1)

    times = np.array(times)
    speeds = np.array(speeds)
    return(times, speeds)


def DatasourceDescrip(lines, numbers, QA_data):
    '''Take a list of lines of text, a list of lists of numbers
    and a list of QA text, all from processing datasources in
    keywords like "fan2" and "damper1".  Build a list of lines
    of human-readable text with the description of what datasource
    the data came from, then what each column is, the column number
    (and column name, if used) and the values in the column.
    These are written to the log file for checking.

    Parameters:
        lines           [str]           A list of descriptions.  First
                                        entry describes what it is for
                                        (e.g. "Fan operation rules"),
                                        the other entries describe
                                        what each column is, e.g.
                                        "Fan operation times",
                                        "Fan operation speeds",
        numbers         [[]]            A list of lists of numbers,
                                        with the data in each column.
        QA_data         []              A list of QA data from PROC
                                        GetTabulated, used to add
                                        descriptors to the lines.

    Returns:
        mod_lines       []              A list of lines with information
                                        about the datasource and the
                                        columns used.
    '''
    # Add the name of the datasource to the first line.
    mod_lines = [lines[0] + " came from" + QA_data[1]]
    # Add the column identifiers to the column descriptions.
    col_texts = []
    for index, entry in enumerate(lines[1:], start = 3):
        col_texts.append(entry + BuildColSource(QA_data[index]))
    # Figure out how much space to add to each line, then add the
    # space and the list of numbers.
    lengths = [len(entry) for entry in col_texts]
    padding = [max(lengths) - length for length in lengths]

    # Turn the lists of numbers into lists of strings in which all the
    # numbers in each list are centred and aligned above each other.
    listofstrings = gen.AlignListPrint(numbers)

    for index, padlength in enumerate(padding):
        mod_lines.append(" * " + col_texts[index] + " " * padlength
                           + listofstrings[index])
    return(mod_lines)


def BuildColSource(col_tuple):
    '''Take the name of a column and its number.  Build a piece of text
    that gives the ordinal number and, if the column name is unique,
    give the column name too.  It is used in various routines to add
    information about the source of timing operations to the log file
    so that engineers can see where the data is being taken from.

        Parameters:
            col_tuple       (str, str)      Tuple of a one-word string
                                            with the column name and
                                            the column number.

        Returns:
            col_text        str             Descriptive text like
                                             "from the 2nd column: "
                                            or
                                             "from the 2nd column (speed): "
    '''
    (col_name, col_num) = col_tuple
    col_text = " from the " + gen.Enth(col_num) + " column: "
    if str(col_num) != col_name:
        # The user set a column name instead of a column number, add it.
        col_text = col_text[:-2] + " (" + col_name + "): "
    return(col_text)


def TransientLoss1(keyword, details, seg_area, settings_dict, file_name,
                  time_list, aero_times, line_triples,  log):
    '''Build lists of data to define the variation of a pressure
    loss with time.  This could be one of the following:
      * loss1: variable area and variable k-factors (+ve flow and
               -ve flow)
      * loss2: variable Atkinson resistance (+ve flow and -ve flow)
    They are both turned into

        Parameters:
            keyword         str             "Loss1" or "loss2".
            details         []              List of words on the line
                                            defining the loss, and the
                                            optional entries on the line.
            seg_area        float           Area in the segment
            settings_dict   {}              Dictionary of the run settings.
            file_name       str             The file name, for error messages.
            line_number     int             The line number, for error messages.
            time_list       []
            line_text       str             The entire line, including comments.
            line_triples    [int, str, str] Data for all the lines.
            log             handle          The handle of the logfile.

        Returns:
            times           []              A list of two np.arrays of
                                            times and speeds, that
                                            the calculation will
                                            interpolate with at each
                                            timestep.
            lines           [str]           A list of lines describing
                                            the fan operating times
                                            and corresponding speeds.
    '''
    time_accuracy = settings_dict["time_accuracy"]

    # Get the name of this loss and the nickname of the datasource that
    # its data comes from.
    (damper_name, nickname) = details[2:4]
    tr_index = details[-1]

    descrip1 = "a damper operating sequence"
    descrip2 = "damper"

    psi = settings_dict["psi"]
    units = settings_dict["units"]
    debug1 = settings_dict["debug1"]

    if keyword == "damper1":
        t_col, a_col, zbf_col, zfb_col = details[4:8]
        # Call a routine that extracts up to four columns from a datasource
        # and faults if it can't find them.
        result = GetTabulated(settings_dict, line_triples, tr_index, log,
                              True, nickname, t_col, a_col, zbf_col, zfb_col)
        if result is None:
            return(None)
        else:
            # We get a list of four lists and a list of QA data back.
            ((times, areas, zetas_bf, zetas_fb), QA_data) = result

            # Check for infinite resistances (zero areas) and negative
            # areas.
            result = AllPositive(a_col, areas, QA_data, nickname, descrip1,
                                 descrip2, "area", file_name, line_triples,
                                 tr_index, log)
            if result is None:
                return(None)
            # Check for negative values of +ve flow pressure loss factor.
            result = NotNegative(zbf_col, zetas_bf, QA_data, nickname, descrip1,
                                 descrip2, "k-factor", file_name,
                                 line_triples, tr_index, log)
            if result is None:
                return(None)
            # Check for negative values of -ve flow pressure loss factor.
            result = NotNegative(zbf_col, zetas_fb, QA_data, nickname, descrip1,
                                 descrip2, "k-factor", file_name,
                                 line_triples, tr_index, log)
            if result is None:
                return(None)

            # Check if we need to convert to SI units.
            if units == "us":
                areas = ListToSI("area", areas, debug1, log)

    elif keyword == "damper2":
        t_col, Rbf_col, Rfb_col = details[4:7]
        # Call a routine that extracts up to four columns from a datasource
        # and faults if it can't find them.
        result = GetTabulated(settings_dict, line_triples, tr_index, log,
                              True, nickname, t_col, Rbf_col, Rfb_col)
        if result is None:
            return(None)
        else:
            # We get a list of three lists and a list of QA data back.
            ((times, Rs_bf, Rs_fb), QA_data) = result

    else:
        # A new type of damper has been added that is not handled here.
        print('Need to add more code to PROC TransientLoss1 to handle "'
              + keyword + '".')
        gen.OopsIDidItAgain(log, file_name)

    # Check that the times start at zero and increase as we go through
    # the sequence.
    zerostart = True
    result = CheckTimesIncrease(t_col, times, QA_data, zerostart, nickname,
                                descrip1, descrip2,
                                file_name, line_triples, tr_index, log)
    if result is None:
        return(None)


    aero_time = settings_dict["aero_time"]
    aero_step = settings_dict["aero_step"]

    if keyword == "damper1":
        # Check if the values extend past the end of the run and if they
        # do not, extend them.  We add the math.isclose clause to preclude
        # weird floating point accuracy shenanigans that sometimes occur.
        if times[-1] <= aero_time or math.isclose(times[-1], aero_time):
            times.append(aero_time + aero_step)
            areas.append(areas[-1])
            zetas_bf.append(zetas_bf[-1])
            zetas_fb.append(zetas_fb[-1])

        # Build lists of the stagnation pressure loss coefficients that
        # apply to this segment's area.  These have linear interpolation
        # of area and k-factor between timesteps, and thus nonlinear
        # interpolation of resistance (= half rho zeta / area**2).

        stags_bf, plottables1 = StagsFromZetas(seg_area, -1., times, areas,
                                               zetas_bf, time_list, aero_times,
                                               psi)
        stags_fb, plottables2 = StagsFromZetas(seg_area, +1., times, areas,
                                               zetas_fb, time_list, aero_times,
                                               psi)

        # Build a list of np.arrays with how the damper properties vary
        # with time: area, zeta_bf, zeta_fb, R_bf, R_fb,
        plottables = (plottables1[0],     # Area     \
                      plottables1[1],     # zeta_bf  \
                      plottables2[1],     # zeta_fb  \
                      plottables1[2],     # R_bf     \
                      plottables2[2],     # R_fb     \
                     )

        # Get the lines of QA data that we write to the log file.
        descriptors = ['"Damper1" keyword operating rules',
                       "Operation times",
                       "Areas",
                       "+ve flow k-factors",
                       "-ve flow k-factors",]
        numbers = (times, areas, zetas_bf, zetas_fb)
        lines = DatasourceDescrip(descriptors, numbers, QA_data)
    elif keyword == "damper2":
        # Build lists of stagnation pressure loss coefficients at each
        # timestep.  These have linear interpolation of resistance
        # between timesteps.  We put in the first value for time zero.
        stags_bf = [Rs_bf[0]]
        stags_fb = [Rs_bf[0]]

        # Get a figure that turns Atkinson resistance into zetas.
        if times[-1] <= aero_time or math.isclose(times[-1], aero_time):
            times.append(aero_time + aero_step)
            Rs_bf.append(Rs_bf[-1])
            Rs_fb.append(Rs_fb[-1])

        adjustment = 2 * seg_area**2 / 1.2
        plot_Rs_bf = [Rs_bf[0]]
        plot_Rs_fb = [Rs_fb[0]]
        for time in time_list:
            R_bf = np.interp(time, times, Rs_bf)
            R_fb = np.interp(time, times, Rs_fb)
            if time in aero_times:
                # This is a plot timestep.  Store the values.
                plot_Rs_bf.append(R_bf)
                plot_Rs_fb.append(R_fb)
            stags_bf.append((1. - adjustment * R_bf) / psi)
            stags_fb.append((1. + adjustment * R_fb) / psi)


        # Build a list of np.arrays with how the damper properties vary
        # with time: R_bf and R_fb.
        plottables = (plot_Rs_bf, plot_Rs_fb)


        # Get the lines of QA data that we write to the log file.
        descriptors = ['"Damper2" keyword operating rules',
                       "Operation times",
                       "+ve flow Atkinson resistances",
                       "-ve flow Atkinson resistances"]
        numbers = (times, Rs_bf, Rs_fb)
        lines = DatasourceDescrip(descriptors, numbers, QA_data)

    # 'damper_details' has the following:
    #  * one np.array of the stagnation pressure loss coefficients at
    #    each timestep for +ve flow,
    #  * one np.array of the stagnation pressure loss coefficients at
    #    each timestep for -ve flow,
    #  * a list containing two or five np.arrays that have values of
    #    the damper's parameters at every plot time.
    #
    # We don't need a list of times because we have a value for every
    # timestep.
    #
    damper_details = [np.array(stags_bf), np.array(stags_fb), plottables]
    return(damper_name, damper_details, lines)


def ToAtkinsons(zeta, area):
    '''Take a pressure loss factor and an area and turn them into an
    Atkinson resistance at air density 1.2 kg/m^3.  Atkinson resistance
    is handier than pressure loss factors (k-factors) because you can't
    apply them to the wrong air velocity in fittings that have a change
    in area.

        Parameters:
            zeta            float           A pressure loss factor applied
                                            to the velocity in an area.
            area            float           The area that the velocity
                                            that the pressure loss factor
                                            occurs in.

        Returns:
            R               float           An Atkinson resistance in SI
                                            units (N-s^2/m^8).
    '''
    return(0.5 * 1.2 * zeta / area**2)


def ListToSI(key, values, debug1, log):
    '''Take a list of things in US units and a key to its conversion factor.
    Convert all the values in the list to SI units and return it.  We make
    a call to ConvertToSI in case these are temperature (32 deg F adjustment)
    and so that the conversions can all be logged if debug1 is True.

        Parameters:
            key             str             A key for the UScustomary.py
                                            script.
            values          []              A list of values in US units.
            debug1          bool            The debug Boolean set by the user.
            log             handle          The handle of the logfile.

        Returns:
            SI_values       []              A list of values in SI units.
    '''
    SI_values = [USc.ConvertToSI(key, val, debug1, log)[0] for val in values]
    return(SI_values)


def StagsFromZetas(seg_area, mult, times, areas, zetas, times_list,
                   aero_times, psi):
    '''Take a list of variations of k-factors and areas and a reference
    area (segment area).  Return a list of stagnation pressure loss
    factors at every calculation time step.  We have to do this at
    every time step because a linear variation of area and zeta leads
    to a nonlinear variation of stagnation pressure loss coefficient
    between the defined points.  We also build lists of area, zetas
    and Atkinson resistances at the print time steps and return a tuple
    of np.arrays with those three.

        Parameters:
            seg_area        float           The area of the segment.
            mult            float           +1 if this is for flow towards
                                            a junction.  -1 if it is for
                                            flow away from the junction.
            times           [float]         A list of times to interpolate
                                            between.
            areas           [float]         A list of areas to interpolate
                                            for.
            zetas           [float]         A list of pressure loss factors
                                            to interpolate for.
            times_list      [float]         A list of all the times being
                                            calculated at.
            aero_times      [float]         A list of all the times being
                                            plotted at.
            psi             float           The ratio of specific heats.

        Returns:
            stags          [float]          A list of stagnation pressure
                                            loss coefficients, one for
                                            each calculation time for
                                            for this flow direction.
            plottables     []               A list of three numpy arrays
                                            giving area, pressure loss
                                            coefficient and Atkinson
                                            resistance at each plot
                                            time step.
    '''
    stags = []
    # Create arrays to hold the values at the plot times.
    plot_areas = [areas[0]]
    plot_zetas = [zetas[0]]
    plot_Rs = [ToAtkinsons(zetas[0], areas[0])]
    seg_areasq = seg_area**2
    for time in times_list:
        area = np.interp(time, times, areas)
        zeta = np.interp(time, times, zetas)
        stag = (1. + mult * zeta * seg_areasq / area**2) / psi
        stags.append(stag)
        if time in aero_times:
            plot_areas.append(area)
            plot_zetas.append(zeta)
            plot_Rs.append(ToAtkinsons(zeta, area))
        plottables = (np.array(plot_areas),
                      np.array(plot_zetas),
                      np.array(plot_Rs))
    return(np.array(stags), plottables)


def AllPositive(col, values, QA_data, nickname, descrip1, descrip2,
                descrip3, file_name, line_triples, tr_index, log):
    '''Take a list of numbers and check that none of them are zero or
    negative.  Issue a detailed error message if one is.

        Parameters:
            col             str or int      The name of a column (str)
                                            or its number (int).
            values          []              List of numbers in the
                                            column of data.
            QA_data         []              A list of QA data from PROC
                                            GetTabulated, used to add
                                            descriptors to the lines.
            nickname        str             The nickname of the data-
                                            source.
            descrip1        str             Phrase describing what the
                                            numbers are for, e.g. "a
                                            damper operating sequence".
            descrip2        str             Word describing what the
                                            entity is, e.g. "damper".
            descrip3        str             Word describing what the
                                            column is to be used for,
                                            e.g. "area".
            file_name       str             The file name, for error messages.
            line_triples  [(int, str, str)] List of lines in the file.
            tr_index        int             Pointer to where the block
                                            starts in line_triples.
            log             handle          The handle of the logfile.

        Returns:
            result          bool            True if all the numbers are
                                            above zero, None if one is
                                            not.

        Errors:
            Aborts with 2581 if one or more zero or negative numbers
            are found in the list.
    '''
    minval = min(values)
    if minval <= 0.0:
        # Build some values for the error message and get the lines
        # we want to complain about.
        try:
            err_text1 = gen.Enth(int(col)) + ' column)\n> '
        except ValueError:
            err_text1 = 'column named\n> "' + col + '") '

        line_num1, discard, line1_text = line_triples[tr_index]
        line_num2, discard, line2_text = line_triples[QA_data[2]]

        # Figure out whether we have zero values or negative values and
        # how many.
        zerocount = len([value for value in values if value == 0.0])
        negcount = len([value for value in values if value < 0.0])
        dudcount = zerocount + negcount

        if math.isclose(minval, 0.0):
            err_text2 = " zero value"
        elif zerocount > 0:
            err_text2 = " -ve/zero value"
        else:
            err_text2 = " negative value"

        if dudcount == 1:
            err_text3 = 'a' + err_text2
        else:
            err_text3 =  str(dudcount) + err_text2 + 's'

        # A quick crib for the confused (me in a year or so):
        #  descrip1 == "a damper sequence"
        #  descrip2 == "damper"
        #  descrip3 == "area"
        err = ('> In the file named "' + file_name + '",\n'
               '> the datasource nicknamed "' + nickname + '" was\n'
               '> used to define ' + descrip1 + '.\n'
               '> The column assigned to ' + descrip3 + ' (the '
                 + err_text1 + 'had ' + err_text3
                 + ' in it, which is\n'
               '> not appropriate for ' + descrip3 + 's.\n'
               '> Please edit the file to use another column, use\n'
               '> a different datasource, or edit the datasource\n'
               '> to correct the value' + gen.Plural(dudcount) + '.'
              )
        gen.WriteError(2581, err, log)
        gen.ErrorOnTwoLines(line_num1, line1_text,
                            line_num2, line2_text, log, False, False, "Relevant")
        return(None)
    return(True)


def NotNegative(col, values, QA_data, nickname, descrip1, descrip2,
                descrip3, file_name, line_triples, tr_index, log):
    '''Take a list of numbers and check that none of them are negative.
    Only positive numbers and zero are allowed.  Issue a detailed error
    message if a negative number is found.

        Parameters:
            col             str or int      The name of a column (str)
                                            or its number (int).
            values          []              List of numbers in the
                                            column of data.
            QA_data         []              A list of QA data from PROC
                                            GetTabulated, used to add
                                            descriptors to the lines.
            nickname        str             The nickname of the data-
                                            source.
            descrip1        str             Phrase describing what the
                                            numbers are for, e.g. "a
                                            damper operating sequence".
            descrip2        str             Word describing what the
                                            entity is, e.g. "damper".
            descrip3        str             Word describing what the
                                            column is to be used for,
                                            e.g. "area".
            file_name       str             The file name, for error messages.
            line_triples  [(int, str, str)] List of lines in the file.
            tr_index        int             Pointer to where the block
                                            starts in line_triples.
            log             handle          The handle of the logfile.

        Returns:
            result          bool            True if all the numbers are
                                            above zero, None if one is
                                            not.

        Errors:
            Aborts with 2582 if one or more negative numbers are found
            in the list.
    '''
    if min(values) < 0.0:
        # Build some values for the error message and get the lines
        # we want to complain about.
        try:
            err_text1 = gen.Enth(int(col)) + ' column'
        except ValueError:
            err_text1 = 'column named "' + col + '"'

        line_num1, discard, line1_text = line_triples[tr_index]
        line_num2, discard, line2_text = line_triples[QA_data[2]]

        dudcount = len([value for value in values if value < 0.0])
        if dudcount == 1:
            err_text2 = 'a negative value'
        else:
            err_text2 =  str(dudcount) + ' negative values'

        err = ('> In the file named "' + file_name + '",\n'
               '> the datasource nicknamed "' + nickname + '" was\n'
               '> used to define ' + descrip1 + '.\n'
               '> The column assigned to ' + descrip3 + ' (the '
                 + err_text1 + ')\n'
               '> had ' + err_text2 + ' in it, which is not\n'
               '> appropriate for ' + descrip3 + 's.\n'
               '> Please edit the file to use another column, use\n'
               '> a different datasource, or edit the datasource\n'
               '> to correct the value' + gen.Plural(dudcount) + '.'
              )
        gen.WriteError(2582, err, log)
        gen.ErrorOnTwoLines(line_num1, line1_text,
                            line_num2, line2_text, log, False, False, "Relevant")
        return(None)
    return(True)


def CheckTimesIncrease(t_col, times, QA_data, zerostart, nickname, descrip1,
                       descrip2, file_name, line_triples, tr_index, log):
    '''Take a list of numbers and check that they increase as we go forward
    in the list.  If zerostart is True, complain if it doesn't start at
    zero.
        Parameters:
            t_col           str or int      The name of a column (str)
                                            or its number (int).
            times           []              List of times to check.
            QA_data         []              A list of QA data from PROC
                                            GetTabulated, used to add
                                            descriptors to the lines.
            zerostart       bool            True if we want the times
                                            to start at zero, False
                                            if we don't.
            nickname        str             The nickname of the data-
                                            source.
            descrip1        str             Phrase describing what the
                                            numbers are for, e.g. "a
                                            damper operating sequence".
            descrip2        str             Word describing what the
                                            entity is, e.g. "damper".
            descrip3        str             Word describing what the
                                            column is to be used for,
                                            e.g. "area".
            file_name       str             The file name, for error messages.
            line_triples  [(int, str, str)] List of lines in the file.
            tr_index        int             Pointer to where the block
                                            starts in line_triples.
            log             handle          The handle of the logfile.

        Returns:
            result          bool            True if all the numbers are
                                            above zero, None if one is
                                            not.

        Errors:
            Aborts with 2541 if the time series had only one entry.
            Aborts with 2542 if we want the series to start at zero
            and it did not.
            Aborts with 2543 if two times were the same or if a time
            was less than the previous time.
    '''
    # Build some values we may need in error messages.

    # First get a descriptor of the column.  If the user gave a number
    # to identify the column use its ordinal number, if the user gave
    # the name of a column use that.
    try:
        err_text = gen.Enth(int(t_col)) + ' column'
    except ValueError:
        err_text = 'column named "' + t_col + '"'

    line_num1, discard, line1_text = line_triples[tr_index]
    line_num2, discard, line2_text = line_triples[QA_data[2]]

    if len(times) < 2:
        err = ('> In the file named "' + file_name + '",\n'
               '> the datasource nicknamed "' + nickname + '" was used\n'
               '> to define ' + descrip1 + '.\n'
               '> Unfortunately the datasource had fewer than two\n'
               '> entries in it, which doth not a time-series make.\n'
               '> Please edit the file to use another column for\n'
               '> time, use a different datasource, or edit the\n'
               '> datasource to add more entries.'
              )
        gen.WriteError(2541, err, log)
        gen.ErrorOnTwoLines(line_num1, line1_text,
                            line_num2, line2_text, log, False)
        return(None)

    elif zerostart and not math.isclose(times[0], 0.0):
        err = ('> In the file named "' + file_name + '",\n'
               '> the datasource nicknamed "' + nickname + '" was used\n'
               '> to define ' + descrip1 + '.\n'
               '> Unfortunately the column assigned as time (the\n'
               '> ' + err_text + ') did not start at zero time.\n'
               '> Please edit the file to use another column for\n'
               '> time, use a different datasource, or edit the\n'
               '> datasource to start the column at time zero.'
              )
        gen.WriteError(2542, err, log)
        gen.ErrorOnTwoLines(line_num1, line1_text,
                            line_num2, line2_text, log, False)
        return(None)

    # If we get to here we have at least two entries and the first time
    # is zero.  Check that the times increase as we step through the
    # array.  We don't accept two times the same, that's a step change.
    for index, time in enumerate(times[1:]):
        prev = times[index]
        if time <= prev:
            err = ('> In the file named "' + file_name + '",\n'
                   '> the datasource nicknamed "' + nickname + '" was used\n'
                   '> to define ' + descrip1 + '.\n'
                   '> Unfortunately in the column assigned as time (the\n'
                   '> ' + err_text + ') the times were out of sequence.\n'
                   '> See the ' + PairText(index + 1, index + 2, "times", times)
                      + '.\n'
                   '> Please edit the file to use another column for\n'
                   '> time, use a different datasource, or edit the\n'
                   '> datasource to make the time values increase.'
                  )
            gen.WriteError(2543, err, log)
            gen.ErrorOnTwoLines(line_num1, line1_text, line_num2, line2_text,
                                log, False, False, "Relevant")
            return(None)
    return(True)


def CelVelFan(vals1, vals2, props1, props2, time,
              fric_app_num, psi, debug1, have_ftn):
    '''Take values of celerity and velocity in two cells on on either side
    of an axial or centrifugal fan.  Take the fixed properties in those cells
    and the fan characteristic.  Take the current time.
    Calculate the values of celerity and velocity at the two gridpoints at
    the connection in the next timestep, accounting for the pressure rise
    at the fan appropriate to the air velocity at the forward end of the
    first segment, which may differ from the air velocity at the back end
    of the second segment (if the fan is more like a blower than a fan).
    The fan characteristic is scaled by the speed multiplier derived from
    the current time (volume flow is multiplied by fractional speed,
    pressure rise is multiplied by the square of fractional speed).

        Parameters:
            vals1           []              Two values of celerity and two
                                            of velocity in the cell on one
                                            side of the fan.
            vals2           []              Two values of celerity and two
                                            of velocity in the cell on the
                                            other side of the fan.
            time            float           The current time.  Used to get
                                            the fan speed ratio from lists
                                            of time-speed pairs held in props1.
                                            Also used in messages when the
                                            calculation failed to converge.
            props1          []              A list of fixed properties for the
                                            cell on one side of the fan,
                                            including the fan characteristics
                                            and operating speeds.
            props2          []              A list of fixed properties for the
                                            cell on the other side of the fan.
            fric_app_num    int             A number representing the equation
                                            used to calculate friction in the
                                            calculation.
            psi             float           A constant in the characteristics
                                            equation.
            debug1          bool            The debug Boolean set by the user.
            have_ftn        bool            True if the Fortran routines can be
                                            used, False otherwise.

        Returns:
            c_N1            float           Celerity on one side of the fan
                                            in the next timestep.
            u_N1            float           Velocity on one side of the fan
                                            in the next timestep.
            c_N2            float           Celerity on the other side of the
                                            fan in the next timestep.
            u_N2            float           Velocity on the other side of the
                                            fan in the next timestep.
            success         str             Either the message "The solution
                                            converged", or a message with details
                                            of what went wrong.
    '''
    # Unpack the values we need for the two cells.  Fans are slightly
    # different to other two-way junctions because we know for a fact
    # that we will have one cell at the forward end of a segment and
    # the other cell at the back end of the adjacent segment.  We don't
    # have to include code to check whether we need to flip them.

    c_M1, c_L, u_M1, u_L, JFTRdrag1 = vals1
    c_M2, c_R, u_M2, u_R, JFTRdrag2 = vals2

    (back_end1, area1, d_h1, roughness1, rr1, rr371,
     fric_const1, dtdx1, stag_1) = props1[:9]
    (back_end2, area2, d_h2, roughness2, rr2, rr372,
     fric_const2, dtdx2, stag_2) = props2[:9]
    # A note about the stagnation loss terms: this boundary is at a
    # fan and there are no pressure loss factors (zeta) applied at
    # fan boundaries (because we want to see the true fan total
    # pressure rise in plots).  So the four stagnation pressure loss
    # coefficients are all equal and only depend on ratio of specific
    # heats, gamma: we don't need to get different values for different
    # flow directions.  This would no longer be the case if someone
    # forks the code and adds losses at fan junctions.


    # Get the properties of the fan and the rules for operating it.
    (fwd_vels, fwd_presses,
     rev_vels, rev_presses,
     times, speeds, fan_name) = props1[15:]

    # Get the current fan speed from the time and the pair of lists that
    # define the fan operation.  We use linear interpolation if we are
    # between points on the curve.
    speed = np.interp(time, times, speeds)
    speedsq = speed * abs(speed)

    # Multiply the flow of the fan characteristic by the speed fraction
    # at the current time step.
    # Multiply the pressure of the fan characteristic by the square of
    # speed fraction.
    if speed >= 0.0:
        fan_vels = fwd_vels * speed
        fan_presses = fwd_presses * speedsq
    else:
        fan_vels = rev_vels * speed
        fan_presses = rev_presses * speedsq

    def TwoWayMoC2Fan(x0):
        '''Define four equations that need to be satisfied to calculate new
        values of celerity and velocity on each side of a two-way junction
        that have the same area on the two sides and a fan between them.
        These are:
         * a forward characteristic in the first cell,
         * a backwards characteristic in the second cell,
         * one mass continuity equation at the node, and
         * the difference between stagnation pressures at both sides of
           the node.  The stagnation pressure factors may vary with time
           as the fan runs up and down.  They may also vary as the system
           characteristic seen by the fan changes and the fan moves to a
           different duty point on its characteristic.

        '''
        # Get the first approximations to the values out of the
        # arguments (these are from the previous timestep).
        c_N1, u_N1, c_N2, u_N2 = x0
        # Note that because this procedure is defined inside another
        # procedure, all the variables used in the enclosing procedure
        # are available to it (c_M1, u_M1 etc.).  The equivalent routine
        # in Fortran (ftn.twowaymoc2fan) takes a second array argument
        # with ??? entries.

        c_f1, u_f1 = BaseFwd(dtdx1, c_M1, u_M1, c_L, u_L, c_N1, u_N1)
        c_b2, u_b2 = BaseBack(dtdx2, c_M2, u_M2, c_R, u_R, c_N2, u_N2)
        Fanning_1 = FricFac(d_h1, roughness1, rr1, rr371, u_f1, fric_app_num)
        Fanning_2 = FricFac(d_h2, roughness2, rr2, rr372, u_b2, fric_app_num)

        # Get new values of the friction terms and add the traffic drag
        # term and jet fan thrust term.
        E_1dt = (fric_const1 * Fanning_1) * u_f1 * abs(u_f1)  \
                + JFTRdrag1
        E_2dt = (fric_const2 * Fanning_2) * u_b2 * abs(u_b2)  \
                + JFTRdrag2

        # Define functions for the two characteristics.  These both
        # should equal zero.
        char1 = psi * (c_N1 - c_f1) + (u_N1 - u_f1) + E_1dt
        char2 = psi * (c_N2 - c_b2) - (u_N2 - u_b2) - E_2dt

        # Define the continuity equation for compressible flow.  The way
        # the signs of the velocities are set up means that this should
        # also equal zero.  We don't need to include area because it is
        # the same in both cells on either side of the fan.
        continuity = u_N1 * (c_N1/c_N2)**psi - u_N2

        # Get the duty point of the fan at the current timestep.  We take
        # the value of u_N1 and find a fan pressure rise (c^2 in m^2/s^2)
        # that matches it.  As scipy.optimize.fsolve calculates, the fan
        # characteristic will move to the appropriate value for u_N1.
        if math.isclose(speed, 0.0):
            # The fan is turned off (speed zero).  Set the fan total
            # pressure rise term to zero.
            c_fansq = 0.0
            # print("Fan is off", time, c_fansq)
        elif speed > 0.0:
            # The fan is rotating in the forwards direction.  The
            # velocity upwind of the fan is u_N1.
            # print("range", speed, fan_vels[0], u_N1, fan_vels[-1])
            if fan_vels[0] <= u_N1 <= fan_vels[-1]:
                # We can interpolate the fan curve for the duty point.
                c_fansq = np.interp(u_N1, fan_vels, fan_presses)
                # print("Forwards np.interp", u_N1, c_fansq)
            elif u_N1 < fan_vels[0]:
                # We have moved beyond the fan characteristic and must
                # extrapolate into the reverse flow region.  We use the
                # (much slower) Python function that takes two points
                # and can be told to extrapolate.
                c_fansq = gen.Interpolate(fan_vels[1], fan_vels[0],
                                          fan_presses[1], fan_presses[0],
                                          u_N1, True)  # Allow extrapolation
                print("Forwards backflow", c_fansq)
            else:
                c_fansq = gen.Interpolate(fan_vels[-2], fan_vels[-1],
                                          fan_presses[-2], fan_presses[-1],
                                          u_N1, True)  # Allow extrapolation
                print("Forwards freewheeling", c_fansq)
        else:
            # The fan is rotating in the reverse direction.  Keep in mind
            # that the velocities in the characteristic have already been
            # multiplied by -1 before we got here, so if you visualize the
            # flow-pressure characteristic, think of it as having been
            # flipped on a vertical axis.  This is a restriction imposed
            # by the np.interp function, the X-values you feed it must
            # increase.  Also, u_N2 is the velocity at the fan inlet when
            # the fan is running in reverse.

            # print("range", speed, fan_vels[0], u_N2, fan_vels[-1])
            if fan_vels[0] <= u_N2 <= fan_vels[-1]:
               # We can interpolate the fan curve for the duty point.
                c_fansq = np.interp(u_N2, fan_vels, fan_presses)
                # print("Reverse np.interp", c_fansq)
            elif u_N2 > fan_vels[-1]:
                # We have moved beyond the fan characteristic and must
                # extrapolate into the reverse flow region.  We use the
                # (much slower) Python function that can extrapolate into
                # the fan's backflow region.
                c_fansq = gen.Interpolate(fan_vels[-1], fan_vels[-2],
                                           fan_presses[-1], fan_presses[-2],
                                           u_N2, True)  # Allow extrapolation
                # print("Reverse backflow", c_fansq)
            else:
                # The fan is freewheeling.
                c_fansq = gen.Interpolate(fan_vels[0], fan_vels[1],
                                           fan_presses[0], fan_presses[1],
                                           u_N2, True)  # Allow extrapolation
                # print("Reverse freewheeling", c_fansq)


        # Define an equation giving the difference between the stagnation
        # pressure at N1 and the stagnation pressure at N2.  This should
        # be zero.  The c_fansq term accounts for the fan pressure change.
        P_stag =  c_N1**2 + c_fansq + stag_1  * u_N1**2   \
                - c_N2**2           - stag_2  * u_N2**2

        return(char1, char2, continuity, P_stag)


    # Solve the system of equations using the values at the previous timestep
    # as our first estimate of the values at the next one.

    # First we set up an array of the values that scipy.optimize.fsolve changes.
    x0 = np.array((c_M1, u_M1, c_M2, u_M2))

    # Call scipy.optimize.fsolve, passing a routine and two or three lists.
    if have_ftn:
        # Set up an array of the values that scipy.optimize.fsolve treats
        # as constants.  We appear to need to pass these to Fortran in one
        # array, so it gets messy.
        args2 = np.append(
                  np.append(
                    np.array((psi, fric_app_num,
                             dtdx1, area1, d_h1, rr1, rr371, fric_const1,
                             roughness1, stag_1, JFTRdrag1,
                             c_M1, u_M1, c_L, u_L,
                             dtdx2, area2, d_h2, rr2, rr372, fric_const2,
                             roughness2, stag_2, JFTRdrag2,
                             c_M2, u_M2, c_R, u_R,
                             speed, len(fan_vels))),
                  fan_vels),
                fan_presses)

        result = scipy.optimize.fsolve(ftn.twowaymoc2fan, x0, args2,
                                       full_output = True)
    else:
        # We can use the routine defined here without having to pass the
        # fixed values in 'args2' above.  They are already in scope.
        result = scipy.optimize.fsolve(TwoWayMoC2Fan, x0, full_output = True)
    c_N1, u_N1, c_N2, u_N2 = result[0]
    success = result[-1]
    if success != "The solution converged.":
        # There was a glitch in the calculation.
        success = ('> scipy.optimize warned about fan "' + fan_name
                   + '" at ' + gen.FloatText(time) +' sec:\n  ' + success)

    # Set this True to print fault-finding data.
    if False:
        # Get the last values at the base of the characteristics for printing.
        c_f, u_f = BaseFwd(dtdx1, c_M1, u_M1, c_L, u_L, c_N1, u_N1)
        c_b, u_b = BaseBack(dtdx2, c_M2, u_M2, c_R, u_R, c_N2, u_N2)
        Fanning_f = FricFac(d_h1, roughness1, rr1, rr371, u_f, fric_app_num)
        Fanning_b = FricFac(d_h2, roughness2, rr2, rr372, u_b, fric_app_num)

        print("CelVelFan print\n"
          "  u_L =", "{:<28.8F}".format(u_L), "c_L =", "{:<18.8F}".format(c_L),
        "\n  u_f =", "{:<28.8F}".format(u_f), "c_f =", "{:<18.8F}".format(c_f),
        "\n  u_M1=", "{:<28.8F}".format(u_M1), "c_M1=", "{:<18.8F}".format(c_M1),
        "\n  u_N1=", "{:<28.8F}".format(u_N1), "c_N1=", "{:<18.8F}".format(c_N1),
        "\n  u_N2=", "{:<28.8F}".format(u_N2), "c_N2=", "{:<18.8F}".format(c_N2),
        "\n  u_M2=", "{:<28.8F}".format(u_M2), "c_M2=", "{:<18.8F}".format(c_M2),
        "\n  u_b =", "{:<28.8F}".format(u_b), "c_b =", "{:<18.8F}".format(c_b),
        "\n  u_R =", "{:<28.8F}".format(u_R),  "c_R =", "{:<18.8F}".format(c_R),
        "\n  Fanning_f =", "{:<10.8F}".format(Fanning_f),
            "Fanning_b =", "{:<10.8F}".format(Fanning_b))
    return(c_N1, u_N1, c_N2, u_N2, success)


def CelVel2(vals1, vals2, props1, props2, time, fric_app_num, psi,
            debug1, have_ftn):
    '''Take values of celerity, velocity traffic drag and jet fan thrust
    in two cells on on either side of a join, adit or area change.
    Take the fixed properties in those cells.
    Calculate the values of celerity and velocity at the two gridpoints at
    the connection in the next timestep.

    The calculation uses scipy.optimize.fsolve.  In some circumstances
    this routine will fail to get a converged solution.  These are trapped
    and the error messages printed to the screen and log file for
    diagnostics.

        Parameters:
            vals1           []              Two values of celerity, two
                                            of velocity, one value of
                                            drag (traffic and jet fans)
                                            in the cell on one side of
                                            the entity.
            vals2           []              Two values of celerity, two
                                            of velocity, one value of
                                            drag (traffic and jet fans)
                                            in the cell on the other
                                            side of the entity.
            props1          []              A list of fixed properties for the
                                            cell on one side of a node.  Used
                                            in the calculation.
            props2          []              A list of fixed properties for the
                                            cell on the other side of a node.
            time            float           The current time.  Used in messages
                                            when the calculation failed to
                                            converge.
            fric_app_num    int             A number representing the equation
                                            used to calculate friction in the
                                            calculation.
            psi             float           A constant in the characteristics
                                            equation.
            debug1          bool            The debug Boolean set by the user.
            have_ftn        bool            True if the Fortran routines can be
                                            used, False otherwise.

        Returns:
            c_N1            float           Celerity on one side of the entity
                                            in the next timestep.
            u_N1            float           Velocity on one side of the entity
                                            in the next timestep.
            c_N2            float           Celerity on the other side of the
                                            entity in the next timestep.
            u_N2            float           Velocity on the other side of the
                                            entity in the next timestep.
            success         str             Either the message "The solution
                                            converged", or a message with details
                                            of what went wrong.
    '''
    # Unpack the values we need for the two cells.  We use c_L and U_L
    # for the values at the gridpoint away from the junction because the
    # way this function works we want a positive value of velocity
    # going towards the junction.
    c_M1, c_L1, u_M1, u_L1, JFTRdrag1 = vals1
    c_M2, c_L2, u_M2, u_L2, JFTRdrag2 = vals2
    (back_end1, area1, d_h1, roughness1, rr1, rr371,
     fric_const1, dtdx1, stag_away1, stag_twd1) = props1[:10]
    (back_end2, area2, d_h2, roughness2, rr2, rr372,
     fric_const2, dtdx2, stag_away2, stag_twd2) = props2[:10]

    # Check if we have to adjust the value of velocity to account for
    # segment orientations at the junction and choose which stagnation
    # coefficient to use, based on the flow direction at the previous
    # timestep.
    u_L1, u_M1, JFTRdrag1, stag_1 = FlipCell(back_end1, u_L1, u_M1,
                                           JFTRdrag1, stag_twd1, stag_away1)
    u_L2, u_M2, JFTRdrag2, stag_2 = FlipCell(back_end2, u_L2, u_M2,
                                             JFTRdrag2, stag_twd2, stag_away2)

    # Define a sub-function to solve the flow.  This may be called below
    # or a Fortran function with the same calculation may be used.
    def TwoWayMoC2b(x0):
        '''Define four equations that need to be satisfied to calculate new
        values of celerity and velocity on each side of a two-way junction
        that may have different areas on the two sides.  These are:
         * two forward characteristics in the two cells,
         * one mass continuity equation at the node, and
         * the difference between stagnation pressures at both sides of
           the node.
        All four equations are expressed such that when the values
        of celerity and velocity at the next timestep are correct,
        the four equations sum up to zero (scipy.optimize.fsolve
        solves the system by homing in on the properties that give
        zero for the equations, like a more complicated goal seek
        in a spreadsheet).

        The function calculates a new value of friction factor during
        every iteration, which might be a bit over the top.  We'll wait
        and see how slow the whole thing is and what effect calculating
        c_f from u_M1 and u_M2 has later.
        '''
        # Get the first approximations to the values out of the
        # arguments (these are from the previous timestep).
        c_N1, u_N1, c_N2, u_N2 = x0
        # Note that because this procedure is defined inside another
        # procedure, all the variables used in the enclosing procedure
        # are available to it (c_M1, u_M1 etc.).  The equivalent routine
        # in Fortran (ftn.twowaymoc2a) takes a second array argument
        # with all the entries that this routine doesn't need.

        c_f1, u_f1 = BaseFwd(dtdx1, c_M1, u_M1, c_L1, u_L1, c_N1, u_N1)
        c_f2, u_f2 = BaseFwd(dtdx2, c_M2, u_M2, c_L2, u_L2, c_N2, u_N2)
        Fanning_1 = FricFac(d_h1, roughness1, rr1, rr371, u_f1, fric_app_num)
        Fanning_2 = FricFac(d_h2, roughness2, rr2, rr372, u_f2, fric_app_num)

        # Get new values of the friction terms and add the traffic drag
        # term and jet fan thrust term.
        E_1dt = (fric_const1 * Fanning_1) * u_f1 * abs(u_f1)  \
                + JFTRdrag1
        E_2dt = (fric_const2 * Fanning_2) * u_f2 * abs(u_f2)  \
                + JFTRdrag2

        # Define functions for the two characteristics.  These both
        # should equal zero.
        char1 = psi * (c_N1 - c_f1) + (u_N1 - u_f1) + E_1dt
        char2 = psi * (c_N2 - c_f2) + (u_N2 - u_f2) + E_2dt

        # Define the continuity equation for compressible flow.  The way
        # the signs of the velocities are set up means that this should
        # also equal zero.
        continuity = area1 * u_N1 * (c_N1/c_N2)**psi + area2 * u_N2
        # The following expression of the continuity equation is clearer
        # but causes stability problems for some reason.
        #   continuity = area1 * u_N1 * c_N1**psi + area2 * u_N2 * c_N2**psi

        # Define an equation giving the difference between the stagnation
        # pressure at N1 and the stagnation pressure at N2.  This should
        # be zero.
        P_stag = c_N1**2 + stag_1 * u_N1**2 - c_N2**2 - stag_2 * u_N2**2

        # A clearer expression for this is:
        #
        #   P_stag = psi*(c_N1**2 - c_N2**2) + (1 - zeta_1)*u_N1**2 - (1 + zeta_2)*u_N2**2
        #
        # where 'psi' is 2 / (gamma - 1).
        #
        # But it is more efficient to incorporate the zeta terms and psi
        # into stag_1 and stag_2 outside the loop.

        return(char1, char2, continuity, P_stag)


    # Solve the system of equations using the values at the previous timestep
    # as our first estimate of the values at the next one.

    # First we set up an array of the values that scipy.optimize.fsolve changes.
    x0 = np.array((c_M1, u_M1, c_M2, u_M2))

    # Call scipy.optimize.fsolve, passing a routine and two or three lists.
    if have_ftn:
        # Set up an array of the values that scipy.optimize.fsolve treats
        # as constants.  We need to pass these to Fortran explicitly.
        args2 = np.array((psi, float(fric_app_num),
                          dtdx1, area1, d_h1, rr1, rr371, fric_const1,
                          roughness1, stag_1, JFTRdrag1,
                          c_M1, u_M1, c_L1, u_L1,
                          dtdx2, area2, d_h2, rr2, rr372, fric_const2,
                          roughness2, stag_2, JFTRdrag2,
                          c_M2, u_M2, c_L2, u_L2))
        result = scipy.optimize.fsolve(ftn.twowaymoc2a, x0, args2,
                                       full_output = True)
    else:
        # We can use the routine defined here without having to pass the
        # fixed values in 'args2' above.  They are already in scope.
        result = scipy.optimize.fsolve(TwoWayMoC2b, x0, full_output = True)
    c_N1, u_N1, c_N2, u_N2 = result[0]
    success = result[-1]
    if success != "The solution converged.":
        # The solution had problems and warned about them.  Turn it into
        # a string that can be written to the screen and to the log file
        # warning about a possible dud result.
        # Check if this location is a named entity like a damper.
        if len(props1) > 14:
            # This is a named entity. The last two entries are the
            # type and the name.
            etype, name = props1[-2:]
            if etype == "d":
                success = ('> scipy.optimize warned about damper "'
                             + name + '" at '+ gen.FloatText(time)
                             + ' sec:\n  ' + success)
            else:
                # Put in something for entities of unknown type.
                success = ('> scipy.optimize warned about entity "'
                             + name + '" at '+ gen.FloatText(time)
                             + ' sec:\n  ' + success)
        else:
            success = ('> scipy.optimize warned in CelVel2 at '
                         + gen.FloatText(time) +
                       ' sec, seg_IDs ' + str(int(props1[11])) + " & "
                         + str(-int(props2[11])) + '".\n  ' + success)

    # Now restore the signs of the velocities we switched, if we need to.
    if back_end1:
            u_N1 = -u_N1
    if back_end2:
            u_N2 = -u_N2

    # Set this True to print fault-finding data.
    if False:
        # Get the last values at the base of the characteristics for printing.
        c_f, u_f = BaseFwd(dtdx1, c_M1, u_M1, c_L, u_L, c_N1, u_N1)
        c_b, u_b = BaseFwd(dtdx2, c_M2, u_M2, c_R, u_R, c_N2, u_N2)
        Fanning_f = FricFac(d_h1, roughness1, rr1, rr371, u_f, fric_app_num)
        Fanning_b = FricFac(d_h2, roughness2, rr2, rr372, u_b, fric_app_num)

        print("CelVel2a print\n"
          "  u_L =", "{:<28.8F}".format(u_L), "c_L =", "{:<18.8F}".format(c_L),
        "\n  u_f =", "{:<28.8F}".format(u_f), "c_f =", "{:<18.8F}".format(c_f),
        "\n  u_M1=", "{:<28.8F}".format(u_M1), "c_M1=", "{:<18.8F}".format(c_M1),
        "\n  u_N1=", "{:<28.8F}".format(u_N1), "c_N1=", "{:<18.8F}".format(c_N1),
        "\n  u_N2=", "{:<28.8F}".format(u_N2), "c_N2=", "{:<18.8F}".format(c_N2),
        "\n  u_M2=", "{:<28.8F}".format(u_M2), "c_M2=", "{:<18.8F}".format(c_M2),
        "\n  u_b =", "{:<28.8F}".format(u_b), "c_b =", "{:<18.8F}".format(c_b),
        "\n  u_R =", "{:<28.8F}".format(u_R),  "c_R =", "{:<18.8F}".format(c_R),
        "\n  Fanning_f =", "{:<10.8F}".format(Fanning_f),
            "Fanning_b =", "{:<10.8F}".format(Fanning_b))
    return(c_N1, u_N1, c_N2, u_N2, success)


def FlipCell(back_end, u_L, u_M, JFTRdrag, stag_twd, stag_away):
    '''Take a Boolean that is True if a connection at a node is at
    the back end of a segment.  If it is, change the sign of the
    two air velocities u_L and u_M and the drag term (traffic plus
    jet fans) so that the cell behaves as if the cell is at the
    forward end. Then choose whether to calculate with the stagnation
    term for flow towards the forward or away from the forward end,
    based on the value of u_M.

        Parameters:
            back_end    bool            True if the back end is attached,
                                        False if the forward end is.
            u_L         float           Velocity at the left gridpoint
                                        in the current timestep.
            u_M         float           Velocity at the middle gridpoint
                                        in the current timestep.
            JFTRdrag    float           Traffic drag and jet fan thrust
                                        term in the cell.
            stag_twd    float           Multiplier on the v**2 term in the
                                        stagnation pressure equation for
                                        flow towards from the junction.
            stag_away   float           Multiplier on the v**2 term in the
                                        stagnation pressure equation for
                                        flow away from the junction.
        Returns:
            u_L, u_M    floats          Velocity values, possibly switched.
            JFTRdrag    float           Traffic drag and jet fan thrust
                                        term in the cell, possibly
                                        multiplied by -1.
            stag2use    float           Multiplier on the v**2 term to
                                        use in the calculation at this
                                        time (depends on the sign of
                                        the velocity, which may change
                                        at each timestep).  Represents
                                        fixed pressure losses at the
                                        end of this cell attached to
                                        the junction.
    '''
    if back_end:
        # The branch is connected at its back end.  Multiply the velocities
        # by -1 so that positive airflows are towards the junction.
        # Multiply the traffic drag and jet fan thrust terms by -1
        # so that they operate correctly.
         u_L,  u_M,  JFTRdrag =  -u_L, -u_M, -JFTRdrag

    # Figure out which pressure loss factor to apply, depending on
    # the sign of the velocity at the previous time step.
    if u_M >= 0.0:
        stag2use =  stag_twd
    else:
        stag2use =  stag_away
    return(u_L, u_M, JFTRdrag, stag2use)


def CelVel3(vals1, vals2, vals3, props1, props2, props3,
            fric_app_num, psi, debug1, have_ftn):
    '''Take values of celerity and velocity in three cells at a join or
    node.  Take the fixed properties in those cells.  Calculate the
    values of celerity and velocity at the three gridpoints at the
    connection in the next timestep.

    The calculation uses scipy.optimize.fsolve.  In some circumstances
    this routine will print messages to the screen or fail to get to
    a solution.  These are not trapped.

        Parameters:
            vals1           []              Two values of celerity, two
                                            of velocity, one value of
                                            drag (traffic and jet fans)
                                            in the cell on one side of
                                            the entity.
            vals2           []              Two values of celerity, two
                                            of velocity, one value of
                                            drag (traffic and jet fans)
                                            in the cell on another side
                                            side of the entity.
            vals3           []              Two values of celerity, two
                                            of velocity, one value of
                                            drag (traffic and jet fans)
                                            in the cell on another side
                                            side of the entity.
            props1          []              A list of fixed properties for
                                            the first cell attached to a node.
                                            Used in the calculation.
            props2          []              A list of fixed properties for
                                            the second cell attached to a node.
            props3          []              A list of fixed properties for
                                            the third cell attached to a node.
            fric_app_num    int             A number representing the equation
                                            used to calculate friction in the
                                            calculation.
            psi             float           A constant in the characteristics
                                            equation.
            debug1          bool            The debug Boolean set by the user.
            have_ftn        bool            True if the Fortran routines can be
                                            used, False otherwise.

        Returns:
            c_N1            float           Celerity at the gridpoint at the
                                            node in the first cell in the
                                            next timestep.
            u_N1            float           Velocity at the gridpoint at the
                                            node in the first cell in the
                                            next timestep.
            c_N2            float           Celerity at the gridpoint at the
                                            node in the second cell in the
                                            next timestep.
            u_N2            float           Velocity at the gridpoint at the
                                            node in the second cell in the
                                            next timestep.
            c_N3            float           Celerity at the gridpoint at the
                                            node in the third cell in the
                                            next timestep.
            u_N3            float           Velocity at the gridpoint at the
                                            node in the third cell in the
                                            next timestep.
    '''
    # Unpack the values we need for the two cells.  We use c_L and U_L
    # for the values at the gridpoint away from the junction because the
    # way this function works we want a positive value of velocity is
    # going towards the junction.
    c_M1, c_L1, u_M1, u_L1, JFTRdrag1 = vals1
    c_M2, c_L2, u_M2, u_L2, JFTRdrag2 = vals2
    c_M3, c_L3, u_M3, u_L3, JFTRdrag3 = vals3
    (back_end1, area1, d_h1, roughness1, rr1, rr371,
     fric_const1, dtdx1, stag_away1, stag_twd1) = props1[:10]
    (back_end2, area2, d_h2, roughness2, rr2, rr372,
     fric_const2, dtdx2, stag_away2, stag_twd2) = props2[:10]
    (back_end3, area3, d_h3, roughness3, rr3, rr373,
     fric_const3, dtdx3, stag_away3, stag_twd3) = props3[:10]

    # Check if we have to adjust the value of velocity to account for
    # segment orientations at the junction and choose which stagnation
    # coefficient to use.
    # Check if we have to adjust the value of velocity to account for
    # segment orientations at the junction and choose which stagnation
    # coefficient to use, based on the flow direction at the previous
    # timestep.
    u_L1, u_M1, JFTRdrag1, stag_1 = FlipCell(back_end1, u_L1, u_M1,
                                           JFTRdrag1, stag_twd1, stag_away1)
    u_L2, u_M2, JFTRdrag2, stag_2 = FlipCell(back_end2, u_L2, u_M2,
                                             JFTRdrag2, stag_twd2, stag_away2)
    u_L3, u_M3, JFTRdrag3, stag_3 = FlipCell(back_end3, u_L3, u_M3,
                                           JFTRdrag3, stag_twd3, stag_away3)

    # Define the sub-procedure used in the call to scipy.optimize.fsolve.
    def ThreeWayMoC2b(x0):
        '''Define six equations that need to be satisfied to calculate new
        values of celerity and velocity at three gridpoints in cells that
        join together at a three-way junction.  These are:
         * three forward characteristics in the three cells,
         * one mass continuity equation at the node, and
         * two calculations of stagnation pressures at the node.
        All six equations are expressed such the correct result is zero,
        which is how scipy.optimize.fsolve can solve the system.
        '''
        # Get the first approximations to the values out of the
        # arguments (these are from the previous timestep).
        c_N1, u_N1, c_N2, u_N2, c_N3, u_N3 = x0

        c_f1, u_f1 = BaseFwd(dtdx1, c_M1, u_M1, c_L1, u_L1, c_N1, u_N1)
        c_f2, u_f2 = BaseFwd(dtdx2, c_M2, u_M2, c_L2, u_L2, c_N2, u_N2)
        c_f3, u_f3 = BaseFwd(dtdx3, c_M3, u_M3, c_L3, u_L3, c_N3, u_N3)

        Fanning_1 = FricFac(d_h1, roughness1, rr1, rr371, u_f1, fric_app_num)
        Fanning_2 = FricFac(d_h2, roughness2, rr2, rr372, u_f2, fric_app_num)
        Fanning_3 = FricFac(d_h3, roughness3, rr3, rr373, u_f3, fric_app_num)

        # Get new values of the friction terms and add the traffic drag
        # term and jet fan thrust term.
        E_1dt = (fric_const1 * Fanning_1) * u_f1 * abs(u_f1)  \
                + JFTRdrag1
        E_2dt = (fric_const2 * Fanning_2) * u_f2 * abs(u_f2)  \
                + JFTRdrag2
        E_3dt = (fric_const3 * Fanning_3) * u_f3 * abs(u_f3)  \
                + JFTRdrag3

        # Define functions for the three characteristics.
        char1 = psi * (c_N1 - c_f1) + (u_N1 - u_f1) + E_1dt
        char2 = psi * (c_N2 - c_f2) + (u_N2 - u_f2) + E_2dt
        char3 = psi * (c_N3 - c_f3) + (u_N3 - u_f3) + E_3dt

        # Define the continuity equation for compressible flow in the
        # three branches.
        continuity = area1 * u_N1 * (c_N1/c_N2)**psi + area2 * u_N2 \
                   + area3 * u_N3 * (c_N3/c_N2)**psi

        # There are terms in the stagnation pressure equations that we use
        # in more than one equation.  Put them into one variable.
        stag1_term = c_N1**2 + stag_1 * u_N1**2

        # Define two equations giving the difference between the stagnation
        # pressures in two pairs of the three gridpoints at the node.
        P_stag1 = stag1_term - c_N2**2 - stag_2*u_N2**2
        P_stag2 = stag1_term - c_N3**2 - stag_3*u_N3**2

        return[char1, char2, char3, continuity, P_stag1, P_stag2]

    # Solve the system of equations using the values at the previous timestep
    # as our first estimate of the values at the next one.
    x0 = np.array((c_M1, u_M1, c_M2, u_M2, c_M3, u_M3))
    if have_ftn:
        # Make a list of all the values that the Fortran version of this
        # routine will need to be told.  These are in scope for the Python
        # version of the solver (above), so we only need this array here.
        args3 = np.array((psi, float(fric_app_num),
                          dtdx1, area1, d_h1, rr1, rr371, fric_const1,
                          roughness1, stag_1, JFTRdrag1,
                          c_M1, u_M1, c_L1, u_L1,
                          dtdx2, area2, d_h2, rr2, rr372, fric_const2,
                          roughness2, stag_2, JFTRdrag2,
                          c_M2, u_M2, c_L2, u_L2,
                          dtdx3, area3, d_h3, rr3, rr373, fric_const3,
                          roughness3, stag_3, JFTRdrag3,
                          c_M3, u_M3, c_L3, u_L3))
        (c_N1, u_N1, c_N2, u_N2,
         c_N3, u_N3) = scipy.optimize.fsolve(ftn.threewaymoc2a, x0, args3)
    else:
        (c_N1, u_N1, c_N2, u_N2,
         c_N3, u_N3) = scipy.optimize.fsolve(ThreeWayMoC2b, x0)

    # Now restore the signs of the velocities we switched, if we need to.
    if back_end1:
            u_N1 = -u_N1
    if back_end2:
            u_N2 = -u_N2
    if back_end3:
            u_N3 = -u_N3

    return(c_N1, u_N1, c_N2, u_N2, c_N3, u_N3)


def CelVel4(vals1, vals2, vals3, vals4, props1, props2, props3, props4,
            fric_app_num, psi, debug1, have_ftn):
    '''Take values of celerity and velocity in four cells at a join or
    node.  Take the fixed properties in those cells.  Calculate the
    values of celerity and velocity at the four gridpoints at the
    connection in the next timestep.

    The calculation uses scipy.optimize.fsolve.  In some circumstances
    this routine will print messages to the screen or fail to get to
    a solution.  These are not trapped.

        Parameters:
            vals1           []              Two values of celerity, two
                                            of velocity, one value of
                                            drag (traffic and jet fans)
                                            in the cell on one side of
                                            the entity.
            vals2           []              Two values of celerity, two
                                            of velocity, one value of
                                            drag (traffic and jet fans)
                                            in the cell on another side
                                            side of the entity.
            vals3           []              Two values of celerity, two
                                            of velocity, one value of
                                            drag (traffic and jet fans)
                                            in the cell on another side
                                            side of the entity.
            vals4           []              Two values of celerity, two
                                            of velocity, one value of
                                            drag (traffic and jet fans)
                                            in the cell on another side
                                            side of the entity.
            props1          []              A list of fixed properties for
                                            the first cell attached to a node.
                                            Used in the calculation.
            props2          []              A list of fixed properties for
                                            the second cell attached to a node.
            props3          []              A list of fixed properties for
                                            the third cell attached to a node.
            props4          []              A list of fixed properties for
                                            the fourth cell attached to a node.
            fric_app_num    int             A number representing the equation
                                            used to calculate friction in the
                                            calculation.
            psi             float           A constant in the characteristics
                                            equation.
            debug1          bool            The debug Boolean set by the user.
            have_ftn        bool            True if the Fortran routines can be
                                            used, False otherwise.

        Returns:
            c_N1            float           Celerity at the gridpoint at the
                                            node in the first cell in the
                                            next timestep.
            u_N1            float           Velocity at the gridpoint at the
                                            node in the first cell in the
                                            next timestep.
            c_N2            float           Celerity at the gridpoint at the
                                            node in the second cell in the
                                            next timestep.
            u_N2            float           Velocity at the gridpoint at the
                                            node in the second cell in the
                                            next timestep.
            c_N3            float           Celerity at the gridpoint at the
                                            node in the third cell in the
                                            next timestep.
            u_N3            float           Velocity at the gridpoint at the
                                            node in the third cell in the
                                            next timestep.
            c_N4            float           Celerity at the gridpoint at the
                                            node in the fourth cell in the
                                            next timestep.
            u_N4            float           Velocity at the gridpoint at the
                                            node in the fourth cell in the
                                            next timestep.
    '''
    # Unpack the values we need for the two cells.  We use c_L and U_L
    # for the values at the gridpoint away from the junction because the
    # way this function works we want a positive value of velocity is
    # going towards the junction.
    c_M1, c_L1, u_M1, u_L1, JFTRdrag1 = vals1
    c_M2, c_L2, u_M2, u_L2, JFTRdrag2 = vals2
    c_M3, c_L3, u_M3, u_L3, JFTRdrag3 = vals3
    c_M4, c_L4, u_M4, u_L4, JFTRdrag4 = vals4

    (back_end1, area1, d_h1, roughness1, rr1, rr371,
     fric_const1, dtdx1, stag_away1, stag_twd1) = props1[:10]
    (back_end2, area2, d_h2, roughness2, rr2, rr372,
     fric_const2, dtdx2, stag_away2, stag_twd2) = props2[:10]
    (back_end3, area3, d_h3, roughness3, rr3, rr373,
     fric_const3, dtdx3, stag_away3, stag_twd3) = props3[:10]
    (back_end4, area4, d_h4, roughness4, rr4, rr374,
     fric_const4, dtdx4, stag_away4, stag_twd4) = props4[:10]

    # Check if we have to adjust the value of velocity to account for
    # segment orientations at the junction and choose which stagnation
    # coefficient to use.
    u_L1, u_M1, JFTRdrag1, stag_1 = FlipCell(back_end1, u_L1, u_M1,
                                           JFTRdrag1, stag_twd1, stag_away1)
    u_L2, u_M2, JFTRdrag2, stag_2 = FlipCell(back_end2, u_L2, u_M2,
                                             JFTRdrag2, stag_twd2, stag_away2)
    u_L3, u_M3, JFTRdrag3, stag_3 = FlipCell(back_end3, u_L3, u_M3,
                                           JFTRdrag3, stag_twd3, stag_away3)
    u_L4, u_M4, JFTRdrag4, stag_4 = FlipCell(back_end4, u_L4, u_M4,
                                             JFTRdrag4, stag_twd4, stag_away4)

    # Define the sub-procedure used in the call to scipy.optimize.fsolve.
    def FourWayMoC2b(x0):
        '''Define eight equations that need to be satisfied to calculate new
        values of celerity and velocity at four gridpoints in cells that
        join together at a four-way junction.  These are:
         * four forward characteristics in the four cells,
         * one mass continuity equation at the node, and
         * three calculations of stagnation pressures at the node.
        All eight equations are expressed such the correct result is zero,
        which is how scipy.optimize.fsolve can solve the system.
        '''
        # Get the first approximations to the values out of the
        # arguments (these are from the previous timestep).
        c_N1, u_N1, c_N2, u_N2, c_N3, u_N3, c_N4, u_N4 = x0

        c_f1, u_f1 = BaseFwd(dtdx1, c_M1, u_M1, c_L1, u_L1, c_N1, u_N1)
        c_f2, u_f2 = BaseFwd(dtdx2, c_M2, u_M2, c_L2, u_L2, c_N2, u_N2)
        c_f3, u_f3 = BaseFwd(dtdx3, c_M3, u_M3, c_L3, u_L3, c_N3, u_N3)
        c_f4, u_f4 = BaseFwd(dtdx4, c_M4, u_M4, c_L4, u_L4, c_N4, u_N4)

        Fanning_1 = FricFac(d_h1, roughness1, rr1, rr371, u_f1, fric_app_num)
        Fanning_2 = FricFac(d_h2, roughness2, rr2, rr372, u_f2, fric_app_num)
        Fanning_3 = FricFac(d_h3, roughness3, rr3, rr373, u_f3, fric_app_num)
        Fanning_4 = FricFac(d_h4, roughness4, rr4, rr374, u_f4, fric_app_num)

        # Get new values of the friction terms and add the traffic drag
        # term and jet fan thrust term.
        E_1dt = (fric_const1 * Fanning_1) * u_f1 * abs(u_f1)  \
                + JFTRdrag1
        E_2dt = (fric_const2 * Fanning_2) * u_f2 * abs(u_f2)  \
                + JFTRdrag2
        E_3dt = (fric_const3 * Fanning_3) * u_f3 * abs(u_f3)  \
                + JFTRdrag3
        E_4dt = (fric_const4 * Fanning_4) * u_f4 * abs(u_f4)  \
                + JFTRdrag4

        # Define functions for the four characteristics.
        char1 = psi * (c_N1 - c_f1) + (u_N1 - u_f1) + E_1dt
        char2 = psi * (c_N2 - c_f2) + (u_N2 - u_f2) + E_2dt
        char3 = psi * (c_N3 - c_f3) + (u_N3 - u_f3) + E_3dt
        char4 = psi * (c_N4 - c_f4) + (u_N4 - u_f4) + E_4dt

        # Define the continuity equation for compressible flow in the
        # four branches.
        continuity = area1 * u_N1 * (c_N1/c_N2)**psi + area2 * u_N2 \
                   + area3 * u_N3 * (c_N3/c_N2)**psi  \
                   + area4 * u_N4 * (c_N4/c_N2)**psi

        # There are terms in the stagnation pressure equations that we use
        # in more than one equation.  Put them into one variable.
        stag1_term = c_N1**2 + stag_1 * u_N1**2

        # Define three equations giving the difference between the stagnation
        # pressures in three pairs of gridpoints at the node.
        P_stag1 = stag1_term - c_N2**2 - stag_2*u_N2**2
        P_stag2 = stag1_term - c_N3**2 - stag_3*u_N3**2
        P_stag3 = stag1_term - c_N4**2 - stag_4*u_N4**2

        return[char1, char2, char3, char4, continuity,
               P_stag1, P_stag2, P_stag3]

    # Solve the system of equations using the values at the previous timestep
    # as our first estimate of the values at the next one.
    x0 = np.array((c_M1, u_M1, c_M2, u_M2, c_M3, u_M3, c_M4, u_M4))
    if have_ftn:
        args4 = np.array((psi, float(fric_app_num),
                          dtdx1, area1, d_h1, rr1, rr371, fric_const1,
                          roughness1, stag_1, JFTRdrag1,
                          c_M1, u_M1, c_L1, u_L1,
                          dtdx2, area2, d_h2, rr2, rr372, fric_const2,
                          roughness2, stag_2, JFTRdrag2,
                          c_M2, u_M2, c_L2, u_L2,
                          dtdx3, area3, d_h3, rr3, rr373, fric_const3,
                          roughness3, stag_3, JFTRdrag3,
                          c_M3, u_M3, c_L3, u_L3,
                          dtdx4, area4, d_h4, rr4, rr374, fric_const4,
                          roughness4, stag_4, JFTRdrag4,
                          c_M4, u_M4, c_L4, u_L4))
        (c_N1, u_N1, c_N2, u_N2, c_N3, u_N3,
         c_N4, u_N4) = scipy.optimize.fsolve(ftn.fourwaymoc2a, x0, args4)
    else:
        (c_N1, u_N1, c_N2, u_N2, c_N3, u_N3,
         c_N4, u_N4) = scipy.optimize.fsolve(FourWayMoC2b, x0)

    # Now restore the signs of the velocities we switched, if we need to.
    if back_end1:
            u_N1 = -u_N1
    if back_end2:
            u_N2 = -u_N2
    if back_end3:
            u_N3 = -u_N3
    if back_end4:
            u_N4 = -u_N4

    return(c_N1, u_N1, c_N2, u_N2, c_N3, u_N3, c_N4, u_N4)


def CelVel5(vals1, vals2, vals3, vals4, vals5,
            props1, props2, props3, props4, props5,
            fric_app_num, psi, debug1, have_ftn):
    '''Take values of celerity and velocity in five cells at a join or
    node.  Take the fixed properties in those cells.  Calculate the
    values of celerity and velocity at the five gridpoints at the
    connection in the next timestep.

    The calculation uses scipy.optimize.fsolve.  In some circumstances
    this routine will print messages to the screen or fail to get to
    a solution.  These are not trapped.

        Parameters:
            vals1           []              Two values of celerity, two
                                            of velocity, one value of
                                            drag (traffic and jet fans)
                                            in the cell on one side of
                                            the entity.
            vals2           []              Two values of celerity, two
                                            of velocity, one value of
                                            drag (traffic and jet fans)
                                            in the cell on another side
                                            side of the entity.
            vals3           []              Two values of celerity, two
                                            of velocity, one value of
                                            drag (traffic and jet fans)
                                            in the cell on another side
                                            side of the entity.
            vals4           []              Two values of celerity, two
                                            of velocity, one value of
                                            drag (traffic and jet fans)
                                            in the cell on another side
                                            side of the entity.
            vals5           []              Two values of celerity, two
                                            of velocity, one value of
                                            drag (traffic and jet fans)
                                            in the cell on another side
                                            side of the entity.
            props1          []              A list of fixed properties for
                                            the first cell attached to a node.
                                            Used in the calculation.
            props2          []              A list of fixed properties for
                                            the second cell attached to a node.
            props3          []              A list of fixed properties for
                                            the third cell attached to a node.
            props4          []              A list of fixed properties for
                                            the fourth cell attached to a node.
            props5          []              A list of fixed properties for
                                            the fifth cell attached to a node.
            fric_app_num    int             A number representing the equation
                                            used to calculate friction in the
                                            calculation.
            psi             float           A constant in the characteristics
                                            equation.
            debug1          bool            The debug Boolean set by the user.
            have_ftn        bool            True if the Fortran routines can be
                                            used, False otherwise.

        Returns:
            c_N1            float           Celerity at the gridpoint at the
                                            node in the first cell in the
                                            next timestep.
            u_N1            float           Velocity at the gridpoint at the
                                            node in the first cell in the
                                            next timestep.
            c_N2            float           Celerity at the gridpoint at the
                                            node in the second cell in the
                                            next timestep.
            u_N2            float           Velocity at the gridpoint at the
                                            node in the second cell in the
                                            next timestep.
            c_N3            float           Celerity at the gridpoint at the
                                            node in the third cell in the
                                            next timestep.
            u_N3            float           Velocity at the gridpoint at the
                                            node in the third cell in the
                                            next timestep.
            c_N4            float           Celerity at the gridpoint at the
                                            node in the fourth cell in the
                                            next timestep.
            u_N4            float           Velocity at the gridpoint at the
                                            node in the fourth cell in the
                                            next timestep.
            c_N5            float           Celerity at the gridpoint at the
                                            node in the fifth cell in the
                                            next timestep.
            u_N5            float           Velocity at the gridpoint at the
                                            node in the fifth cell in the
                                            next timestep.
    '''
    # Unpack the values we need for the two cells.  We use c_L and U_L
    # for the values at the gridpoint away from the junction because the
    # way this function works we want a positive value of velocity is
    # going towards the junction.
    c_M1, c_L1, u_M1, u_L1, JFTRdrag1 = vals1
    c_M2, c_L2, u_M2, u_L2, JFTRdrag2 = vals2
    c_M3, c_L3, u_M3, u_L3, JFTRdrag3 = vals3
    c_M4, c_L4, u_M4, u_L4, JFTRdrag4 = vals4
    c_M5, c_L5, u_M5, u_L5, JFTRdrag5 = vals5

    (back_end1, area1, d_h1, roughness1, rr1, rr371,
     fric_const1, dtdx1, stag_away1, stag_twd1) = props1[:10]
    (back_end2, area2, d_h2, roughness2, rr2, rr372,
     fric_const2, dtdx2, stag_away2, stag_twd2) = props2[:10]
    (back_end3, area3, d_h3, roughness3, rr3, rr373,
     fric_const3, dtdx3, stag_away3, stag_twd3) = props3[:10]
    (back_end4, area4, d_h4, roughness4, rr4, rr374,
     fric_const4, dtdx4, stag_away4, stag_twd4) = props4[:10]
    (back_end5, area5, d_h5, roughness5, rr5, rr375,
     fric_const5, dtdx5, stag_away5, stag_twd5) = props5[:10]

    # Check if we have to adjust the value of velocity to account for
    # segment orientations at the junction and choose which stagnation
    # coefficient to use.
    u_L1, u_M1, JFTRdrag1, stag_1 = FlipCell(back_end1, u_L1, u_M1,
                                           JFTRdrag1, stag_twd1, stag_away1)
    u_L2, u_M2, JFTRdrag2, stag_2 = FlipCell(back_end2, u_L2, u_M2,
                                             JFTRdrag2, stag_twd2, stag_away2)
    u_L3, u_M3, JFTRdrag3, stag_3 = FlipCell(back_end3, u_L3, u_M3,
                                           JFTRdrag3, stag_twd3, stag_away3)
    u_L4, u_M4, JFTRdrag4, stag_4 = FlipCell(back_end4, u_L4, u_M4,
                                             JFTRdrag4, stag_twd4, stag_away4)
    u_L5, u_M5, JFTRdrag5, stag_5 = FlipCell(back_end5, u_L5, u_M5,
                                             JFTRdrag5, stag_twd5, stag_away5)

    # Define the sub-procedure used in the call to scipy.optimize.fsolve.
    def FiveWayMoC2b(x0):
        '''Define ten equations that need to be satisfied to calculate new
        values of celerity and velocity at five gridpoints in cells that
        join together at a five-way junction.  These are:
         * five forward characteristics in the five cells,
         * one mass continuity equation at the node, and
         * four calculations of stagnation pressures at the node.
        All ten equations are expressed such the correct result is zero,
        which is how scipy.optimize.fsolve can solve the system.
        '''
        # Get the first approximations to the values out of the
        # arguments (these are from the previous timestep).
        c_N1, u_N1, c_N2, u_N2, c_N3, u_N3, c_N4, u_N4, c_N5, u_N5 = x0

        c_f1, u_f1 = BaseFwd(dtdx1, c_M1, u_M1, c_L1, u_L1, c_N1, u_N1)
        c_f2, u_f2 = BaseFwd(dtdx2, c_M2, u_M2, c_L2, u_L2, c_N2, u_N2)
        c_f3, u_f3 = BaseFwd(dtdx3, c_M3, u_M3, c_L3, u_L3, c_N3, u_N3)
        c_f4, u_f4 = BaseFwd(dtdx4, c_M4, u_M4, c_L4, u_L4, c_N4, u_N4)
        c_f5, u_f5 = BaseFwd(dtdx5, c_M5, u_M5, c_L5, u_L5, c_N5, u_N5)

        Fanning_1 = FricFac(d_h1, roughness1, rr1, rr371, u_f1, fric_app_num)
        Fanning_2 = FricFac(d_h2, roughness2, rr2, rr372, u_f2, fric_app_num)
        Fanning_3 = FricFac(d_h3, roughness3, rr3, rr373, u_f3, fric_app_num)
        Fanning_4 = FricFac(d_h4, roughness4, rr4, rr374, u_f4, fric_app_num)
        Fanning_5 = FricFac(d_h5, roughness5, rr5, rr375, u_f5, fric_app_num)

        # Get new values of the friction terms and add the traffic drag
        # term and jet fan thrust term.
        E_1dt = (fric_const1 * Fanning_1) * u_f1 * abs(u_f1)  \
                + JFTRdrag1
        E_2dt = (fric_const2 * Fanning_2) * u_f2 * abs(u_f2)  \
                + JFTRdrag2
        E_3dt = (fric_const3 * Fanning_3) * u_f3 * abs(u_f3)  \
                + JFTRdrag3
        E_4dt = (fric_const4 * Fanning_4) * u_f4 * abs(u_f4)  \
                + JFTRdrag4
        E_5dt = (fric_const5 * Fanning_5) * u_f5 * abs(u_f5)  \
                + JFTRdrag5

        # Define functions for the four characteristics.
        char1 = psi * (c_N1 - c_f1) + (u_N1 - u_f1) + E_1dt
        char2 = psi * (c_N2 - c_f2) + (u_N2 - u_f2) + E_2dt
        char3 = psi * (c_N3 - c_f3) + (u_N3 - u_f3) + E_3dt
        char4 = psi * (c_N4 - c_f4) + (u_N4 - u_f4) + E_4dt
        char5 = psi * (c_N5 - c_f5) + (u_N5 - u_f5) + E_5dt

        # Define the continuity equation for compressible flow in the
        # four branches.
        continuity = area1 * u_N1 * (c_N1/c_N2)**psi + area2 * u_N2 \
                   + area3 * u_N3 * (c_N3/c_N2)**psi  \
                   + area4 * u_N4 * (c_N4/c_N2)**psi  \
                   + area5 * u_N5 * (c_N5/c_N2)**psi

        # There are terms in the stagnation pressure equations that we use
        # in more than one equation.  Put them into one variable.
        stag1_term = c_N1**2 + stag_1 * u_N1**2

        # Define three equations giving the difference between the stagnation
        # pressures in three pairs of gridpoints at the node.
        P_stag1 = stag1_term - c_N2**2 - stag_2*u_N2**2
        P_stag2 = stag1_term - c_N3**2 - stag_3*u_N3**2
        P_stag3 = stag1_term - c_N4**2 - stag_4*u_N4**2
        P_stag4 = stag1_term - c_N5**2 - stag_5*u_N5**2

        return[char1, char2, char3, char4, char5, continuity,
               P_stag1, P_stag2, P_stag3, P_stag4]

    # Solve the system of equations using the values at the previous timestep
    # as our first estimate of the values at the next one.
    x0 = np.array((c_M1, u_M1, c_M2, u_M2, c_M3, u_M3, c_M4, u_M4, c_M5, u_M5))
    if have_ftn:
        args4 = np.array((psi, float(fric_app_num),
                          dtdx1, area1, d_h1, rr1, rr371, fric_const1,
                          roughness1, stag_1, JFTRdrag1,
                          c_M1, u_M1, c_L1, u_L1,
                          dtdx2, area2, d_h2, rr2, rr372, fric_const2,
                          roughness2, stag_2, JFTRdrag2,
                          c_M2, u_M2, c_L2, u_L2,
                          dtdx3, area3, d_h3, rr3, rr373, fric_const3,
                          roughness3, stag_3, JFTRdrag3,
                          c_M3, u_M3, c_L3, u_L3,
                          dtdx4, area4, d_h4, rr4, rr374, fric_const4,
                          roughness4, stag_4, JFTRdrag4,
                          c_M4, u_M4, c_L4, u_L4,
                          dtdx5, area5, d_h5, rr5, rr375, fric_const5,
                          roughness5, stag_5, JFTRdrag5,
                          c_M5, u_M5, c_L5, u_L5))
        (c_N1, u_N1, c_N2, u_N2, c_N3, u_N3, c_N4, u_N4,
         c_N5, u_N5) = scipy.optimize.fsolve(ftn.fivewaymoc2a, x0, args4)
    else:
        (c_N1, u_N1, c_N2, u_N2, c_N3, u_N3, c_N4, u_N4,
         c_N5, u_N5) = scipy.optimize.fsolve(FiveWayMoC2b, x0)

    # Now restore the signs of the velocities we switched, if we need to.
    if back_end1:
            u_N1 = -u_N1
    if back_end2:
            u_N2 = -u_N2
    if back_end3:
            u_N3 = -u_N3
    if back_end4:
            u_N4 = -u_N4
    if back_end5:
            u_N5 = -u_N5

    return(c_N1, u_N1, c_N2, u_N2, c_N3, u_N3, c_N4, u_N4, c_N5, u_N5)


def CelVel6(vals1, vals2, vals3, vals4, vals5, vals6,
            props1, props2, props3, props4, props5, props6,
            fric_app_num, psi, debug1, have_ftn):
    '''Take values of celerity and velocity in six cells at a join or
    node.  Take the fixed properties in those cells.  Calculate the
    values of celerity and velocity at the six gridpoints at the
    connection in the next timestep.

    The calculation uses scipy.optimize.fsolve.  In some circumstances
    this routine will print messages to the screen or fail to get to
    a solution.  These are not trapped.

        Parameters:
            vals1           []              Two values of celerity, two
                                            of velocity, one value of
                                            drag (traffic and jet fans)
                                            in the cell on one side of
                                            the entity.
            vals2           []              Two values of celerity, two
                                            of velocity, one value of
                                            drag (traffic and jet fans)
                                            in the cell on another side
                                            side of the entity.
            vals3           []              Two values of celerity, two
                                            of velocity, one value of
                                            drag (traffic and jet fans)
                                            in the cell on another side
                                            side of the entity.
            vals4           []              Two values of celerity, two
                                            of velocity, one value of
                                            drag (traffic and jet fans)
                                            in the cell on another side
                                            side of the entity.
            vals5           []              Two values of celerity, two
                                            of velocity, one value of
                                            drag (traffic and jet fans)
                                            in the cell on another side
                                            side of the entity.
            vals6           []              Two values of celerity, two
                                            of velocity, one value of
                                            drag (traffic and jet fans)
                                            in the cell on another side
                                            side of the entity.
            props1          []              A list of fixed properties for
                                            the first cell attached to a node.
                                            Used in the calculation.
            props2          []              A list of fixed properties for
                                            the second cell attached to a node.
            props3          []              A list of fixed properties for
                                            the third cell attached to a node.
            props4          []              A list of fixed properties for
                                            the fourth cell attached to a node.
            props5          []              A list of fixed properties for
                                            the fifth cell attached to a node.
            props6          []              A list of fixed properties for
                                            the sixth cell attached to a node.
            fric_app_num    int             A number representing the equation
                                            used to calculate friction in the
                                            calculation.
            psi             float           A constant in the characteristics
                                            equation.
            debug1          bool            The debug Boolean set by the user.
            have_ftn        bool            True if the Fortran routines can be
                                            used, False otherwise.

        Returns:
            c_N1            float           Celerity at the gridpoint at the
                                            node in the first cell in the
                                            next timestep.
            u_N1            float           Velocity at the gridpoint at the
                                            node in the first cell in the
                                            next timestep.
            c_N2            float           Celerity at the gridpoint at the
                                            node in the second cell in the
                                            next timestep.
            u_N2            float           Velocity at the gridpoint at the
                                            node in the second cell in the
                                            next timestep.
            c_N3            float           Celerity at the gridpoint at the
                                            node in the third cell in the
                                            next timestep.
            u_N3            float           Velocity at the gridpoint at the
                                            node in the third cell in the
                                            next timestep.
            c_N4            float           Celerity at the gridpoint at the
                                            node in the fourth cell in the
                                            next timestep.
            u_N4            float           Velocity at the gridpoint at the
                                            node in the fourth cell in the
                                            next timestep.
            c_N5            float           Celerity at the gridpoint at the
                                            node in the fifth cell in the
                                            next timestep.
            u_N5            float           Velocity at the gridpoint at the
                                            node in the fifth cell in the
                                            next timestep.
            c_N6            float           Celerity at the gridpoint at the
                                            node in the sixth cell in the
                                            next timestep.
            u_N6            float           Velocity at the gridpoint at the
                                            node in the sixth cell in the
                                            next timestep.
    '''
    # Unpack the values we need for the two cells.  We use c_L and U_L
    # for the values at the gridpoint away from the junction because the
    # way this function works we want a positive value of velocity is
    # going towards the junction.
    c_M1, c_L1, u_M1, u_L1, JFTRdrag1 = vals1
    c_M2, c_L2, u_M2, u_L2, JFTRdrag2 = vals2
    c_M3, c_L3, u_M3, u_L3, JFTRdrag3 = vals3
    c_M4, c_L4, u_M4, u_L4, JFTRdrag4 = vals4
    c_M5, c_L5, u_M5, u_L5, JFTRdrag5 = vals5
    c_M6, c_L6, u_M6, u_L6, JFTRdrag6 = vals6

    (back_end1, area1, d_h1, roughness1, rr1, rr371,
     fric_const1, dtdx1, stag_away1, stag_twd1) = props1[:10]
    (back_end2, area2, d_h2, roughness2, rr2, rr372,
     fric_const2, dtdx2, stag_away2, stag_twd2) = props2[:10]
    (back_end3, area3, d_h3, roughness3, rr3, rr373,
     fric_const3, dtdx3, stag_away3, stag_twd3) = props3[:10]
    (back_end4, area4, d_h4, roughness4, rr4, rr374,
     fric_const4, dtdx4, stag_away4, stag_twd4) = props4[:10]
    (back_end5, area5, d_h5, roughness5, rr5, rr375,
     fric_const5, dtdx5, stag_away5, stag_twd5) = props5[:10]
    (back_end6, area6, d_h6, roughness6, rr6, rr376,
     fric_const6, dtdx6, stag_away6, stag_twd6) = props6[:10]

    # Check if we have to adjust the value of velocity to account for
    # segment orientations at the junction and choose which stagnation
    # coefficient to use.
    u_L1, u_M1, JFTRdrag1, stag_1 = FlipCell(back_end1, u_L1, u_M1,
                                           JFTRdrag1, stag_twd1, stag_away1)
    u_L2, u_M2, JFTRdrag2, stag_2 = FlipCell(back_end2, u_L2, u_M2,
                                             JFTRdrag2, stag_twd2, stag_away2)
    u_L3, u_M3, JFTRdrag3, stag_3 = FlipCell(back_end3, u_L3, u_M3,
                                           JFTRdrag3, stag_twd3, stag_away3)
    u_L4, u_M4, JFTRdrag4, stag_4 = FlipCell(back_end4, u_L4, u_M4,
                                             JFTRdrag4, stag_twd4, stag_away4)
    u_L5, u_M5, JFTRdrag5, stag_5 = FlipCell(back_end5, u_L5, u_M5,
                                             JFTRdrag5, stag_twd5, stag_away5)
    u_L6, u_M6, JFTRdrag6, stag_6 = FlipCell(back_end6, u_L6, u_M6,
                                             JFTRdrag6, stag_twd6, stag_away6)

    # Define the sub-procedure used in the call to scipy.optimize.fsolve.
    def SixWayMoC2b(x0):
        '''Define twelve equations that need to be satisfied to calculate
        new values of celerity and velocity at five gridpoints in cells
        that join together at a six-way junction.  These are:
         * six forward characteristics in the four cells,
         * one mass continuity equation at the node, and
         * five calculations of stagnation pressures at the node.
        All twelve equations are expressed such the correct result is zero,
        which is how scipy.optimize.fsolve can solve the system.
        '''
        # Get the first approximations to the values out of the
        # arguments (these are from the previous timestep).
        c_N1, u_N1, c_N2, u_N2, c_N3, u_N3, c_N4, u_N4,  \
        c_N5, u_N5, c_N6, u_N6 = x0

        c_f1, u_f1 = BaseFwd(dtdx1, c_M1, u_M1, c_L1, u_L1, c_N1, u_N1)
        c_f2, u_f2 = BaseFwd(dtdx2, c_M2, u_M2, c_L2, u_L2, c_N2, u_N2)
        c_f3, u_f3 = BaseFwd(dtdx3, c_M3, u_M3, c_L3, u_L3, c_N3, u_N3)
        c_f4, u_f4 = BaseFwd(dtdx4, c_M4, u_M4, c_L4, u_L4, c_N4, u_N4)
        c_f5, u_f5 = BaseFwd(dtdx5, c_M5, u_M5, c_L5, u_L5, c_N5, u_N5)
        c_f6, u_f6 = BaseFwd(dtdx6, c_M6, u_M6, c_L6, u_L6, c_N6, u_N6)

        Fanning_1 = FricFac(d_h1, roughness1, rr1, rr371, u_f1, fric_app_num)
        Fanning_2 = FricFac(d_h2, roughness2, rr2, rr372, u_f2, fric_app_num)
        Fanning_3 = FricFac(d_h3, roughness3, rr3, rr373, u_f3, fric_app_num)
        Fanning_4 = FricFac(d_h4, roughness4, rr4, rr374, u_f4, fric_app_num)
        Fanning_5 = FricFac(d_h5, roughness5, rr5, rr375, u_f5, fric_app_num)
        Fanning_6 = FricFac(d_h6, roughness6, rr6, rr376, u_f6, fric_app_num)

        # Get new values of the friction terms and add the traffic drag
        # term and jet fan thrust term.
        E_1dt = (fric_const1 * Fanning_1) * u_f1 * abs(u_f1)  \
                + JFTRdrag1
        E_2dt = (fric_const2 * Fanning_2) * u_f2 * abs(u_f2)  \
                + JFTRdrag2
        E_3dt = (fric_const3 * Fanning_3) * u_f3 * abs(u_f3)  \
                + JFTRdrag3
        E_4dt = (fric_const4 * Fanning_4) * u_f4 * abs(u_f4)  \
                + JFTRdrag4
        E_5dt = (fric_const5 * Fanning_5) * u_f5 * abs(u_f5)  \
                + JFTRdrag5
        E_6dt = (fric_const6 * Fanning_6) * u_f6 * abs(u_f6)  \
                + JFTRdrag6

        # Define functions for the four characteristics.
        char1 = psi * (c_N1 - c_f1) + (u_N1 - u_f1) + E_1dt
        char2 = psi * (c_N2 - c_f2) + (u_N2 - u_f2) + E_2dt
        char3 = psi * (c_N3 - c_f3) + (u_N3 - u_f3) + E_3dt
        char4 = psi * (c_N4 - c_f4) + (u_N4 - u_f4) + E_4dt
        char5 = psi * (c_N5 - c_f5) + (u_N5 - u_f5) + E_5dt
        char6 = psi * (c_N6 - c_f6) + (u_N6 - u_f6) + E_6dt

        # Define the continuity equation for compressible flow in the
        # four branches.
        continuity = area1 * u_N1 * (c_N1/c_N2)**psi + area2 * u_N2 \
                   + area3 * u_N3 * (c_N3/c_N2)**psi  \
                   + area4 * u_N4 * (c_N4/c_N2)**psi  \
                   + area5 * u_N5 * (c_N5/c_N2)**psi  \
                   + area6 * u_N6 * (c_N6/c_N2)**psi

        # There are terms in the stagnation pressure equations that we use
        # in more than one equation.  Put them into one variable.
        stag1_term = c_N1**2 + stag_1 * u_N1**2

        # Define three equations giving the difference between the stagnation
        # pressures in three pairs of gridpoints at the node.
        P_stag1 = stag1_term - c_N2**2 - stag_2*u_N2**2
        P_stag2 = stag1_term - c_N3**2 - stag_3*u_N3**2
        P_stag3 = stag1_term - c_N4**2 - stag_4*u_N4**2
        P_stag4 = stag1_term - c_N5**2 - stag_5*u_N5**2
        P_stag5 = stag1_term - c_N6**2 - stag_6*u_N6**2

        return[char1, char2, char3, char4, char5, char6, continuity,
               P_stag1, P_stag2, P_stag3, P_stag4, P_stag5]

    # Solve the system of equations using the values at the previous timestep
    # as our first estimate of the values at the next one.
    x0 = np.array((c_M1, u_M1, c_M2, u_M2, c_M3, u_M3,
                   c_M4, u_M4, c_M5, u_M5, c_M6, u_M6))
    if have_ftn:
        args4 = np.array((psi, float(fric_app_num),
                          dtdx1, area1, d_h1, rr1, rr371, fric_const1,
                          roughness1, stag_1, JFTRdrag1,
                          c_M1, u_M1, c_L1, u_L1,
                          dtdx2, area2, d_h2, rr2, rr372, fric_const2,
                          roughness2, stag_2, JFTRdrag2,
                          c_M2, u_M2, c_L2, u_L2,
                          dtdx3, area3, d_h3, rr3, rr373, fric_const3,
                          roughness3, stag_3, JFTRdrag3,
                          c_M3, u_M3, c_L3, u_L3,
                          dtdx4, area4, d_h4, rr4, rr374, fric_const4,
                          roughness4, stag_4, JFTRdrag4,
                          c_M4, u_M4, c_L4, u_L4,
                          dtdx5, area5, d_h5, rr5, rr375, fric_const5,
                          roughness5, stag_5, JFTRdrag5,
                          c_M5, u_M5, c_L5, u_L5,
                          dtdx6, area6, d_h6, rr6, rr376, fric_const6,
                          roughness6, stag_6, JFTRdrag6,
                          c_M6, u_M6, c_L6, u_L6))
        (c_N1, u_N1, c_N2, u_N2, c_N3, u_N3, c_N4, u_N4, c_N5, u_N5,
         c_N6, u_N6) = scipy.optimize.fsolve(ftn.sixwaymoc2a, x0, args4)
    else:
        (c_N1, u_N1, c_N2, u_N2, c_N3, u_N3, c_N4, u_N4, c_N5, u_N5,
         c_N6, u_N6) = scipy.optimize.fsolve(SixWayMoC2b, x0)

    # Now restore the signs of the velocities we switched, if we need to.
    if back_end1:
            u_N1 = -u_N1
    if back_end2:
            u_N2 = -u_N2
    if back_end3:
            u_N3 = -u_N3
    if back_end4:
            u_N4 = -u_N4
    if back_end5:
            u_N5 = -u_N5
    if back_end6:
            u_N6 = -u_N6

    return(c_N1, u_N1, c_N2, u_N2, c_N3, u_N3,
           c_N4, u_N4, c_N5, u_N5, c_N6, u_N6)


def TwoPoints(cels, vels, JFTR_array, seg_ID, index):
    '''Take two arrays of celerity and velocity and a segment index.  If
    seg_ID is negative, return four values near the back end of the segment.
    If it is positive, return four values near the forward end.

        Parameters:
            cels            [np.array]      A list of np.arrays of celerities
                                            in every segment.
            vels            [np.array]      A list of np.arrays of velocities
                                            in every segment.
            JFTR_array      [np.array]      A list of np.arrays with the
                                            sum of traffic drag terms and
                                            jet fan terms in every segment.
            seg_ID          int             The ID of the segment, signed.
                                            Negative if we need the values
                                            at the back end.

        Returns:
            c_M             float           Celerity at the back end or forward
                                            end of the segment.
            c_LR            float           Celerity one cell away from the back
                                            end or forward end of the segment.
            u_M             float           Velocity at the back end or forward
                                            end of the segment.
            u_LR            float           Velocity one cell away from the back
                                            end or forward end of the segment.
            u_LR            float           Velocity one cell away from the back
                                            end or forward end of the segment.
            TRdrag          float           Traffic drag in the cell next to
                                            the
    '''
    c = cels[index]
    v = vels[index]
    if seg_ID < 0:
        # This is the back end of the segment.
        return(c[0], c[1], v[0], v[1], JFTR_array[index][0])
    else:
        # It's the forward end.
        return(c[-1], c[-2], v[-1], v[-2], JFTR_array[index][-1])


def JoinProperty(segments_consts, tunnelfans_dict, dampertiming_dict,
                 psi, seg_ID):
    '''Take a list of segment constants, dictionaries of active items
    (fans, dampers etc.) psi for air and a segment index.
    Build a list of fixed properties needed to solve in one segment at a
    junction, choosing the correct set of fixed loss values for the end
    attached at the junction.  The constant psi is factored into the
    stagnation pressure term so that we don't need to include it in the
    junction flow calculation at every iteration.

        Parameters:
            segments_consts [()]            A list of tuples of the fixed
                                            values in segments.  See below.
            tunnelfans_dict {}              A dictionary of fan data.  We
                                            may take data from it and add it
                                            to the join, for the calculation.
            dampertiming_dict {}            A dictionary of damper data.  We
                                            may take data from it and add it
                                            to the join, for the calculation.
            psi             float           An exponent in the isentropic
                                            flow calculation.
            seg_ID          int             The ID of the segment, signed.
                                            Negative if we need the values
                                            at the back end.

        Returns:
            A list:
                back_end    bool            True if the back end is attached,
                                            False if the forward end is.
                area        float           Area in the segment
                d_h         float           Hydraulic diameter of the segment
                roughness   float           If +ve, the roughness height of
                                            the segment.  If -ve, the fixed
                                            Fanning friction factor (-c_f).
                fric_const  float           A constant in the friction calculation,
                                            0.5 * perimeter / area * dt
                dtdx        float           A constant in the characteristics
                                            calculation, dt/dx_local
                stag_away   float           Multiplier on the v**2 term in the
                                            stagnation pressure equation for
                                            flow away from the junction.
                stag_twd    float           Multiplier on the v**2 term in the
                                            stagnation pressure equation for
                                            flow towards from the junction.
                zeta_away   float           Pressure loss factor (k-factor) for
                                            flow away from the junction.
                zeta_twd    float           Pressure loss factor (k-factor) for
                                            flow towards from the junction.
    '''
    seg_index = abs(seg_ID) - 1
    seg_consts = segments_consts[seg_index]
    # Pick out the properties we need to solve at a node.
    # (area, perim, d_h, roughness, rr, rr37,
    #  zeta_back_bf, zeta_back_fb,
    #  zeta_fwd_bf, zeta_fwd_fb,
    #  back_type, back_value, fwd_type, fwd_value,
    #  fric_const, dtdx, dx_local, gridpoints,
    #  end_tests, navigable)
    area = seg_consts[0]
    d_h =  seg_consts[2]
    roughness = seg_consts[3]
    rr = seg_consts[4]
    rr37 = seg_consts[5]
    fric_const = seg_consts[14]
    dtdx = seg_consts[15]

    if seg_ID < 0:
        # The back end of the segment is attached at the node.
        zeta_away = seg_consts[6] # bf at back end
        zeta_twd = seg_consts[7]  # fb at back end
        back_end = True
    else:
        # It's the forward end.
        zeta_away = seg_consts[9] # fb at fwd end
        zeta_twd = seg_consts[8]  # bf at fwd end
        back_end = False
    # Now generate the values of pressure change we'll use in the
    # stagnation pressure equations when solving flow at joins.  The
    # The '1' represents the dynamic pressure of the flow.  The pressure
    # loss factors are given signs to represent their effects in
    # calculating the stagnation pressures.  We incorporate psi
    # so that we don't have to divide by psi in every iteration.
    stag_twd = (1. - zeta_twd) / psi
    stag_away = (1. + zeta_away) / psi

    # Build a list of the properties used in the junction calculations.
    properties = [back_end, area, d_h, roughness, rr, rr37, fric_const, dtdx,
                 stag_away, stag_twd, zeta_away, zeta_twd, seg_ID, seg_index]

    # Check if we have an entity at the forward end that has transient
    # properties (like a fan, damper, PSD or tunnel door).  If it does,
    # we add the letter and the key in the entity's dictionary that gives
    # its properties.
    letter = seg_consts[12]
    if letter == "f":
        fan_data = tunnelfans_dict[seg_consts[13]]
        additions = ["f"] + fan_data[:7]
        properties.extend(additions)
    elif letter == "d":
        damper_name = seg_consts[13]
        damper_data = dampertiming_dict[damper_name]
        # for index, line in enumerate(damper_data):
        #     print(index, line)
        # sys.exit()
        additions = ["d", damper_data[0], damper_data[1], damper_name]
        properties.extend(additions)
    if letter != "d":
        # This is not a damper, so it is safe to turn the properties
        # into a tuple.
        properties = tuple(properties)
    return(properties)


def AddToJoin(joins_dict, node_name, seg_ID):
    '''Take a dictionary of joins/nodes and add a new entry at a join.
    If the join has not been defined yet, create it and add its first
    branch.

        Parameters:
            joins_dict      {}              A dictionary of joins.  The key
                                            is the join name, it returns a
                                            list of which seg_IDs are attached.
            node_name       str             The key of the join.
            seg_ID          int             The ID of the segment (signed) to
                                            be added to the join dictionary.

        Returns:
            joins_dict      {}              An updated dictionary.
    '''
    try:
        legs = joins_dict[node_name.lower()]
    except KeyError:
        # This is the first time this node has been processed.
        legs = [seg_ID]
    else:
        legs.append(seg_ID)
    joins_dict.__setitem__(node_name.lower(), legs)
    return(joins_dict)


def ZetaClash(totest, optionals_dict, file_name, tunnel_name,
              line_number, line_text, log):
    '''Check if two mutually exclusive optional entries for pressure loss
    factors have been set at a portal definition.  Complain if they have.

        Parameters:
            totest          ()              A tuple of two optional entry
                                            keys.
            optionals_dict  {}              Dictionary of the optional entries
                                            on the line.
            file_name       str             The file name, for error messages.
            tunnel_name     str             The tunnel name, for error messages.
            line_number     int             The line number, for error messages.
            line_text       str             The entire line, including comments.
            log             handle          The handle of the logfile.

        Returns:
            "ok"            str             Something that is not None, to
                                            signifying success.
    '''
    if totest[0] in optionals_dict and totest[1] in optionals_dict:
        err = ('> In the file named "' + file_name + '"\n'
               '> two optional arguments in tunnel "'
                 + tunnel_name + '"\n'
               '> set the same custom k-factor.  One was "' + totest[0] + '",\n'
               '> the other was "' + totest[1] + '".  Please remove one of\n'
               '> them.'
              )
        gen.WriteError(2441, err, log)
        gen.ErrorOnLine(line_number, line_text, log, False)
        return(None)
    return("ok")


def BordaCarnot(small, large):
    '''Take two areas, calculate the loss by the empirical Borda-Carnot
    equation for abrupt flow expansion from the small area to the large
    area.  See equation 4.1 in Idelchik, "Handbook of Hydraulic resistance",
    (3rd and 4th editions).  This loss must be applied to the dynamic
    pressure in the smaller of the two areas, not the larger.

        Parameters:
            small           float           Area of the smaller of the two
                                            tunnels.
            large           float           Area of the larger of the two
                                            tunnels.

        Returns:
            zeta_exp        float           Expansion pressure loss factor.
    '''
    return(math.pow(1. - small / large, 2))


def ContractionLoss1(small, large):
    '''Take two areas, calculate the loss for an abrupt contraction from
    the curve for sharp-edged contractions (r/d = 0) in Figure 14 of
    Miller, "Internal Flow Systems", 2nd edition, page 374.  Linear
    interpolation between a set of points is used.
    This loss must be applied to the dynamic pressure in the smaller of
    the two areas, not the larger.

        Parameters:
            small           float           Area of the smaller of the two
                                            tunnels.
            large           float           Area of the larger of the two
                                            tunnels.

        Returns:
            zeta_con        float           Contraction pressure loss factor.
    '''
    # Area ratio from 0.0 to 1.0 in steps of 0.1.
    ratio = np.arange(0, 1.05, 0.1)
    # Values on the curve, picked off with a ruler.
    loss = np.array((0.5, 0.475, 0.445, 0.413, 0.375, 0.32,  # 0.0 to 0.5
                          0.25, 0.167, 0.08, 0.025, 0.0))    # 0.6 to 1.0
    zeta = np.interp(small / large, ratio, loss)
    return(zeta)


def ContractionLoss2(small, large):
    '''Take two areas, calculate the loss for an abrupt contraction by the
    empirical expression given by Idelchik.  It is the equation at the
    start of Diagram 4-9 of Idelchik, "Handbook of Hydraulic resistance"
    (at the bottom of page 216 in the 3rd edition and at the bottom of
    page 254 of the 4th edition).
    This loss must be applied to the dynamic pressure in the smaller of
    the two areas, not the larger.

        Parameters:
            small           float           Area of the smaller of the two
                                            tunnels.
            large           float           Area of the larger of the two
                                            tunnels.

        Returns:
            zeta_con        float           Contraction pressure loss factor.
    '''
    return(0.5 * math.pow(1. - small / large, 0.75))


def GetChangeZetas(area_f, area_b, optionals_dict, file_name, tunnel_name,
                   line_number, line_text, log):
    '''Take two areas on either side of a change of sectype and an
    optionals dictionary applied to a sectype change and figure out
    what the k-factors should be.
    If the two tunnels have the same area, any pressure loss factors will
    be put in the first tunnel.

        Parameters:
            area_f          float           Area of the segment after the area
                                            change.
            area_b          float           Area of the segment before the area
                                            change.
            optionals_dict  {}              Dictionary of the optional entries
                                            on the line, which may contain
                                            user-defined values of zeta.
            file_name       str             The file name, for error messages.
            tunnel_name     str             The tunnel name, for error messages.
            line_number     int             The line number, for error messages.
            line_text       str             The entire line, including comments.
            log             handle          The handle of the logfile.

        Returns:
            zeta_1          float           Pressure loss factor or zero.
            zeta_2          float           Pressure loss factor or zero.
            zeta_3          float           Pressure loss factor or zero.
            zeta_4          float           Pressure loss factor or zero.
    '''
    if "zeta_bf" in optionals_dict:
        zeta_bf = float(optionals_dict["zeta_bf"])
    elif area_b > area_f:
        zeta_bf = ContractionLoss1(area_f, area_b)
    else:
        # This handles equal areas too.
        zeta_bf = BordaCarnot(area_b, area_f)

    if "zeta_fb" in optionals_dict:
        zeta_fb = float(optionals_dict["zeta_fb"])
    elif area_b > area_f:
        zeta_fb = BordaCarnot(area_f, area_b)
    else:
        zeta_fb = ContractionLoss1(area_b, area_f)

    if area_b <= area_f:
        four_ks = (zeta_bf, zeta_fb, 0.0, 0.0)
    else:
        four_ks = (0.0, 0.0, zeta_bf, zeta_fb)
    return(four_ks)


def PortalZetas(end, area, default_in, default_out, optionals_dict,
                settings_dict, file_name, tunnel_name, line_number,
                line_text, log):
    '''Take an optionals dictionary applied to a portal and figure out what
    the k-factors should be.  At back portals we catch clashes between zeta_bf
    and zeta_in, zeta_fb and zeta_out.  At forward portals we catch clashes
    between zeta_fb and zeta_in, zeta_bf and zeta_out (these four pairs are
    mutually exclusive).  We also adjust for an optional argument setting a
    different area at the portal.

        Parameters:
            end             str             Keyword on the line defining a
                                            portal (either "back" or "fwd")
            area            float           Area of the segment.
            default_in      float           Default k-factor for inflow.
            default_out     float           Default k-factor for outflow.
            optionals_dict  {}              Dictionary of the optional entries
                                            on the line, which may contain
                                            user-defined values of zeta or area.
            file_name       str             The file name, for error messages.
            tunnel_name     str             The tunnel name, for error messages.
            line_number     int             The line number, for error messages.
            line_text       str             The entire line, including comments.
            log             handle          The handle of the logfile.

        Returns:
            zeta_bf         float           Pressure loss factor for airflow
                                            from back end to forward end at the
                                            portal (could be inflow or outflow).
            zeta_fb         float           Pressure loss factor for airflow
                                            from forward end to back end.
    '''

    # First check if an area was set to adjust the factors.
    # This used where the entrance of a tunnel is smaller than the tunnel
    # interior.  This is common where a downstand beam is used to support
    # the edge of a cut and cover roof or where an overheight vehicle roof
    # removal system is installed.
    try:
        # Check if the user set a custom portal area
        ptl_area = optionals_dict["area"]
    except KeyError:
        # It wasn't set.
        factor = 1.0
    else:
        # Apply a factor of (tunnel area / portal area)^2.
        factor = (area/ptl_area)**2

    # Set the default k-factors and pairs of names of clashing optional entries.
    if end == "back":
        zeta_bf = default_in * factor
        zeta_fb = default_out * factor
        totest1 = ("zeta_bf", "zeta_in")
        totest2 = ("zeta_fb", "zeta_out")
    else:
        zeta_bf = default_out * factor
        zeta_fb = default_in * factor
        totest1 = ("zeta_fb", "zeta_in")
        totest2 = ("zeta_bf", "zeta_out")
    # Test for two mutually exclusive options.
    result = ZetaClash(totest1, optionals_dict, file_name, tunnel_name,
                       line_number, line_text, log)
    if result is None:
        return(None)
    result = ZetaClash(totest2, optionals_dict, file_name, tunnel_name,
                       line_number, line_text, log)
    if result is None:
        return(None)

    # If we get to here, all is OK and we have no optional entries that
    # clash.  Get any optional entries.
    if "zeta_bf" in optionals_dict:
        value = optionals_dict["zeta_bf"]
        zeta_bf = factor * float(CheckForConstant(value, False, settings_dict))

    if "zeta_fb" in optionals_dict:
        value = optionals_dict["zeta_fb"]
        zeta_fb = factor * float(CheckForConstant(value, False, settings_dict))

    if "zeta_in" in optionals_dict:
        value = optionals_dict["zeta_in"]
        if end == "back":
            zeta_bf = factor * float(CheckForConstant(value, False, settings_dict))
        else:
            zeta_fb = factor * float(CheckForConstant(value, False, settings_dict))

    if "zeta_out" in optionals_dict:
        value = optionals_dict["zeta_out"]
        if end == "fwd":
            zeta_bf = factor * float(CheckForConstant(value, False, settings_dict))
        else:
            zeta_fb = factor * float(CheckForConstant(value, False, settings_dict))
    return(zeta_bf, zeta_fb)


def CellTraffic(x_back, x_fwd, trafficstuff):
    '''Take the distances of a cell and the definition of traffic in
    a segment.  Remove all the traffic definitions that do not overlap
    this cell and return the remaining traffic definitions.
    Note that we know that 'trafficstuff' has at least one entry in
    it; if it is empty this routine would not be called.

        Parameters:
            x_back          float           Distance of the gridpoint
                                            at the back end of the cell.
            x_fwd           float           Distance of the gridpoint
                                            at the forward end of the cell.
            trafficstuff    [[]]            A list of sub-lists.  Each sub-list
                                            defines a block of traffic drag:
                                             [start distance, stop distance,
                                              speed (m/s), drag (zeta/m)]

        Returns:
            filtered        [[]]            A list of sub-lists; trafficstuff
                                            with all the blocks that don't
                                            overlap this cell removed, and
                                            which starts with the distances
                                            of the two gridpoints.
    '''
    # Store the distances and the cell length as the first three entries
    # in the list we return.
    filtered = [x_back, x_fwd, x_fwd - x_back]
    for (x_left, x_right, speed, drag) in trafficstuff:
        # 'block is a list of four numbers:
        #  * back distance of the traffic block
        #  * fwd distance of the traffic block
        #  * speed of the traffic in m/s
        #  * loss factor of the traffic in zeta/m, including the 0.5 term.
        if x_left < x_fwd and x_right > x_back:
            # We have an overlap.  Limit the extents of the
            # traffic to the boundaries of this cell.
            x1 = max(x_left, x_back)
            x2 = min(x_right, x_fwd)
            filtered.append((x1, x2, speed, drag))
    return(filtered)


def JFTimeAdjust(JF_drag, dt, time, vels):
    '''Take the segment-based definitions of jet fans, the current
    calculation time and the array of tunnel air velocities at the
    previous timestep.
    Calculate the (rotational) speeds of all the jet fans at the
    current time and generate arrays that set the jet fan thrust
    terms and jet outlet air speeds to the values appropriate to
    the current time.
    Return a list of np.arrays that have the jet fan thrusts expressed
    as m**2/s**2 per metre length of each cell.
    '''
    # Make a list of segment-based data at this timestep.
    jetfanthrusts = []
    for seg_index, JFcalc_data in enumerate(JF_drag):
        local_plumes = np.array([0.0] * (len(vels[seg_index]) - 1))
        if JFcalc_data != []:
            # At least one jet fan plume crosses this segment.
            for plume in JFcalc_data:
                # Get out the data of the current plume.
                (seg, sub, frac, U_j, drags, times, speeds,
                               tr_index, JF_name, direction) = plume
                # Get the current jet fan speed.
                speed = np.interp(time, times, speeds)
                if not math.isclose(speed, 0.0):
                    # Get the air velocity in the tunnel alongside the
                    # jet fan.
                    u_back, u_fwd = vels[seg][sub: sub + 2]
                    u_t = u_back + frac * (u_fwd - u_back)
                    # if math.isclose(time, 3.0):
                    #     print("JFstuff", JF_name, seg, sub, frac, U_j, drags, speed)
                    # Get the data we need to add to the array.
                    # 'drags' is an np.array of thrusts, one per cell
                    # in the segment.  We multiply by the square of
                    # fractional fan speed, then apply the background
                    # velocity correction.
                    U_jloc = U_j * speed
                    plume = drags * speed**2 * (U_jloc - u_t) / abs(U_jloc)
                    local_plumes += plume
        jetfanthrusts.append(local_plumes * dt)
    return(jetfanthrusts)


def TRTimeAdjust(TR_drag, dt, time, dists, vels, have_ftn):
    '''Take the segment-based definitions of traffic, the current
    calculation time, the distances along the tunnels and the
    air velocities in the tunnels at the previous timestep.
    Calculate the traffic drag in each cell, using the mean air
    velocities in the cells.
    Return a list of np.arrays that have the traffic drag
    expressed as m**2/s**2 per metre length of each cell.
    '''

    # Make a list of cell-based traffic drag data at this timestep.
    trafficdrags = []
    for seg_index, TRcalc_data in enumerate(TR_drag):
        if TRcalc_data != []:
            loc_dists = dists[seg_index]
            loc_vels = vels[seg_index]
            gridpoints = len(loc_dists)
            if have_ftn:
                # Call a Fortran routine to calculate the traffic.
                # This is a runtime routine so it makes sense to have
                # one.
                local_drags = ftn.trafficadjust(loc_dists, loc_vels,
                                                TRcalc_data, gridpoints,
                                                len(TRcalc_data))
            else:
                # At least one block of traffic crosses this segment.
                # Iterate over the gridpoint locations and add bits of
                # traffic drag to the values in the cells.
                # Get the cell length.  These are constant in this segment
                # so it makes sense to calculate it before entering the next
                # loop.
                length = loc_dists[1] - loc_dists[0]
                # Build an array of zeros that we can add traffic drag
                # terms to.
                local_drags = np.array([0.0] * (len(dists[seg_index]) - 1))
                for cell_index, back in enumerate(loc_dists[:-1]):
                    fwd = loc_dists[cell_index + 1]

                    # Calculate the traffic drag in this cell, taking
                    # the air velocity as the mean of the air velocities
                    # at the gridpoints at the ends of the cell.
                    v_mean = sum(loc_vels[cell_index:cell_index + 2]) / 2

                    # Iterate over the blocks of traffic and look for
                    # overlaps with the distances at the ends of this
                    # cell.
                    for (start, stop, speed, drag_per_m) in TRcalc_data:
                        if start < fwd and stop > back:
                            # Get the extents of the traffic in this cell,
                            # calculate the traffic drag based on the mean
                            # air velocity in the cell and add it to the
                            # values to use in the cells in the next
                            # timestep.
                            tr_start = max(start, back)
                            tr_stop = min(stop, fwd)
                            v_diff = (speed - v_mean)
                            mod_drag = (tr_stop - tr_start) / length *   \
                                        drag_per_m * v_diff * abs(v_diff)
                            local_drags[cell_index] -= mod_drag
        else:
            # There are no blocks of traffic in this segment.
            local_drags = np.array([0.0] * (len(dists[seg_index]) - 1))
        trafficdrags.append(local_drags * dt)
    return(trafficdrags)


def CelVel1(c_array, u_array, seg_constants, converge, max_iter,
            fric_app_num, psi, debug1, last_time, vel_frac,
            JFTR_drag):
    '''Calculate the celerities and velocities in a segment that has
    losses, friction, traffic drag and plumes from jet fans.
    This routine skips the setting of values at gridpoints at the
    ends of segments whose values can be solved in conjunction with
    other gridpoints in other segments.

        Parameters:
            c_array         [float]         List of all the celerities at
                                            gridpoints in one segment.
            u_array         [float]         List of all the velocities at
                                            gridpoints in one segment.
            seg_constants   []              A list of fixed properties
                                            for the segment and how to
                                            handle its ends.
            converge        float           A value to decide when the
                                            calculation is converged.  If
                                            a celerity from the current
                                            iteration differs from the
                                            celerity at the previous
                                            iteration, the iterating stops.
            max_iter        int             A counter of the iterations.
            fric_app_num    int             Integer setting which friction
                                            factor approximation to use.
            psi             float           An exponent in the isentropic
                                            flow calculation.
            debug1          bool            The debug Boolean set by the user.
            last_time       float           The time at the previous timestep.
                                            Used in an information message
                                            when the calculation blows up.
            vel_frac        float           A multiplier to use on a fixed
                                            velocity at a portal in the.
                                            The first few seconds of a
                                            calculation.  The multiplier
                                            prevents stability problems
                                            in the calculation (it ramps
                                            up the fixed velocity from
                                            zero to the fixed value
                                            over a few seconds instead
                                            of imposing the full velocity
                                            in the first timestep.
            JFTR_drag       [float]         An np.array of floats, one for
                                            each cell in this segment.
                                            Gives the sum of the jet fan
                                            thrust term and the traffic drag
                                            term in that cell, in units
                                            of m^2/s^2 per metre so they
                                            can be added to the E terms.


        Returns:
            next_cels       [float]         List of celerities in the cell at
                                            the next timestep.  If the first or
                                            last gridpoints are at joins their
                                            values are not calculated.
            next_vels       [float]         List of velocities in the cell at
                                            the next timestep.  If the first or
                                            last gridpoints are at joins their
                                            values are not calculated.
            message         str             A string detailing the state of
                                            the calculation.  It could be one
                                            of the following:
                                              "The solution converged."
                                              "> OpenEnd calc didn't converge
                                               at the back end"
                                              "> FixedVel calc didn't converge
                                               at the back end"
                                              "> OpenEnd calc didn't converge
                                               at the forward end"
                                              "> FixedVel calc didn't converge
                                               at the forward end"
                                              "> CelVel1 calc didn't converge
                                               at interior gridpoints"
                                            If multiple failures occurred, it
                                            will be the last to occur.  Note
                                            that failures may not be fatal,
                                            as the calculation may recover.
    '''
    # Get the fixed properties of this segment.
    (area, perim, d_h, roughness, rr, rr37,
     zeta_back_bf, zeta_back_fb,
     zeta_fwd_bf, zeta_fwd_fb,
     back_type, back_value, fwd_type, fwd_value,
     fric_const, dtdx, dx_local, gridpoints,
     end_test, navigable) = seg_constants

    # Set up numpy arrays to hold the values of celerity and velocity
    # in the next timestep in this segment.  We fill them with NaN values
    # so that if we don't set a value, it raises an error in the next
    # timestep.
    next_cels = np.full(gridpoints, np.nan)
    next_vels = np.full(gridpoints, np.nan)

    # Create a string to catch calculations that don't converge.  We
    # change the string to contain some when something doesn't go right.
    # We check this string when we return to the calling routine and
    # if it has changed, we write it to the screen and to the log file
    # then warn the user of the dire consequences.
    message = "The solution converged."

    for gr_index in range(gridpoints):
        if gr_index == 0:
            # This is the gridpoint at the back end.

            # Check if the end is a type that can be handled here (open
            # to atmosphere or fixed velocity).
            if back_type == 'p':
                # This is an open portal with a fixed air pressure
                # outside it.  We call a routine to calculate with
                # a fixed celerity outside, portal loss factors for
                # inflow and outflow and a characteristic in one cell.
                (next_cels[0], next_vels[0], success) = \
                            OpenEnd(back_value, zeta_back_bf, zeta_back_fb,
                                    True, dtdx, d_h, roughness, rr, rr37,
                                    fric_const, max_iter, fric_app_num,
                                    converge, psi, debug1,
                                    JFTR_drag[0], c_array, u_array)
                if not success:
                    message = "> OpenEnd calc didn't converge at the back end"
            elif back_type == 'v':
                # We call a routine to calculate the celerity at the
                # portal that give us a fixed air velocity.
                next_vels[0] = back_value * vel_frac
                (next_cels[0], success) = FixedVel(next_vels[0],
                                               zeta_back_bf, zeta_back_fb,
                                               True, dtdx, d_h, roughness, rr,
                                               rr37, fric_const, max_iter,
                                               fric_app_num, converge, psi,
                                               JFTR_drag[0], c_array, u_array)
                if not success:
                    message = "> FixedVel calc didn't converge at the back end"
            # else:
                # This connection is an area change, junction, damper
                # or fan.   These are handled in other routines in
                # conjunction with the other segments at the connection.
                # pass
        elif gr_index == end_test:
            # "end_test" is one less than the count of gridpoints.  It is
            # in the seg_constants so that we don't have to do the calculation
            # at every call to this routine.

            if fwd_type == 'p':
                (next_cels[-1], next_vels[-1], success) = \
                        OpenEnd(fwd_value, zeta_fwd_fb, zeta_fwd_bf,
                                False, dtdx, d_h, roughness, rr, rr37,
                                fric_const, max_iter, fric_app_num,
                                converge, psi, debug1,
                                JFTR_drag[-1], c_array, u_array)
                if not success:
                    message = "> OpenEnd calc didn't converge at the forward end"
            elif fwd_type == 'v':
                next_vels[-1] = fwd_value * vel_frac
                (next_cels[-1], success) = FixedVel(next_vels[-1],
                                               zeta_fwd_fb, zeta_fwd_bf,
                                               False, dtdx, d_h, roughness, rr,
                                               rr37, fric_const, max_iter,
                                               fric_app_num, converge, psi,
                                               JFTR_drag[-1], c_array, u_array)
                if not success:
                    message = "> FixedVel calc didn't converge at the forward end"
        else:
            # This is a gridpoint in the middle.  We have a backwards
            # characteristic and a forwards characteristic.  Get the values
            # at the current gridpoint and the gridpoints on each side.
            # Note that if we have traffic, we already have the distances
            # of the gridpoints in fwd_tdrag and/or back_tdrag.
            prev_gp = gr_index - 1
            next_gp  = gr_index + 1
            c_L = c_array[prev_gp]
            u_L = u_array[prev_gp]
            c_M = c_array[gr_index]
            u_M = u_array[gr_index]
            c_R = c_array[next_gp]
            u_R = u_array[next_gp]
            left_JFTRdrag, right_JFTRdrag = JFTR_drag[prev_gp:next_gp]

            # Now iterate to get values of c_N and u_N.  Our first guess
            # is that they are equal to c_M and u_M.
            c_N = c_M
            u_N = u_M

            for count in range(1, max_iter + 1):
                # Get the conditions at the base of the backwards and
                # forwards characteristics, using the current estimate
                # of c_N and u_N.
                c_b, u_b = BaseBack(dtdx, c_M, u_M, c_R, u_R, c_N, u_N)
                c_f, u_f = BaseFwd(dtdx,  c_M, u_M, c_L, u_L, c_N, u_N)

                # Calculate the Fanning friction factors and the friction
                # loss terms.
                Fanning_b = FricFac(d_h, roughness, rr, rr37, u_b, fric_app_num)
                Fanning_f = FricFac(d_h, roughness, rr, rr37, u_f, fric_app_num)

                # Combine the friction terms, traffic drag terms and
                # jet fan thrust terms in the two cells.  Note that the
                # traffic drag and jet fan thrust do not depend on
                # local air velocities u_b or u_f.
                E_bdt = fric_const * Fanning_b * u_b * abs(u_b)  \
                        + right_JFTRdrag
                E_fdt = fric_const * Fanning_f * u_f * abs(u_f)  \
                        + left_JFTRdrag

                # Get a new value of u_N from the two characteristic
                # equations.
                u_N = 0.5 * (u_f + u_b + psi * (c_f - c_b) - E_bdt - E_fdt)

                # Store the current value of c_N.
                c_N_old = c_N
                # Now re-calculate c_N two ways.  Complain if they differ.
                # This was originally used for debugging but now is a useful
                # way of catching cases where the calculation blows up.
                c_N = c_f + (u_f - u_N - E_fdt) / psi
                c_N_alt = c_b + (u_N - u_b + E_bdt) / psi
                if not(math.isclose(c_N, c_N_alt)):
                    print('> Fouled up when calculating with two character-\n'
                          '> istics, ended up with a gridpoint that had the\n'
                          '> following values:\n'
                          '>   celerity =', c_N,'\n'
                          '>   velocity =', u_N,'\n'
                          '> If either of these are "nan" or there are runtime\n'
                          '> warnings printed to the screen, try increasing\n'
                          '> max_vel in the "settings" block to force larger\n'
                          '> cell sizes.  See also the Miscellany chapter in\n'
                          '> the User Manual for more details.\n'
                          '> Try setting the aero_time to '
                            + gen.FloatText(last_time) + ' seconds and\n'
                          '> plotting celerity and velocity values.  This\n'
                          '> will let you plot the state of the system at\n'
                          '> the last stable timestep.')
                    gen.PauseFail()
                # if debug1:
                #     print("two-characteristic iteration", iteration + 1)
                #     print("  c_N =",round(c_N, 12), round(c_N_old, 12))
                #     print("  u_N =",round(u_N, 12))
                if math.isclose(c_N, c_N_old, abs_tol = converge):
                    if debug1:
                        print("Breaking mid at iteration", count + 1)
                        print("c_N and u_N", c_N, u_N)
                    break
                elif count == max_iter:
                    # This is the last iteration in this do loop and it
                    # didn't converge.
                    message = ("> CelVel1 calc didn't converge at "
                               "an interior gridpoint")
            # Once we get to here we have converged or the counter of
            # iterations has run out.
            next_cels[gr_index] = c_N
            next_vels[gr_index] = u_N

    # Once we get to here we have filled as many of the values in the
    # gridpoint arrays as we can.  The ones at the start and finish
    # of the segment will be NaN values if those gridpoints are at
    # a junction.  Those will be calculated later in conjunction with
    # the other segments that meet at that junction.
    return(next_cels, next_vels, message)


def FixedVel(u_N, zeta_in, zeta_out, back_end, dtdx, d_h, roughness,
             rr, rr37, fric_const, max_iter, fric_app_num, converge,
             psi, JFTRdrag, c_array, u_array):

    '''Process the gridpoint at one end of a segment that connects to
    outside atmosphere.  There is a constant air velocity inside the
    portal and a celerity is calculated that maintains that air speed.
    This version of the routine handles traffic drag and jet fan thrust
    in the cell next to the portal.

        Parameters:
            u_N             float           Fixed air velocity to set at the
                                            portal.
            back_end        bool            True if this is a back end.  False
                                            if it is not.  Used to select which
                                            characteristic to use.
            dtdx            float           A constant in the characteristics
                                            calculation, dt/dx_local
            d_h             float           Hydraulic diameter of the segment
            zeta_in         float           Pressure loss factor for inflow.
            zeta_out        float           Pressure loss factor for outflow.
            roughness       float           If +ve, the roughness height of
                                            the segment.  If -ve, the fixed
                                            Fanning friction factor (-c_f).
            rr              float           Relative roughness (roughness/d_h)
            rr37            float           Relative roughness divided by 3.7.
            fric_const      float           A constant in the friction calc,
                                            0.5 * perimeter / area * dt
            max_iter        int             A counter of the iterations.
            fric_app_num    int             Integer setting which friction
                                            factor approximation to use.
            converge        float           A value to decide when the
                                            calculation is converged.  If
                                            a celerity from the current
                                            iteration differs from the
                                            celerity at the previous
                                            iteration, the iterating stops.
            psi             float           An exponent in the isentropic
                                            flow calculation.
            JFTRdrag        float           The sum of the traffic drag
                                            term and the jet fan term in
                                            the cell next to the portal.
                                            Expressed as m**2/s**2 per
                                            metre over the length of
                                            the cell.
            c_array         [float]         List of all the celerities at
                                            gridpoints in one segment.
            u_array         [float]         List of all the velocities at
                                            gridpoints in one segment.

        Returns:
            c_N             float           Celerity at the gridpoint just
                                            inside the portal that is
                                            appropriate for velocity u_N.
            converged       Bool            True if the calculation converged,
                                            False if it did not.
    '''
    if back_end:
        # Get the conditions at the first and second gridpoints.
        c_M = c_array[0]
        c_LR = c_array[1]
        u_M = u_array[0]
        u_LR = u_array[1]

    else:
        # Get the conditions at the last two gridpoints.
        c_M = c_array[-1]
        c_LR = c_array[-2]
        u_M = u_array[-1]
        u_LR = u_array[-2]

    # Create a boolean to monitor convergence.  This is changed to True
    # if the calculation converges.  It stays as False if the loop runs
    # out of iterations before converging.
    converged = False

    # Now iterate to get a new value of c_N.  Our first guess is that it
    # is equal to c_M.
    c_N = c_M
    for iteration in range(max_iter):
        c_N_old = c_N

        # Get the conditions at the base of the characteristic.  If this is
        # the gridpoint at the back end it is a backwards characteristic.
        # If this is the gridpoint at the forward end it is a forwards
        # characteristic.
        if back_end:
            c_bf, u_bf = BaseBack(dtdx, c_M, u_M, c_LR, u_LR, c_N, u_N)
        else:
            c_bf, u_bf = BaseFwd(dtdx, c_M, u_M, c_LR, u_LR, c_N, u_N)

        # Calculate the Fanning friction factor, based on the
        # current value of u_bf.
        Fanning = FricFac(d_h, roughness, rr, rr37, u_bf, fric_app_num)

        # Get the friction loss term, the traffic term and the jet fan term.
        E_bfdt = fric_const * Fanning * u_bf * abs(u_bf) + JFTRdrag

        # Get a new value of c_N from the relevant characteristic equation,
        # assuming a fixed value of u_N.
        if back_end:
            c_N = c_bf + (u_N - u_bf + E_bfdt)/psi
        else:
            c_N = c_bf + (u_bf - u_N - E_bfdt)/psi

        if math.isclose(c_N, c_N_old, abs_tol = converge):
            converged = True
            break

    # When we get to here, the calculation converged or the counter of
    # iterations timed out.
    return(c_N, converged)


def TrafDragCell(tdrag, dt, u_back, u_fwd):
    '''Take a definition of the traffic drag blocks in this cell and
    the air velocities at the two ends of the cell in the previous
    timestep.
    Figure out what the net effect of the traffic is (it may retard
    air flow or impel it) and return a value that can be added to
    the body force term (E_b or E_f).

        Parameters:
            tdrag      [float*3,[]*n]       The distances of the gridpoints
                                            at each end of this cell, the
                                            cell length (three floats) and
                                            one or more sub-lists with
                                            traffic drag.
            dt              float           The aero calculation timestep.
                                            Included as an argument here
                                            in case we need to use shorter
                                            timesteps in certain cases.
            u_back          float           Velocity at the back end of
                                            the cell in the previous
                                            timestep.
            u_fwd          float            Velocity at the forward end of
                                            the cell in the previous
                                            timestep.

        Returns:
            E               float           Traffic drag term to use in
                                            the characteristic equation
                                            alongside friction.
    '''
    # Set a zero value for the traffic drag term and loop over all the
    # blocks of traffic in this cell.
    E = 0.0
    # Define two values that are best calculated outside the 'for' loop.
    length = tdrag[2]
    u_mean = (u_back + u_fwd) / 2.
    for x_left, x_right, speed, drag in tdrag[3:]:
        u_diff = speed - u_mean
        ratio = (x_right - x_left) / length
        # Note that we could have put this 'ratio' term into the 'drag'
        # term in 'tdrag' because the traffic is unchanging, but in future
        # we will write a queueing model with dynamic traffic and those
        # occupancies will need to be calculated at each timestep.
        E -= ratio * drag * u_diff * abs(u_diff) * dt
    #     print("TrafDragCell", x_left, x_right, speed, drag, ratio)
    # print("E", E)
    return(E)


def OpenEnd(c_outside, zeta_in, zeta_out, back_end, dtdx, d_h, roughness,
            rr, rr37, fric_const, max_iter, fric_app_num, converge,
            psi, debug1, JFTRdrag, c_array, u_array):
    '''Process the cell at one end of a segment that connects to outside
    atmosphere.  The air outside the portal has a given pressure (expressed
    as a celerity), the gridpoint just inside the portal has conditions at
    the previous timestep, and there are conditions at the previous timestep
    at the other end of the cell.

    Calculate new values of celerity and velocity at the gridpoint just
    inside the portal.  This version of the routine handles traffic drag
    and jet fan thrust in the cell next to the portal.

    This routine gets out of whack with very large values of zeta, which
    is believed to be due to a bug in PROC PortalCelerities (which may be
    using the wrong equation).

        Parameters:
            c_outside       float           Celerity of air outside the portal.
            zeta_in         float           Pressure loss factor for inflow.
            zeta_out        float           Pressure loss factor for outflow.
            back_end        bool            True if this is a back end.  False
                                            if it is not.  Used to select which
                                            characteristic to use.
            dtdx            float           A constant in the characteristics
                                            calculation, dt/dx_local
            d_h             float           Hydraulic diameter of the segment
            roughness       float           If +ve, the roughness height of
                                            the segment.  If -ve, the fixed
                                            Fanning friction factor (-c_f).
            fric_const      float           A constant in the friction calc,
                                            0.5 * perimeter / area * dt
            max_iter        int             A counter of the iterations.
            fric_app_num    int             Integer setting which friction
                                            factor approximation to use.
            converge        float           A value to decide when the
                                            calculation is converged.  If
                                            a celerity from the current
                                            iteration differs from the
                                            celerity at the previous
                                            iteration, the iterating stops.
            psi             float           An exponent in the isentropic
                                            flow calculation.
            debug1          bool            The debug Boolean set by
                                            the user.
            JFTRdrag        float           The sum of the traffic drag
                                            term and the jet fan term in
                                            the cell next to the portal.
                                            Expressed as m**2/s**2 per
                                            metre over the length of
                                            the cell.
            c_array         [float]         List of all the celerities at
                                            gridpoints in one segment.
            u_array         [float]         List of all the velocities at
                                            gridpoints in one segment.

        Returns:
            c_N             float           Celerity at the gridpoint just
                                            inside the portal.
            u_N             float           Velocity at the gridpoint just
                                            inside the portal.
            converged       Bool            True if the calculation converged,
                                            False if it did not.
    '''

    if back_end:
        # Get the conditions at the first and second gridpoints.
        c_M = c_array[0]
        c_LR = c_array[1]
        u_M = u_array[0]
        u_LR = u_array[1]

    else:
        # Get the conditions at the last two gridpoints.
        c_M = c_array[-1]
        c_LR = c_array[-2]
        u_M = u_array[-1]
        u_LR = u_array[-2]

    # Create a boolean to monitor convergence.  This is changed to True
    # if the calculation converges.  It stays as False if the loop runs
    # out of iterations before converging.
    converged = False

    # Now iterate to get new values of c_N and u_N.  Our first guess
    # is that it is equal to c_M.
    c_N = c_M
    u_N = u_M
    debug_kb_001 = False
    # debug_kb_001 = True
    for iteration in range(max_iter):
        c_N_old = c_N

        # Get the conditions at the base of the characteristic.  If this is
        # the gridpoint at the back end it is a backwards characteristic.
        # If this is the gridpoint at the forward end it is a forwards
        # characteristic.  The Boolean 'back_end' is used to distinguish
        # between the two.
        if back_end:
            c_bf, u_bf = BaseBack(dtdx, c_M, u_M, c_LR, u_LR, c_N, u_N)
        else:
            c_bf, u_bf = BaseFwd(dtdx, c_M, u_M, c_LR, u_LR, c_N, u_N)

        # Calculate the Fanning friction factor, based on the
        # current value of u_bf.
        Fanning =FricFac(d_h, roughness, rr, rr37, u_bf, fric_app_num)

        # Get the friction loss term, the traffic term and the jet fan term.
        E_bfdt = fric_const * Fanning * u_bf * abs(u_bf) + JFTRdrag

        # Get a new value of u_N from the relevant characteristic equation.
        if back_end:
            u_N = u_bf + psi * (c_N - c_bf) - E_bfdt
            c_N = PortalCelerity(c_outside, u_N, zeta_in, zeta_out, psi)
            if debug_kb_001:
                if u_N <= 0.0:
                    print("In OpenEnd1 (back1)", iteration, round(c_outside, 5),
                            round(c_N_old, 5), round(c_N, 5), round(u_N, 5),
                            # " ", round(c_bf, 5), round(u_bf, 5), zeta_out,
                            # round(dtdx * (c_N - u_N), 5))
                            dtdx * (c_N - u_N))
                else:
                    print("In OpenEnd1 (back2)", iteration, round(c_outside, 5),
                            round(c_N_old, 5), round(c_N, 5), round(u_N, 5),
                            # " ", round(c_bf, 5), round(u_bf, 5), zeta_out,
                            # round(dtdx * (c_N - u_N), 5))
                            dtdx * (c_N - u_N))
        else:
            u_N = u_bf - psi * (c_N - c_bf) - E_bfdt
            c_N = PortalCelerity(c_outside, -u_N, zeta_in, zeta_out, psi)
            if debug_kb_001:
                if u_N <= 0.0:
                    print("In OpenEnd1 (fwd1) ", iteration,
                            round(c_N_old, 8), round(c_N, 8), round(u_N, 5),
                            # " ", round(c_bf, 5), round(u_bf, 5), zeta_out,
                            # round(dtdx * (c_N - u_N), 5))
                            dtdx * (c_N - u_N))
                else:
                    print("In OpenEnd1 (fwd2) ", iteration,
                            round(c_N_old, 8), round(c_N, 8), round(u_N, 5),
                            # " ", round(c_bf, 5), round(u_bf, 5), zeta_out,
                            # round(dtdx * (c_N - u_N), 5))
                            dtdx * (c_N - u_N))

        if math.isclose(c_N, c_N_old, abs_tol = converge):
            converged = True
            break

    # if back_end:
    #     print("back_end", E_bfdt)
    # else:
    #     print("fwd_end", E_bfdt)
    # When we get to here, the calculation converged or the counter of
    # iterations timed out.
    return(c_N, u_N, converged)


def PortalCelerity2(c_outside, u_inside, zeta_in, zeta_out, psi):
    '''Take the celerity of air outside a portal, the air velocity inside
    the portal, details of the connection at the portal (k-factors) and
    the constant psi.  Calculate the celerity inside the portal that
    represents the local static pressure.
    The calculation is a simplified version of the un-numbered equation
    between equations 4 and 5 in:
      Henson, D A and Fox, J A, ``An investigation of the transient flows
      in tunnel complexes of the type proposed for the Channel Tunnel'',
      Proc. Instn. Mech. Engnrs vol. 188, 1974.

        Parameters:
            c_outside       float           Celerity of stationary air outside
                                            the portal (m/s).  This may include
                                            a component of wind pressure or
                                            stack pressure.
            u_inside        float           Air velocity inside the portal
                                            (m/s).
            zeta_in         float           Pressure loss factor (k-factor)
                                            for inflow.  Applied to u_inside.
            zeta_out        float           Pressure loss factor (k-factor)
                                            for outflow.  Applied to u_inside.
            psi             float           An exponent in the isentropic
                                            flow calculation.

        Returns:
            c_inside        float           Celerity of air inside the
                                            portal (m/s), after applying the
                                            pressure losses and converting
                                            some total pressure to dynamic
                                            pressure.  c_inside is the
                                            equivalent of static pressure,
                                            not total pressure.
    '''
    gamma = 1.4
    gpow = (gamma - 1) / (2 * gamma)
    if u_inside > 0.0:
        # We have inflow from atmosphere to the portal.
        # We add a factor of 1 to zeta to represent the conversion of
        # part of the total pressure of the stationary atmosphere into
        # dynamic pressure.  The value of c thus represents the static
        # pressure just inside the portal.
        try:
            c_inside = (c_outside - 0.5 * gamma * (zeta_in + 1) * u_inside**2/c_outside)**gpow
        except:
            print("Math domain error in PortalCelerity2:",
                   c_outside, zeta_in, psi, u_inside)
            sys.exit()
    else:
        # We have outflow.
        c_inside = (c_outside + 0.5 * gamma * (zeta_out - 1) * u_inside**2/c_outside)**gpow
    return(c_inside)


def PortalCelerity(c_outside, u_inside, zeta_in, zeta_out, psi):
    '''Take the celerity of air outside a portal, the air velocity inside
    the portal, details of the connection at the portal (k-factors) and
    the constant psi.  Calculate the celerity inside the portal that
    represents the local static pressure.
    The calculation is a simplified version of equation 5 in:
      Fox, J A and Higton, N N, ``Pressure transient predictions in railway
      tunnel complexes'', Proceedings of the 3rd International Symposium on
      the Aerodynamics and Ventilation of Vehicle Tunnels (ISAVVT), BHRA, 1979.
    It may be more appropriate to use the more complex expression in Henson,
    D A and Fox, J A, ``An investigation of the transient flows in tunnel
    complexes of the type proposed for the Channel Tunnel'', Proc. Instn.
    Mech. Engnrs vol. 188, 1974.

        Parameters:
            c_outside       float           Celerity of stationary air outside
                                            the portal (m/s).  This may include
                                            a component of wind pressure or
                                            stack pressure.
            u_inside        float           Air velocity inside the portal
                                            (m/s).
            zeta_in         float           Pressure loss factor (k-factor)
                                            for inflow.  Applied to u_inside.
            zeta_out        float           Pressure loss factor (k-factor)
                                            for outflow.  Applied to u_inside.
            psi             float           An exponent in the isentropic
                                            flow calculation.

        Returns:
            c_inside        float           Celerity of air inside the
                                            portal (m/s), after applying the
                                            pressure losses and converting
                                            some total pressure to dynamic
                                            pressure.  c_inside is the
                                            equivalent of static pressure,
                                            not total pressure.
    '''
    if u_inside > 0.0:
        # We have inflow from atmosphere to the portal.
        # We add a factor of 1 to zeta to represent the conversion of
        # part of the total pressure of the stationary atmosphere into
        # dynamic pressure.  The value of c thus represents the static
        # pressure just inside the portal.
        try:
            c_inside = math.sqrt(c_outside**2 - (zeta_in + 1)/psi * u_inside**2)
        except:
            print("Math domain error in PortalCelerity:",
                   c_outside, zeta_in, psi, u_inside)
            sys.exit()
    elif math.isclose(zeta_out, 1.0):
        # We have outflow and we are losing exactly one velocity head,
        # so we can use a shortcut.  The static pressure inside the
        # tunnel equals the total pressure outside (for what it's worth,
        # it also equals the static pressure outside).
        c_inside = c_outside
    else:
        # We have outflow but we are not losing exactly one velocity
        # head. We have to do the full calculation.
        # We subtract a factor of 1 from zeta_out to represent the
        # dissipation of one dynamic head outside the portal.  If
        # the user set zeta_out to be less than one we get some static
        # regain, as if we had a diffuser at the outflow portal.
        c_inside = math.sqrt(c_outside**2 + (zeta_out - 1)/psi * u_inside**2)
    return(c_inside)


def BaseBack(dtdx, c_M, u_M, c_R, u_R, c_N, u_N):
    '''Take conditions at each end of a cell at the current timestep (c_M,
    u_M, c_R, u_R) and conditions at the end of a characteristic at the
    next timestep (c_N, u_N).  Figure out where a back characteristic
    intersects conditions at the current timestep and get interpolated
    values for the conditions at the start of the characteristic (c_b, u_b).

        Parameters:
            dtdx            float           Timestep divided by cell length
            c_M             float           Celerity at the middle gridpoint
                                            in the current timestep.
            u_M             float           Velocity at the middle gridpoint
                                            in the current timestep.
            c_R             float           Celerity at the right gridpoint
                                            in the current timestep.
            u_R             float           Velocity at the right gridpoint
                                            in the current timestep.
            c_N             float           Celerity at the middle gridpoint
                                            in the next timestep.
            u_N             float           Velocity at the middle gridpoint
                                            in the next timestep.

        Returns:
            c_b             float           Celerity at the base of the back
                                            characteristic at the current
                                            timestep.
            u_b             float           Velocity at the base of the back
                                            characteristic at the current
                                            timestep.
    '''
    # Backwards characteristics have a steep slope.
    fraction = dtdx * (c_N - u_N)

    c_b = c_M + fraction * (c_R - c_M)
    u_b = u_M + fraction * (u_R - u_M)

    return(c_b, u_b)


def BaseBack2(dtdx, c_M, u_M, c_R, u_R, c_N, u_N):
    '''Take conditions at each end of a cell at the current timestep (c_M,
    u_M, c_R, u_R) and conditions at the end of a characteristic at the
    next timestep (c_N, u_N).  Figure out where a back characteristic
    intersects conditions at the current timestep and get interpolated
    values for the conditions at the start of the characteristic
    (c_b, u_b).  Also return the fractional location of the point of
    intersection.  This is currently not used but might be used in
    future.

        Parameters:
            dtdx            float           Timestep divided by cell length
            c_M             float           Celerity at the middle gridpoint
                                            in the current timestep.
            u_M             float           Velocity at the middle gridpoint
                                            in the current timestep.
            c_R             float           Celerity at the right gridpoint
                                            in the current timestep.
            u_R             float           Velocity at the right gridpoint
                                            in the current timestep.
            c_N             float           Celerity at the middle gridpoint
                                            in the next timestep.
            u_N             float           Velocity at the middle gridpoint
                                            in the next timestep.

        Returns:
            c_b             float           Celerity at the base of the back
                                            characteristic at the current
                                            timestep.
            u_b             float           Velocity at the base of the back
                                            characteristic at the current
                                            timestep.
          fraction          float           Fractional distance from the
                                            gridpoint to the point of inter-
                                            section of the characteristic
                                            to the x-T plane at the current
                                            time.
    '''
    # Backwards characteristics have a steep slope.
    fraction = dtdx * (c_N - u_N)

    c_b = c_M + fraction * (c_R - c_M)
    u_b = u_M + fraction * (u_R - u_M)

    return(c_b, u_b, fraction)


def BaseFwd(dtdx, c_M, u_M, c_L, u_L, c_N, u_N):
    '''Take conditions at each end of a cell at the current timestep (c_M,
    u_M, c_L, u_L) and conditions at the end of a characteristic at the
    next timestep (c_N, u_N).  Figure out where a forward characteristic
    intersects conditions at the current timestep and get interpolated
    values for the conditions at the start of the characteristic (c_f, u_f).

        Parameters:
            dtdx            float           Timestep divided by cell length
            c_M             float           Celerity at the middle gridpoint
                                            in the current timestep.
            u_M             float           Velocity at the middle gridpoint
                                            in the current timestep.
            c_L             float           Celerity at the left gridpoint
                                            in the current timestep.
            u_L             float           Velocity at the left gridpoint
                                            in the current timestep.
            c_N             float           Celerity at the middle gridpoint
                                            in the next timestep.
            u_N             float           Velocity at the middle gridpoint
                                            in the next timestep.

        Returns:
            c_f             float           Celerity at the base of the forward
                                            characteristic at the current
                                            timestep.
            u_f             float           Velocity at the base of the forward
                                            characteristic at the current
                                            timestep.
          fraction          float           Fractional distance from the
                                            gridpoint to the point of inter-
                                            section of the characteristic
                                            to the x-T plane at the current
                                            time.
    '''
    # Forwards characteristics have a shallow slope.
    fraction = dtdx * (c_N + u_N)

    c_f = c_M + fraction * (c_L - c_M)
    u_f = u_M + fraction * (u_L - u_M)

    return(c_f, u_f)


def BaseFwd2(dtdx, c_M, u_M, c_L, u_L, c_N, u_N):
    '''Take conditions at each end of a cell at the current timestep (c_M,
    u_M, c_L, u_L) and conditions at the end of a characteristic at the
    next timestep (c_N, u_N).  Figure out where a forward characteristic
    intersects conditions at the current timestep and get interpolated
    values for the conditions at the start of the characteristic (c_f, u_f).
    Also return the fractional location of the point of intersection.
    This is currently not used but might be used in future.

        Parameters:
            dtdx            float           Timestep divided by cell length
            c_M             float           Celerity at the middle gridpoint
                                            in the current timestep.
            u_M             float           Velocity at the middle gridpoint
                                            in the current timestep.
            c_L             float           Celerity at the left gridpoint
                                            in the current timestep.
            u_L             float           Velocity at the left gridpoint
                                            in the current timestep.
            c_N             float           Celerity at the middle gridpoint
                                            in the next timestep.
            u_N             float           Velocity at the middle gridpoint
                                            in the next timestep.

        Returns:
            c_f             float           Celerity at the base of the forward
                                            characteristic at the current
                                            timestep.
            u_f             float           Velocity at the base of the forward
                                            characteristic at the current
                                            timestep.
    '''
    # Forwards characteristics have a shallow slope.
    fraction = dtdx * (c_N + u_N)

    c_f = c_M + fraction * (c_L - c_M)
    u_f = u_M + fraction * (u_L - u_M)

    return(c_f, u_f, fraction)


def PortalConstraints(details, area, p_atm, c_atm, gamma,
                      gammapsi, have_ftn):
    '''Take the details of what exists at the end of a segment and turn
    it into something that identifies the boundary type: fixed pressure,
    fixed velocity, fixed volume flow, node, adit.  Return details that
    later routines will use to handle the end when calculating airflow
    in the segment.

        Parameters:
            details         []              The data on a line of input.
            value           float           A pressure, velocity, volume
                                            flow or mass flow.
            area            float           The area at the portal.  This
                                            is needed for getting velocities
                                            from volume flows and adjusting
                                            pressure loss factors.
            p_atm           float           The reference air pressure (Pa)
            c_atm           float           The reference celerity (m/s)
            sectypes_dict   {}              The sectypes, as a dictionary.
                                            Provided in case we need the
                                            segment area, perimeter etc.
            gamma           float           Ratio of specific heats,
                                            typically 1.4 for air
                                            (dimensionless).
            gammapsi        float           (gamma - 1) / (2 gamma)
            have_ftn        bool            True if the Fortran routines can be
                                            used, False otherwise.

        Returns:
            c               float           The celerity (speed of sound)
                                            in the air (m/s).
    '''
    ptl_type, value = details[2:4]
    if ptl_type == 'portal':
        # We have a fixed pressure boundary.  Get the pressure
        # (gauge pressure in Pa) and turn it into a celerity (m/s).
        ptype = "p"
        descrip = "portal with gauge pressure " + str(value) + " Pa"
        if have_ftn:
            new_value = ftn.calccelerity2(value + p_atm, p_atm, c_atm, gammapsi)
        else:
            new_value = CalcCelerity2(value + p_atm, p_atm, c_atm, gammapsi)
    elif ptl_type[:2] == 'v_':
        # We have a fixed velocity boundary, either v_inflow or v_outflow.
        # Get the inflow velocity (in m/s).
        ptype = "v"
        if ptl_type == "v_outflow":
            # We need to reverse the sign of the value being set.
            new_value = -value
        else:
            new_value = value

        if math.isclose(new_value, 0.0, abs_tol = 1e-9):
            descrip = "closed portal"
        elif value < 0.0:
            descrip = "portal with inflow " + str(new_value) + " m/s"
        else:
            descrip = "portal with outflow " + str(new_value) + " m/s"
    elif ptl_type[:2] == 'q_':
        # We have a fixed volume flow boundary, either q_inflow or q_outflow.
        # Turn the inflow volume flow (in m^3/s) into an inflow velocity using
        # the area.
        ptype = "v"
        flow_text = str(value)
        if ptl_type == "q_inflow":
            new_value = value / area
        else:
            new_value = -value / area
        if math.isclose(value, 0.0, abs_tol = 1e-9):
            descrip = "closed portal"
        elif value < 0.0:
            descrip = "portal with inflow " + flow_text + " m^3/s"
        else:
            descrip = "portal with outflow " + flow_text + " m^3/s"
    elif ptl_type == 'node':
        ptype = "n"
        descrip = 'tunnel end at a node named "' + value + '"'
        # We leave "value" unchanged.  It is a word that identifies a node.
        new_value = value
    # Return the letter and the (possibly modified) value.
    return(ptype, new_value, descrip)


def FricFac(d_h, roughness, rr, rr37, veloc, fric_app_num):
    '''Take a hydraulic diameter, a roughness, relative roughness, an air
    velocity and an integer that maps to one of four friction factor
    approximations. Figure out the Fanning friction factor c_f to use in
    the calculation and return it.

        Parameters:
            d_h             float           Hydraulic diameter (m)
            roughness       float           Roughness height (m) or -ve Fanning
                                            friction factor.
            rr              float           Relative roughness (roughness/d_h)
            rr37            float           Relative roughness divided by 3.7.
            veloc           float           Air velocity (m/s)
            fric_app_num    int             An integer that maps to a friction
                                            factor approximation calculation.
                                            Valid entries are:
                                             1 for Colebrook 1939,
                                             2 for Colebrook-White (iterated)
                                             3 for Moody 1947
                                             4 for SES 1974
                                            Integers are used because this
                                            routine is called at every iteration
                                            and integers can be replaced in the
                                            Fortran routine with fast testing
                                            of integers in a case statement.

        Returns:
            Fanning         float           The Fanning friction factor
    '''
    if roughness < 0.0:
        # The user set a negative roughness, meaning they wanted a constant
        # friction factor.  Note that if they set a Darcy or Atkinson friction
        # factor we converted it to Fanning friction factor well before we
        # got here.
        Fanning = -roughness
    else:
        # Calculate the friction factor from the roughness height.
        # First figure out the Reynolds number.  We assume a constant
        # kinematic viscosity for air of 1.5E-5 m^2/s.
        Re = abs(veloc) * d_h / 0.000015

        # Set the friction factor based on the Reynolds number.  Do the
        # cases of high Reynolds number first, they're the most likely.
        # Then the case of stationary air (the next most likely) and
        # finally the flow in the laminar region and the critical zone.
        if Re >= 2300.:
            # It's in the transition or fully turbulent zones of the
            # Moody chart.
            if fric_app_num == 1:
                # Use the approximation in Colebrook's 1939 ICE paper.  This
                # is the default used in the program because it is better
                # than most of the ones that came after it and is good
                # enough for engineering work.
                Fanning = Colebrook(rr37, Re)
            elif fric_app_num == 2:
                # The user wants to iterate the Colebrook-White function.
                # Start from Colebrook's approximation.
                Fanning_old = Colebrook(rr37, Re)
                for count in range(50):
                    Fanning = 0.0625 / math.log10(rr37
                                    + 1.255/(Re * math.sqrt(Fanning_old)))**2
                    # Check if this re-assessment changed the result by less
                    # than 0.1%.  If we did, break.
                    if math.isclose(Fanning, Fanning_old, rel_tol = 0.001):
                        break
                    else:
                        # Store this value for the next iteration.
                        Fanning_old = Fanning
            elif fric_app_num == 3:
                # Use the approximation in Moody's 1947 ASME paper.
                # This is a poor approximation at high values of
                # relative roughness, but for some reason it is
                # popular.
                Fanning = 0.001375 * (1. + math.pow(20000. * rr
                                                    + 1.E6 / Re, 0.333333)
                                     )
            elif fric_app_num == 4:
                # Use the approximation in SES, a modification of Moody's
                # to improve it at high roughness.  Note that 1431083.5
                # is 16000 * 0.05^(-1.5).  Using it cuts out a division
                # and introduces an error of 0.4 parts per billion in that
                # term (I can live with that).
                Fanning = 0.001375 * (1. + math.pow(rr * (19000. + 1431083.5
                                                          * math.pow(rr, 1.5)
                                                    ) + 1.E6 / Re, 0.333333))
            else:
                # We added a new friction factor approximation but didn't
                # add code to handle it here.  Whoever added it should
                # already have seen a similar message issued ProcessCalc
                # which told them to fix it up there and here, so I think
                # we're justified in ending with sys.exit.
                print('A new type of friction factor has been added\n'
                      "but the code to handle it hasn't been added to\n"
                      'PROC FricFac in Hobyah.py.\n'
                      'Please complain to whoever added it.')
                gen.PauseFail()
                sys.exit()
        elif Re < 0.1:
            # It's practically stationary.  Use a fixed c_f of 160.
            Fanning = 160.
        else:
            # It's laminar flow or in the critical zone.  Treat it as
            # laminar flow.  We don't bother doing anything fancy to
            # handle the critical zone (the transition from laminar flow
            # to turbulent flow) because even in a very small tunnel vent
            # duct (say 1 m diameter) the air velocity to get beyond the
            # the critical zone is 2300 * 1.5E-5 / 1 = 0.035 m/s.  So,
            # too low to be worried about.
            Fanning = 16./Re
        # if False:
        #     print("In friction calc routine:",
        #           fric_app_num, d_h, veloc, Fanning)
    return(Fanning)


def Colebrook(rr37, Re):
    ''' Calculate the Fanning friction factor c_f from the approximation in
    Colebrook's 1939 ICE paper.  We only have this in a separate routine
    because we call it twice from PROC Fricfac.

        Parameters:
            rr37            float           Relative roughness divided by 3.7
                                            (roughness / (3.7 * D_h))
            Re              float           Reynolds number, assuming a
                                            fixed kinematic viscosity of
                                            1.5 *10^-5 m^2/s

        Returns:
            cf              float           Fanning friction factor calculated
                                            from the approximation in
                                            Colebrook's 1939 ICE paper.
    '''
    cf = 0.0625 / math.log10(rr37 + math.pow(7/Re, 0.9))**2
    return(cf)


def BaseConstants(settings_dict):
    '''Take the values in the settings dictionary, figure out some
    constants for the calculation and return them.

        Parameters:
            settings_dict   {}              A dictionary of various
                                            settings from the run.
            log             handle          Handle of a log file, for
                                            writing error messages to.

        Returns:
            c_atm           float           The reference celerity (m/s)
            dx              float           The minimum length of a
                                            cell (m)
            psi             float           2 / (gamma - 1)
            gammapsi        float           (gamma - 1) / (2 gamma)
    '''
    # Get the speed of sound in the atmosphere outside the tunnel.
    p_atm = settings_dict["p_atm"]
    rho_atm = settings_dict["rho_atm"]
    gamma = settings_dict["gamma"]

    c_atm = CalcCelerity1(p_atm, rho_atm, gamma)

    # Get the minimum distance between gridpoints.  This is timestep
    # multiplied by celerity (m/s) plus a safety factor (also expressed
    # in m/s).
    dx = settings_dict["aero_step"] * (c_atm + settings_dict["max_vel"])

    # Get 1/gamma, 2/(gamma - 1) and (gamma - 1)/(2 gamma).  It is
    # easiest to do it here once than calculate it in each different
    # routine that uses them (I think - might have to do some profiling
    # to see if it is actually worth it).
    psi = 2/(gamma - 1)
    gammapsi = 1 / (psi * gamma) # Note that the name is a bit misleading.

    return(c_atm, dx, psi, gammapsi)


def CalcCelerity2(P, P_atm, c_atm, gammapsi):
    '''Take a pressure, a reference pressure and a reference celerity.
    Return the speed of sound at the pressure using the expression for
    speed of sound for a reversible isentropic change.

        Parameters:
            P               float           The air static pressure (Pa)
            P_atm           float           The reference air pressure (Pa)
            c_atm           float           The reference celerity (m/s)
            gammapsi        float           (gamma - 1) / (2 gamma)

        Returns:
            c               float           The celerity (speed of sound)
                                            in the air (m/s).
    '''
    if math.isclose(P, P_atm, abs_tol = 1e-9):
        # The pressure is so close to outside atmosphere that we might
        # as well use the celerity of outside amosphere too.
        c = c_atm
    else:
        c = c_atm * math.pow(P / P_atm, gammapsi)
    return(c)


def CalcCelerity1(P, rho, gamma):
    '''Take a pressure, a density and the ratio of specific heats,
    return the speed of sound using the expression for speed of
    sound in a perfect gas.

        Parameters:
            P               float           The air static pressure (Pa)
            rho             float           The air density (kg/m^3)
            gamma           float           Ratio of specific heats,
                                            typically 1.4 for air
                                            (dimensionless).

        Returns:
            c               float           The celerity (speed of sound)
                                            in the air (m/s).
    '''
    c = math.sqrt(gamma * P / rho)
    return(c)


def ProcessTunnel(line_triples, tr_index, settings_dict, sectypes_dict, log):
    '''Read all the data defining a tunnel and add an entry for it into the
    tunnels dictionary.  Return the updated tunnels dictionary.

        Parameters:
            line_triples [(int, str, str)]  List of lines in the file.
            tr_index        int             Pointer to where the block starts
                                            in line_triples.
            settings_dict   {}              Dictionary of the run settings.
            sectypes_dict   {}              The sectypes, as a dictionary, for
                                            checking.
            log             handle          The handle of the logfile.

        Returns:
            tunnel_name     str             Name of the new tunnel, used as
                                            a dictionary.
            new_tun_dict    {}              Dictionary defining a new tunnel.

        Errors:
            Aborts with 2201 if the name of the sectype at the back end of
            the tunnel is not in the sectypes block.
            Aborts with 2202 if the name of the sectype at a change of sectype
            is not in the sectypes block.
            Aborts with 2203 if the name of the tunnel started with a "-".
            This is not allowed because it conflicts with the orientation
            of tunnels in routes.
            Aborts with 2204 if the distance at the back end is above
            the distance at the forward end.
            Aborts with 2205 if an entity (area change, fan or whatnot)
            is at a chainage outwith the tunnel.
            Aborts with 2206 if the tunnel has nodes at each end and the
            name of the nodes are the same (the tunnel is like a doughnut).
            Aborts with 2207 if two joins in the tunnel set by separate
            "join" commands gave the same name to the join.
            Aborts with 2208 if two joins in the tunnel had the same name
            and one or both was set by a "joins" command.
            Aborts with 2209 if two entities are at the same chainage.
            Aborts with 2210 if two entities were closer than 1 metre.
    '''
    debug1 = settings_dict["debug1"]
    file_name = settings_dict["file_name"]
    units = settings_dict["units"]
    # Create a dictionary to hold the data for this tunnel.

    # Define the entries for the keywords.  Each entry for a keyword
    # has a list of the entries that it must have and the names of
    # the optional keywords.  This best explained by example.
    #
    # Take the keyword "back".  The required entries are a chainage (float)
    # and a sectype to use (any string).  There is an optional descriptive
    # entry (QAstr) and any weird stuff (such as height) can be set as optional
    # entries.  They cannot be duplicated so they don't appear in the
    valid_settings = {
                      # Required keywords in the first dictionary.  Each keyword has
                      # two or three entries: the type of entry ("int" = "integer",
                      # "float" = floating point number, "#name" = any string (the
                      # case of the text will be preserved), ("string",) = one of a given
                      # list of strings, all of which will be converted to lower case.
                      # Note that #name consumes one word while QAstr consumes
                      # all the words on the rest of the line.
                      "back":   ("float any dist1 a chainage",
                                 ("portal float any press1 a portal pressure",
                                 "v_inflow float any speed1 an inflow velocity",
                                 "q_inflow float any volflow an inflow volume flow",
                                 "v_outflow float any speed1 an outflow velocity",
                                 "q_outflow float any volflow an outflow volume flow",
                                 "node #name"),
                                 "#name",  # Name of the sectype at the back end
                                 "QAstr"),
                      "fwd":    ("float any dist1 a chainage",
                                 ("portal float any press1 a portal pressure",
                                 "v_inflow float any speed1 an inflow velocity",
                                 "q_inflow float any volflow an inflow volume flow",
                                 "v_outflow float any speed1 an outflow velocity",
                                 "q_outflow float any volflow an outflow volume flow",
                                 "node #name"),
                                 "QAstr"),
                      "change": ( "float any dist1 a chainage",
                                  "#name", "QAstr"),  # new sectype name
                                  # Use optional arguments to trigger fans.
                      "fan1":   ( "float any dist1 a chainage",
                                  "#name",  # Name to refer to this fan by in plots
                                  "#name",  # Name of a fanchar definition
                                  "float any null a fan speed", # a fixed fan speed
                                  "QAstr",),
                      "fan2":   ( "float any dist1 a chainage",
                                  "#name",  # Name to refer to this fan by in plots
                                  "#name",  # Name of a fanchar definition
                                  "#name",  # Nickname of a data source
                                  "#name",  # Name or number of the column setting time
                                  "#name",  # Name or number of a column setting speed
                                  "QAstr"),
                      "jetfans1":("float any dist1 a chainage",
                                  "#name",  # Name to refer to this bank of
                                            # jet fans by in plots
                                  "float 0+ null a count of jetfans",
                                  "#name",  # Name of a jet fan type
                                  "float any null a fan speed", # a fixed fan speed
                                  "QAstr"),
                      "loss1":  ( "float any dist1 a chainage",
                                  "float +  dist1 an area",
                                  "float 0+ null a k-factor",         # zeta_bf
                                  "float 0+ null a k-factor",         # zeta_fb
                                  "QAstr"),
                      "loss2":  ( "float any dist1 a chainage",
                                  "float 0+ atk a resistance",        # R_bf
                                  "float 0+ atk a resistance",        # R_fb
                                  "QAstr"),
                      "damper1":( "float any dist1 a chainage",
                                  "#name",     # Name to refer to this damper by
                                  "#name",     # Nickname of a data source
                                  "#name",     # Name or number of the column setting time
                                  "#name",     # Name or number of a column setting area
                                  "#name",     # Name or number of the column setting zeta_bf
                                  "#name",     # Name or number of the column setting zeta_fb
                                  "QAstr"),
                      "damper2":( "float any dist1 a chainage",
                                  "#name",     # Name to refer to this damper by
                                  "#name",     # Nickname of a data source
                                  "#name",     # Name or number of the column setting time
                                  "#name",     # Name or number of the column setting R_bf
                                  "#name",     # Name or number of the column setting R_fb
                                  "QAstr"),
                      "join":   ( "float any dist1 a chainage",
                                  "#name",
                                  "QAstr"),
                      "joins":  ( "QAstr",),  # We don't process this in ProcessBlock
                  "numbering":  ( "QAstr",),  # We don't process this ProcessTunnel
                   "sectypes":  ( "QAstr",),  # We don't process this ProcessTunnel
                   "sespragmat": ("int + null  SES segment numbering range",
                                  ("line", "vent"))
                     }
    #
    # Define the optional entries allowed in each keyword
    optionals = {"back":  {"zeta_bf": "float 0+ null   a k-factor",
                           "zeta_fb": "float 0+ null   a k-factor",
                           "zeta_in": "float 0+ null   an inflow k-factor",
                           "zeta_out": "float 0+ null   an outflow k-factor",
                           "area": "float 0+ dist1   a portal area",
                          },
                 "fwd":   {"zeta_bf": "float 0+ null   a k-factor",
                           "zeta_fb": "float 0+ null   a k-factor",
                           "zeta_in": "float 0+ null   an inflow k-factor",
                           "zeta_out": "float 0+ null   an outflow k-factor",
                           "area": "float 0+ dist1   a portal area",
                          },
                 "change":{"zeta_bf": "float 0+ null   a k-factor",
                           "zeta_fb": "float 0+ null    a k-factor",
                           "height": "float any dist1   a height",
                           "area": ("smaller", "larger", "back", "fwd"),
                          },
                 "join":  {"zeta_bf1": "float 0+ null   a k-factor",
                           "zeta_fb1": "float 0+ null   a k-factor",
                           "zeta_bf2": "float 0+ null   a k-factor",
                           "zeta_fb2": "float 0+ null   a k-factor",
                           "sectype": "QAstr",
                          },
                 "joins": {"zeta_bf1": "float 0+ null   a k-factor",
                           "zeta_fb1": "float 0+ null   a k-factor",
                           "zeta_bf2": "float 0+ null   a k-factor",
                           "zeta_fb2": "float 0+ null   a k-factor",
                           "sectype": "QAstr",
                           "start": "int 0+ null   a counter",
                          },
                 "fan1":  {"start": "float 0+ null   a fan start time",
                           "stop": "float + null   a fan stop time",
                           "runup": "float + null   a fan runup time",
                           "rundown": "float + null   a fan rundown time",
                          },
                 "jetfans1":{"start": "float 0+ null   a fan start time",
                             "stop": "float + null   a fan stop time",
                             "runup": "float + null   a fan runup time",
                             "rundown": "float + null   a fan rundown time",
                          },
                 "sespragmat":{"route": "QAstr"},
                }
    #
    # Make a list of what entries we must have.
    requireds = ("back", "fwd")
    #
    # Make a list of what entries can be duplicated, as long as their
    # first number (chainage) has not already been used.
    duplicates = ("change", "loss1", "loss2", "fan1", "fan2",
                  "jetfans1", "join", "joins")

    settings = (valid_settings, requireds, optionals, duplicates)
    block_name = "tunnel"
    result = ProcessBlock(line_triples, tr_index, settings_dict,
                          block_name, {}, settings, log)
    if result is None:
        return(None)
    else:
        (tunnel_name, new_tun_dict) = result

    # Remove any entries for the "numbering" and "sectypes" lines of
    # entry.  These are processed in ProcessClones instead of here,
    # but we need to keep them in the copy of line_triples passed by
    # ProcessClones so as to keep the line numbering correct.
    keys = list(new_tun_dict.keys())
    for key in keys:
        if key[:9] == "numbering" or key[:8] == "sectypes":
            new_tun_dict.pop(key)

    # Make a dictionary.  The keys are the names of joins already defined
    # by the "join" keyword.  The values returned are the indices (in
    # line_triples) of the lines that defined them.  This helps us catch
    # duplicate join names when we generate new joins from the "joins"
    # keyword (done in the loop below).  While we're doing this, we can
    # also check for duplicate join names set by "join" commands.
    join_names = {}
    for key in new_tun_dict:
        if key[:5] == "join#":
            jname = new_tun_dict[key][1]
            if jname in join_names:
                # A join with this name already exists.  Get its line
                # for an error message.
                old_index = join_names[jname]
                line_num1, discard, line1_text = line_triples[old_index]
                new_index = new_tun_dict[key][-1]
                line_num2, discard, line2_text = line_triples[new_index]
                err = ('> In the file named "' + file_name + '"\n'
                       '> there were two joins with the same name in\n'
                       '> tunnel "' + tunnel_name
                         + '", both called "' + jname + '".\n'
                       '> Please edit the file to distinguish between\n'
                       '> them.'
                      )
                gen.WriteError(2207, err, log)
                gen.ErrorOnTwoLines(line_num1, line1_text,
                                    line_num2, line2_text,log, False)
                return(None)
            else:
                join_names.__setitem__(new_tun_dict[key][1],
                                       new_tun_dict[key][-1])
    # Now check that the sectypes used all exist and process any definitions
    # of multiple joins.  We make a new dictionary to store each join
    # generated, then merge it afterwards.
    new_joins_dict = {}
    for key, tun_settings in new_tun_dict.items():
        line_index = tun_settings[-1]
        # Get data for error messages.
        line_number, line_data, line_text = line_triples[line_index]
        if key == "back":
            # Check the name of the sectype.
            # "back":     "float any dist1 a chainage",
            #             ("portal float", "node #name"),
            #             "#name",
            #             "QAstr"
            name = tun_settings[-4].strip()
            if name.lower() not in sectypes_dict:
                err = ('> In the file named "' + file_name + '"\n'
                       '> the name of the sectype at the back end\n'
                       '> of the tunnel named "' + tunnel_name + '" does not\n'
                       '> exist (the name given was "' + name + '").\n'
                       '> Please edit the file to correct it.  For\n'
                       "> what it's worth, here are the names of\n"
                       "> the sectype(s) you've set:\n"
                         + gen.FormatOnLines(sectypes_dict.keys()))
                gen.WriteError(2201, err, log)
                gen.ErrorOnLine(line_number, line_text, log, False)
                return(None)
        elif key[:7] == "change#":
            # Check the name in an area change.
            # "change":   "float any dist1 a chainage",
            #             "#name",
            #             "QAstr")
            name = tun_settings[1].strip()
            if name.lower() not in sectypes_dict and name != "same":
                err = ('> In the file named "' + file_name + '"\n'
                       '> the name of the sectype in a change of\n'
                       '> sectype in tunnel "' + tunnel_name + '" does not\n'
                       '> exist (the name given was "' + name + '").\n'
                       '> Please edit the file to correct it.  For\n'
                       "> what it's worth, here are the names of\n"
                       "> the sectype(s) you've set:\n"
                         + gen.FormatOnLines(sectypes_dict.keys()))
                gen.WriteError(2202, err, log)
                gen.ErrorOnLine(line_number, line_text, log, False)
                return(None)
        elif key[:6] == "joins#":
            # The joins keyword is converted into multiple "join" entries
            # in the tunnel dictionary.  We process it here instead of
            # in ProcessBlock because the first entry could be a list
            # generator for the chainages enclosed in double quotes or
            # single quotes with whitespace in it, and ProcessBlock can
            # only split on whitespace.
            # Call a routine that looks for one word or a phrase (with
            # or without spaces in it) enclosed in single quotes or
            # double quotes.
            result = gen.WordOrPhrase(tun_settings[0], "a list of chainages",
                                  line_number, line_text, file_name,
                                  debug1, log)
            if result is None:
                return(None)
            else:
                join_text, rest_of_line = result

            # Check if we have a word on the rest of the line to
            # use as the root of the name of the join.  If there isn't
            # anything, we use a number as the join name.
            # If the word contains "*" we put a number in its place
            # and use the parts of the word on either side of the "*"
            # This lets us create joins called "XP1up", "XP2up",
            # "XP3up", "XP4up" in one tunnel by using "XP*up" and
            # joins called "XP1down", "XP2down", "XP3down", "XP4down"
            # by using "XP*down" in another tunnel.
            if rest_of_line.strip() == "":
                jroot1 = ""
                jroot2 = ""
            else:
                jroot1 = rest_of_line.lower().split()[0]
                # Check for "*" and split there  if we find it.
                #
                if "*" in jroot1:
                    # If there is more than one "*", we ignore all
                    # except the first.
                    jroot1, jroot2 = jroot1.split(sep = "*", maxsplit = 1)
                else:
                    jroot2 = ""
            # Everything else on the line is comment (QAstr).
            splits = rest_of_line.split(maxsplit = 1)
            if len(splits) == 2:
                comment = splits[1]
            else:
                comment = ""


            # Expand out the chainages.
            if debug1:
                print("Checking for join chainages in", join_text)
            result = CheckListAndRange(join_text, "spoof_it",
                                       settings_dict, line_triples,
                                       line_number, line_text, False, log)
            if result is None:
                return(None)
            else:
                join_chs = result
                # It is possible to use the "joins" keyword but give it
                # one number, which is returned as a float instead of a
                # list.  There's no point (better to use "join" instead),
                # but it is not forbidden.  We turn the lone float into
                # a one-element list so that the code below works.
                if type(join_chs) is float:
                    join_chs = [join_chs]

            # Now figure out where we need to start numbering the name
            # of the joins from.
            if "start" in tun_settings[-2]:
                jstart = int(tun_settings[-2]["start"])
            else:
                # Start the numbering at 1.
                jstart = 1

            # Now generate a "join" entry for every chainage in the "joins"
            # command.  We catch duplicate join names in this tunnel here
            # too, although we will also do a similar check later to look
            # for joins with the same name in different tunnels.
            for index, join_ch in enumerate(join_chs, start = jstart):
                jname = jroot1 + str(index) + jroot2
                new_key = "join#" + "_" + jname
                if jname in join_names:
                    # A join with this name already exists.  Get its line
                    # for an error message.
                    old_index = join_names[jname]
                    line_num1, discard, line1_text = line_triples[old_index]
                    err = ('> In the file named "' + file_name + '"\n'
                           '> there were two joins with the same name in\n'
                           '> tunnel "' + tunnel_name
                             + '", both called "' + jname + '".\n'
                           '> Please edit the file to distinguish between\n'
                           '> them.  Note that one or both may have been\n'
                           '> generated by a "joins" command, so it may not\n'
                           '> be immediately obvious how the conflict occurred.'
                          )
                    gen.WriteError(2208, err, log)
                    gen.ErrorOnTwoLines(line_num1, line1_text,
                                        line_number, line_text,log, False)
                    return(None)
                else:
                    new_value = (join_ch, jname, comment, tun_settings[-2],
                                 tun_settings[-1])
                    new_joins_dict.__setitem__(new_key, new_value)
                    join_names.__setitem__(jname, tun_settings[-1])
    # Add the new joins to the tunnel dictionary.
    new_tun_dict.update(new_joins_dict)


    # Check the tunnel names for forbidden characters.
    if tunnel_name[0] == '-':
        err = ('> In the file named "' + file_name + '"\n'
               '> the name of tunnel "' + tunnel_name + '" is not\n'
               '> valid because it starts with a "-", which\n'
               '> is a reserved character for routes.\n'
               '> Please edit the file to correct it.'
              )
        gen.WriteError(2203, err, log)
        gen.ErrorOnLine(line_number, line_text, log, False)
        return(None)


    # Now check that intermediate chainages are between the back chainage
    # and the forward chainage.  While we're doing that, we build a list
    # of chainages of entities that we can use in a later block to pick
    # up entities that are too close together (closer than one metre).
    back = new_tun_dict["back"]
    fwd = new_tun_dict["fwd"]
    back_ch = back[0]
    fwd_ch = fwd[0]

    if back_ch > fwd_ch:
        line_num1, discard, line_text1 = line_triples[back[-1]]
        line_num2, discard, line_text2 = line_triples[fwd[-1]]
        err = ('> In the file named "' + file_name + '"\n'
               '> the tunnel named "' + tunnel_name + '" has its\n'
               '> back end located above its forward end,\n'
               '> which is not allowed.\n'
               '> Please edit the file to correct it.'
              )
        gen.WriteError(2204, err, log)
        gen.ErrorOnTwoLines(line_num1, line_text1,
                            line_num2, line_text2, log, False)
        return(None)

    # This list has a chainage and the tr_index of the line it came from,
    # so we can print the clashing lines of input.  We start with the back
    # end and forward end of the tunnel.
    chs = [(back_ch, back[-1]), (fwd_ch, fwd[-1])]

    for key, settings in new_tun_dict.items():
        if key not in ("back", "fwd", "sectypes", "numbering",  \
                       "sespragmat", "block_index") and key[:6] != "joins#":
            location = settings[0]
            line_index = settings[-1]
            if not(back_ch < location < fwd_ch):
                entity, distance = key.split(sep='#')
                err = ('> In the file named "' + file_name + '",    \n'
                       '> the tunnel named "' + tunnel_name + '" had an\n'
                       '> entity ("' + entity + '") at ' + str(location)
                         + ' that is\n'
                       '> not inside tunnel (which goes from\n'
                       '> ' + gen.RoundText(back_ch, 3) + ' to '
                         + gen.RoundText(fwd_ch, 3) + ').\n'
                       '> Please edit the file to correct it.'
                      )
                gen.WriteError(2205, err, log)
                gen.ErrorOnLine2(line_index, line_triples, log, False)
                return(None)
            else:
                # Append a tuple of the chainage and the index of the line
                # it came from to the list of chainages.  We use this in a
                # double loop below to find entities that are too close to
                # one another.
                chs.append((location, line_index))
    # Check for bad topology.
    if (back[1] == fwd[1] == "node") and back[2].lower() == fwd[2].lower():
        # Both tunnel ends are at nodes and the nodes have the same name.
        # The tunnel is a doughnut, which can lead to problems when
        # deciding which way to run a route through the tunnel.  So
        # we raise an error.
        line1_num, discard, line1_text = line_triples[back[-1]]
        line2_num, discard, line2_text = line_triples[fwd[-1]]
        err = ('> In the file named "' + file_name + '"\n'
               '> tunnel "' + tunnel_name + '" started and ended at\n'
               '> the same node (named "' + back[2].lower()
                 + '").  This is not\n'
               '> allowed.  Please edit the file to correct this,\n'
               '> either by correcting the node names or splitting\n'
               '> the tunnel in two.'
              )
        gen.WriteError(2206, err, log)
        gen.ErrorOnTwoLines(line1_num, line1_text,
                            line2_num, line2_text,
                            log)
        return(None)

    # Check for things that are too close together.  We complain if
    # things are closer than 1 metre (someone, somewhere will want
    # to use a timestep less than 0.0025 seconds now that computers
    # are ridiculously fast).
    for (index, (out_ch, out_index)) in enumerate(chs[:-1]):
        for (in_ch, in_index) in chs[index + 1:]:
            if abs(out_ch - in_ch) <= 1.0:
                # We have two things that are too close.  Figure out
                # what the first one is.
                prev_line = line_triples[out_index]
                current_line = line_triples[in_index]
                if units == "si":
                    # Allow one metre.  This would work with an aero timestep
                    # of 0.0027 sec but that isn't really worth using in the
                    # tunnel vent field.
                    errtext = "one metre."
                else:
                    # Allow 3.28 feet
                    errtext = "3.28 feet."
                if math.isclose(out_ch, in_ch, abs_tol = 1e-9):
                    # They are at the same chainage.
                    err = ('> Came across two entities that are at the\n'
                           '> same location in "' + file_name + '".\n'
                           "> Entities can't be closer than " + errtext
                          )
                    gen.WriteError(2209, err, log)
                    gen.ErrorOnTwoLines(prev_line[0], prev_line[2],
                                        current_line[0], current_line[2],
                                        log, False)
                    return(None)
                else:
                    # They are less than 1 metre apart.
                    err = ('> Came across two entities that are too close\n'
                           '> in "' + file_name + '".\n'
                           "> Entities can't be closer than " + errtext
                          )
                    gen.WriteError(2210, err, log)
                    gen.ErrorOnTwoLines(prev_line[0], prev_line[2],
                                        current_line[0], current_line[2],
                                        log, False)
                    return(None)
    # If we get to here, all is well.
    return(tunnel_name, new_tun_dict)


def ProcessClones(line_triples, tr_index, settings_dict, tunnels_dict,
                  sectypes_dict, log):
    '''Read the data defining a group of tunnels (they're called clones
    but they may actually differ).  Generate a list of lines to generate
    each tunnel and call ProcessTunnel to build them.
    The tunnels differ in their names.  The names which must
    incorporate a "*", which is replaced by a number that is incremented
    for each clone.  Likewise, any joins they connect to, fans and dampers
    in the cloned tunnels must have a "*" in their names, so that the
    same number can be included in the names of the entities.
    Tunnels may also have different initial sectypes, so that when you
    define (say) 100 cross-passages, you can give 97 the sectype
    appropriate to closed cross-passage doors and three the sectype
    appropriate to open cross-passage doors.

        Parameters:
            line_triples [(int, str, str)]  List of lines in the file.
            tr_index        int             Pointer to where the block
                                            starts in line_triples.
            settings_dict   {}              Dictionary of the run settings.
            tunnels_dict    {}              Dictionary of the tunnels, for
                                            checking
            sectypes_dict   {}              The sectypes, as a dictionary,
                                            for checking.
            log             handle          The handle of the logfile.

        Returns:
            clone_tun_dict  {}              Dictionary defining a set of
                                            new tunnels, which are added
                                            to tunnels_dict.
            tunnel_lines    [str]           A list of lines that contain
                                            one block of tunnel definition
                                            for each clone.  These lines
                                            are written to the log file
                                            so that users can copy them
                                            out and put them in their
                                            input files if they need to
                                            make modifications that the
                                            "tunnelclones" block can't
                                            handle.

        Errors:
            Aborts with 2681 if the name of a tunnel did not have a '*'
            character in it (to be replaced by a number).
            Aborts with 2682 if one of the automated names clashed with
            the name of an existing tunnel.
            Aborts with 2683 if the name of the sectype at the back end
            of the set of cloned tunnels was not valid.
            Aborts with 2684 if the sectypes in the back end pointed to
            a list in a "sectypes" keyword but there was no line of
            entry starting with "sectypes".
            Aborts with 2685 if the length of the list of sectypes was
            not the same as the count of tunnels being generated.
            Aborts with 2686 if a list of sectypes contained an invalid
            name of a sectype.
            Aborts with 2687 if a list of sectypes contained more than
            one invalid names of sectypes.
    '''
    debug1 = settings_dict["debug1"]
    file_name = settings_dict["file_name"]
    units = settings_dict["units"]
    # Create a dictionary to hold the data for this tunnel.

    valid_settings = {
                      "back":   ("float any dist1  a chainage",
                                 ("portal float any press1 a portal pressure",
                                 "v_inflow float any speed1  an inflow velocity",
                                 "q_inflow float any volflow  an inflow volume flow",
                                 "v_outflow float any speed1  an outflow velocity",
                                 "q_outflow float any volflow  an outflow volume flow",
                                 "node #name"),
                                 "#name",  # Name of the sectype at the back end
                                 "QAstr"),
                      "fwd":    ("float any dist1 a chainage",
                                 ("portal float any press1  a portal pressure",
                                 "v_inflow float any speed1  an inflow velocity",
                                 "q_inflow float any volflow  an inflow volume flow",
                                 "v_outflow float any speed1  an outflow velocity",
                                 "q_outflow float any volflow  an outflow volume flow",
                                 "node #name"),
                                 "QAstr"),
                      "fan1":   ( "float any dist1 a chainage",
                                  "#name",  # Name to refer to this fan by in plots
                                  "#name",  # Name of a fanchar definition
                                  "float any null a fan speed", # a fixed fan speed
                                  "QAstr",),
                      "fan2":   ( "float any dist1 a chainage",
                                  "#name",     # Name to refer to this fan by
                                  "#name",     # Name of a fanchar definition
                                  "#name",     # Nickname of a data source
                                  "#name",     # Name or number of the column setting time
                                  "#name",     # Name or number of a column setting speed
                                  "QAstr"),
                      "damper1":( "float any dist1 a chainage",
                                  "#name",     # Name to refer to this damper by
                                  "#name",     # Nickname of a data source
                                  "#name",     # Name or number of the column setting time
                                  "#name",     # Name or number of a column setting area
                                  "#name",     # Name or number of the column setting zeta_bf
                                  "#name",     # Name or number of the column setting zeta_fb
                                  "QAstr"),
                      "damper2":( "float any dist1 a chainage",
                                  "#name",     # Name to refer to this damper by
                                  "#name",     # Nickname of a data source
                                  "#name",     # Name or number of the column setting time
                                  "#name",     # Name or number of the column setting R_bf
                                  "#name",     # Name or number of the column setting R_fb
                                  "QAstr"),
                      "numbering": ("QAstr",),# A range of numbers
                      "sectypes":("QAstr",),  # A range of sectype names.
                      "#name": ("QAstr",)     # This catches all other lines,
                                              # which are fed to ProcessTunnel.
                     }
    #
    # Define the optional entries allowed in each keyword.  These are copied
    # from the tunnels block and should match them.
    optionals = {"back":  {"zeta_bf": "float 0+ null   a k-factor",
                           "zeta_fb": "float 0+ null   a k-factor",
                           "zeta_in": "float 0+ null   an inflow k-factor",
                           "zeta_out": "float 0+ null   an outflow k-factor",
                           "area": "float 0+ dist1   a portal area",
                          },
                 "fwd":   {"zeta_bf": "float 0+ null   a k-factor",
                           "zeta_fb": "float 0+ null   a k-factor",
                           "zeta_in": "float 0+ null   an inflow k-factor",
                           "zeta_out": "float 0+ null   an outflow k-factor",
                           "area": "float 0+ dist1   a portal area",
                          },
                 "fan1":  {"start": "float 0+ null   a fan start time",
                           "stop": "float + null   a fan stop time",
                           "runup": "float + null   a fan runup time",
                           "rundown": "float + null   a fan rundown time",
                          },
                }
    #
    # Make a list of what entries we must have.
    requireds = ("back", "fwd", "numbering")
    #
    # Make a list of what entries can be duplicated, as long as their
    # first number (chainage) has not already been used.
    duplicates = ()

    settings = (valid_settings, requireds, optionals, duplicates)
    block_name = "tunnelclones"
    result = ProcessBlock(line_triples, tr_index, settings_dict,
                          block_name, {}, settings, log)
    if result is None:
        return(None)
    else:
        (basename, clone_dict) = result

    # First check if the base name has a "*" in it (which this code
    # replaces with a number before it calls ProcessTunnel).
    (line_number, line_data, line_text) = line_triples[tr_index]
    # base = line_data.split()[-1]
    base = clone_dict["block_index"][1]
    if "*" not in basename:
        err = ('> In the file named "' + file_name + '"\n'
               '> the name of a set of cloned tunnels did not\n'
               '> have a "*" character in it, meaning that its\n'
               '> tunnels could not be generated with numbers\n'
               '> in place of the "*".  Please add one, e.g. "' + base + '*".'
              )
        gen.WriteError(2681, err, log)
        gen.ErrorOnLine(line_number, line_text, log, False)
        return(None)

    result = CheckAsterisks(base, "name of a set of cloned tunnels",
                            file_name, line_number, line_text, log)
    if result is None:
        return(None)
    else:
        (tname, tstring) = result


    # Now we get the range of numbers.  This is in the "numbering" line.
    range_line = clone_dict["numbering"]
    (line_number, line_data, line_text) = line_triples[range_line[-1]]
    result = gen.WordOrPhrase(range_line[0], "a list of tunnel numbers",
                          line_number, line_text, file_name,
                          debug1, log)
    if result is None:
        return(None)
    else:
        range_text, rest_of_line = result
    # Expand out the list of numbers.
    result = CheckListAndRange(range_text, "spoof_it",
                               settings_dict, line_triples,
                               line_number, line_text, False, log)
    if result is None:
        return(None)
    else:
        t_nums = result
        t_names = BuildNameList(t_nums, tname, tstring)


    # Check if any of the names of the cloned tunnels clash with the
    # name of an individually-defined tunnel or another cloned tunnel.
    for t_name in t_names:
        low_name = t_name.lower()
        if low_name in tunnels_dict:
            # Get the line that defined the tunnel name for the error
            # message.  This is not an ideal message, we really want
            # the line (or lines) defining the other tunnel, but we
            # don't have those in the dictionary.
            (line2_number, discard, line2_text) = line_triples[tr_index]
            ex_index = tunnels_dict[low_name]["block_index"][-1]
            (line3_number, discard, line3_text) = line_triples[ex_index]
            err = ('> In the file named "' + file_name + '"\n'
                   '> a set of cloned tunnels generated a tunnel\n'
                   '> name ("' + t_name + '") that clashed with the \n'
                   '> name of another tunnel.  Please edit the\n'
                   '> file to resolve the name clash'
                  )
            gen.WriteError(2682, err, log)
            gen.ErrorOnThreeLines(line3_number, line3_text,
                                  line3_number, line2_text,
                                  line_number, line_text, log, False)
            return(None)

    # Get the count of new tunnels.
    count = len(t_names)
    # Check if the forward end is at a node and build a list of the
    # node names (using the same numbers that were used to build the
    # names of the tunnels.  If the forward end is not a node, we make
    # a note of that.
    fwd_end = clone_dict["fwd"]
    fwd_index = fwd_end[-1]
    (fwd_number, fwd_data, fwd_text) = line_triples[fwd_index]
    fname = fwd_data.split()[2]
    if fwd_end[1] == "node":
        fname = fwd_end[2]
        if "*" in fname:
            # The user wants to use the number put into the tunnel names
            # in the nodes too.
            result = CheckAsterisks(fname, "name of a set of forward end nodes",
                                    file_name, fwd_number, fwd_text, log)
            if result is None:
                return(None)
            else:
                (fname, fstring) = result
                # This is a list of the node names at the back end.
                f_names = BuildNameList(t_nums, fname, fstring)
        else:
            # This is a list of the node names at the back end.
            f_names = (fname,) * count
    else:
        # This is not a node at the forward end, it is a portal or
        # something else.
        f_names = "#notnode"

    # Now do the same for the back end.
    back_end = clone_dict["back"]
    back_index = back_end[-1]
    (back_number, back_data, back_text) = line_triples[back_index]
    bname = back_data.split()[2]
    if back_end[1] == "node":
        bname = back_end[2]
        if "*" in bname:
            # The user wants to use the number put into the tunnel names
            # in the nodes too.
            result = CheckAsterisks(bname, "name of a set of back end nodes",
                                    file_name, back_number, back_text, log)
            if result is None:
                return(None)
            else:
                (bname, bstring) = result
                b_names = BuildNameList(t_nums, bname, bstring)
        else:
            # It's a bit weird, but these clone tunnels all attach to the
            # same node.  This isn't forbidden but it is unusual.
            # This is a list of the node names at the back end.
            b_names = (bname,) * count
    else:
        # This is not a node at the back end, it is a portal or
        # something else.
        b_names = "#notnode"

    # Check to see if the tunnel has the name of a sectype or the word
    # "sectypes".  If the word is "sectypes", we read another line of
    # entry with the keyword "sectypes" with a list of sectypes in it
    # and use that.
    sectype = back_end[3].lower()
    if sectype != "sectypes":
        # The user set the name of a sectype, we hope.
        if sectype not in sectypes_dict:
            err = ('> In the file named "' + file_name + '",\n'
                   '> the back end of a set of cloned tunnels used\n'
                   '> the name of a sectype ("' + sectype
                     + '") that was not\n'
                   '> valid.  Please either use a valid name or\n'
                   '> use the word "sectypes" and include a line\n'
                   '> defining a list of names of sectypes.  For\n'
                   "> what it's worth, here are the names of the\n"
                   "> sectype(s) you've set:\n"
                      )
            err = err + gen.FormatOnLines(tuple(sectypes_dict.keys()))
            gen.WriteError(2683, err, log)
            gen.ErrorOnLine(back_number, back_text, log, False)
            return(None)
        else:
            # Make a list of names of the sectype.
            secnames = (sectype,)*count
    else:
        # If we get to here, the word was "sectypes".  Check that the
        # "sectypes" entry exists.
        if "sectypes" not in clone_dict:
            err = ('> In the file named "' + file_name + '",\n'
                   '> the back end of a set of cloned tunnels indicated\n'
                   '> that the sectypes for the tunnels were set by\n'
                   '> a "sectypes" line of entry, but there was no\n'
                   '> such line of entry.  Please either add one or\n'
                   '> change the name at the back end of the tunnel\n'
                   '> to be the name of a valid sectype.'
                  )
            gen.WriteError(2684, err, log)
            gen.ErrorOnLine(back_number, back_text, log, False)
            return(None)
        else:
            sectypes = clone_dict["sectypes"]
            (line_number, line_data, line_text) = line_triples[sectypes[-1]]
        result = gen.WordOrPhrase(sectypes[0], "a list of sectypes",
                              line_number, line_text, file_name,
                              debug1, log)
        if result is None:
            return(None)
        else:
            (sec_list, rest_of_line) = result

        # Expand out the list of names.  We send it a True argument
        # so that it accepts words as entries.
        result = CheckListAndRange(sec_list, "spoof_it",
                                   settings_dict, line_triples,
                                   line_number, line_text, True, log)
        if result is None:
            return(None)
        else:
            secnames = result
        # Check the length of the list of secnames and complain if
        # it is not the same as the count of tunnels.
        if len(secnames) != count:
            numbering = clone_dict["numbering"]
            (line2_number, discard, line2_text) = line_triples[numbering[-1]]
            err = ('> In the file named "' + file_name + '",\n'
                   '> the list of sectypes for some cloned tunnels\n'
                   '> was not the same length as the list of tunnel\n'
                   '> numbers.  Please ensure they are the same length\n'
                   '> (the lengths are ' + str(len(secnames))
                     + ' and ' + str(count) + ' respectively).'
                  )
            gen.WriteError(2685, err, log)
            gen.ErrorOnTwoLines(line_number, line_text,
                                line2_number, line2_text,
                                log, False)

            return(None)
        # Check that all the secnames in the list exist.  We could let
        # this be caught in ProcessTunnel, but it makes sense to write
        # a more informative error message here.
        duds = set([name for name in secnames if name not in sectypes_dict])
        if len(duds) == 1:
            err_text = "one\n> invalid sectype."
            name = list(duds)[0]
            err = ('> In the file named "' + file_name + '",\n'
                   '> a "sectypes" line of entry in a set of cloned\n'
                   '> tunnels contained one invalid sectype name,\n'
                   '> "' + name + '".  Please edit the list so that all\n'
                   "> the entries in it are the names of valid\n"
                   "> sectypes.  For what it's worth, here are the\n"
                   "> names of the sectype(s) you've set:\n"
                  )
            err = err + gen.FormatOnLines(sectypes_dict.keys())
            gen.WriteError(2686, err, log)
            gen.ErrorOnLine(line_number, line_text, log, False)
            return(None)
        elif len(duds) > 1:
            err_text = "some\n> invalid sectypes."
            err = ('> In the file named "' + file_name + '",\n'
                   '> a "sectypes" line of entry in a set of cloned\n'
                   '> tunnels contained ' + str(len(duds))
                     +' invalid sectype names:\n'
                    + gen.FormatOnLines(list(duds)) + '\n'
                   '> Please edit the list so that all the entries\n'
                   "> in the list are the names of valid sectypes.\n"
                   "> For what it's worth, here are the names of\n"
                   "> the sectype(s) you've set:\n"
                  )
            err = err + gen.FormatOnLines(sectypes_dict.keys())
            gen.WriteError(2687, err, log)
            gen.ErrorOnLine(line_number, line_text, log, False)
            return(None)

    # Now check for any fans or dampers in the clone tunnels.  Fans are
    # pretty unlikely, but dampers are not (we'll use them for dampers
    # in trackside adits and cross-passages).  Fans and dampers both
    # have names, which we look for a "*" in and replace it with our
    # list of numbers.
    ent_keys = []
    for key in clone_dict:
        if key[:6] == "damper" or key[:3] == "fan":
            ent_keys.append(key)

    # Make a list of the line indices so that we know which lines to
    # amend later.
    ent_indices = []
    # Make a list of the base names we need to change.
    ent_basenames = []
    # Make a list of lists of the names we need to replace the base
    # names with.
    ent_names = []
    # Store the lines these entities are on and store a list of lists
    # of their new names.
    for key in ent_keys:
        dict_entry = clone_dict[key]
        # Store the index of the line that this came from.
        ent_indices.append(dict_entry[-1])
        base_name = dict_entry[1]

        # Check for an asterisk in the name of the entity.
        if key in ("fan1", "fan2"):
            err_text = "name of a fan"
        elif key in ("damper1", "damper2"):
            err_text = "name of a damper"
        else:
            # We can only get here if the 'if' statement that fills
            # the list of entities is changed (see above).
            message = ("Oops, need to add code to catch a dangling else in "
                       "PROC ProcessClones.")
            gen.WriteOut(message, log)
            print(message)
            gen.OopsIDidItAgain(log)

        # Look for asterisks and replace them with the relevant
        # numbers.
        result = CheckAsterisks(base_name, err_text,
                                file_name, back_number, back_text, log)
        if result is None:
            return(None)
        else:
            (ent_name, format_string) = result
        names = BuildNameList(t_nums, ent_name, format_string)
        # Store the base name and the list of names to use.
        ent_basenames.append(base_name)
        ent_names.append(names)

    # We now have enough information to create a tunnel by calling
    # ProcessTunnel multiple times.  First we make a copy of the
    # lines in the file and turn it into a list, so that we can
    # change it.
    clonedtunnels_dict = {}
    mod_triples = line_triples.copy()

    # Figure out where the tunnelclones block ends and change it
    # from "end tunnelclones" to "end tunnel" in the copy.
    for (added, candidate) in enumerate(line_triples[tr_index:]):
        parts = candidate[1].lower().split()
        if parts == ["end", "tunnelclones"]:
            # This is the line to change.
            replacement = (candidate[0], "end tunnel", "end tunnel")
            mod_triples[tr_index + added] = replacement
            break

    # Create a list that contains all the commands use to create the
    # cloned tunnels, except the "sectypes" and "numbering" keywords.
    # We return this at the end and the calling routine can print it
    # out in the log file.
    tunnel_lines = []

    for (index, tun_name) in enumerate(t_names):
        # Get a copy of line_triples.  We replace the lines defining
        # the back end and forward end with our modified ones, then
        # we call ProcessTunnel.  By this means, any error messages
        # still point to the correct source line in line_triples.

        # Change the data in the line defining the start of the block.
        # We already have its index from the arguments sent to this
        # routine.
        tun_line = list(line_triples[tr_index])
        tun_line[1] = "begin tunnel " + tun_name
        mod_triples[tr_index] = tun_line

        # Change the line defining the back end and turn it into
        # a list too.
        back = clone_dict["back"]
        back_line = list(line_triples[back_index])
        # Build the entries we want on the line defining the back end.
        back_list = ["back", str(back[0]), str(back[1])]
        if b_names == "#notnode":
            # Add the value associated with whatever is at the portal.
            back_list.append(str(back[2]))
        else:
            # Add the modified name of the node.
            back_list.append(b_names[index])
        # Add the name of the sectype and optional entries.
        back_list.extend([secnames[index], back[4]])
        back_list.extend(RebuildOptionals(back))
        back_data = " ".join([str(entry) for entry in back_list])
        # Change the data on the line in the the triple defining
        # the back end.
        back_line[1] = back_data
        mod_triples[back_index] = back_line

        # Change the line defining the forward end and turn it into
        # a list too.
        fwd = clone_dict["fwd"]
        fwd_line = list(line_triples[fwd_index])
        # Build the entries we want on the line defining the forward end.
        fwd_list = ["fwd", str(fwd[0]), str(fwd[1])]
        if f_names == "#notnode":
            # Add the value associated with whatever is at the portal.
            fwd_list.append(str(fwd[2]))
        else:
            # Add the modified name of the node.
            fwd_list.append(f_names[index])
        # Add the optional entries.
        fwd_list.extend(RebuildOptionals(fwd))
        fwd_data = " ".join([str(entry) for entry in fwd_list])
        # Change the data on the line in the the triple defining
        # the forward end
        fwd_line[1] = fwd_data
        mod_triples[fwd_index] = fwd_line

        # Now change all the entities (fans or dampers) in the clone tunnel.
        for ent_index, oldname, newnames in zip(ent_indices,
                                                ent_basenames,
                                                ent_names):
            # print(ent_index, oldname, newnames, len(t_names), len(newnames))
            line_num, line_data, line_text = line_triples[ent_index]
            # Get the new name to use in this particular clone.
            newname = newnames[index]
            # Replace the first instance of the generic name with
            # the clone's name.
            mod_data = line_data.lower().replace(oldname, newname, 1)
            mod_triples[ent_index] = (line_num, mod_data, line_text)

        # Write the lines for the new tunnels to the list of tunnel
        # creation lines.  Don't include the lines specific to the
        # tunnelclones block.
        for index2 in range(tr_index, tr_index + added + 1):
            orig_line = line_triples[index2][2]
            mod_line = mod_triples[index2][1]
            # Figure out the indentation on the line, so we can use
            # the same indentation in the lines of input written to
            # the log file.
            first_word = orig_line.split()[0]
            whitespace = orig_line.split(sep = first_word)[0]
            if first_word.lower() not in ("numbering", "sectypes"):
                # print(whitespace + mod_line)
                tunnel_lines.append(whitespace + mod_line)
        # Include a blank line between the lines of tunnel definition.
        tunnel_lines.append("")

        # Now call ProcessTunnel to create a new tunnel.
        result = ProcessTunnel(mod_triples, tr_index, settings_dict,
                               sectypes_dict, log)
        if result is None:
            return(None)
        else:
            (tunnel_name, new_tun_dict) = result
            clonedtunnels_dict.__setitem__(tunnel_name, new_tun_dict)
    return(clonedtunnels_dict, tunnel_lines)


def GetJFBlocks(routes_dict, tunnels_dict, settings_dict,
                entities_dict, line_triples, log):
    '''Take the tunnel, route and jet fan type definitions.  Check
    the following:
     * all the jet fan types used in tunnels or routes have been
       defined in the "jetfantypes" block.
     * none of the definitions try to run a unidirectional jet fan
       in reverse
     * where jet fans are placed in tunnels, the jet plumes don't
       extend beyond the ends of the tunnels
     * jet fans in routes and jet fans in tunnels don't try to put
       jet fans in the same tunnel.
    Build dictionaries of blocks of jet fan thrust for calculating
    and for plotting (all the banks of jet fans can be plotted at,
    but some banks don't need to be in the calculation arrays because
    the fans are off.
    '''
    debug1 = settings_dict["debug1"]
    file_name = settings_dict["file_name"]
    units = settings_dict["units"]
    JFcalc_dict = settings_dict["JFcalc_dict"]
    # Make a dictionary to hold blocks of jet fan thrust that appear in
    # individual tunnels.  The key is the tunnel name, the result is a
    # list of lists of numbers that define each block of jet fan thrust
    # and its behaviours.
    t_JF_dict = {}
    JF_plot_dict = {}
    for tun_name, tun_defn in tunnels_dict.items():
        # Set an empty list for the blocks of jet fan thrust in this
        # tunnel.
        t_JF_dict.__setitem__(tun_name, [])

    # First assess the jet fan definitions in each tunnel.
    for tun_name, tun_defn in tunnels_dict.items():
        firstbank = True
        for key, settings in tun_defn.items():
            # Check for jet fans and get their definitions in a form that
            # we can turn into segment-based body forces.  We pass a
            # False value because the jet fans are in tunnels, not in
            # routes.  The False value means that the routine faults if
            # the plume from the bank of fans extends outside the tunnel.
            if key[:8] == "jetfans1":
                if firstbank:
                    gen.WriteOut('Tunnel "' + tun_name +'" has one or'
                                 ' more banks of jet fans, as follows:', log)
                    firstbank = False
                result = CheckJFBanks(settings, tun_name, tun_defn, False,
                                      settings_dict, line_triples, log)
                if result is None:
                    return(None)
                else:
                    JF_name, JFcalcdata, JFplotdata = result
                    # A few quick cribs.
                    #   If jet fans in the bank are active:
                    #     JFcalcdata = [JF_name, jet_back, jet_fwd, E_bank,
                    #                   jetspeed, [times], [speeds], tr_index]
                    #   If the jet fan speed or count of jet fans is zero:
                    #     JFcalcdata = []
                    #   In all cases:
                    #     JFplotdata = [JFtype, tun_name, dist, count,
                    #                   thrust, effic, jetspeed, base speed]
                if JFcalcdata!= []:
                    # This bank of fans is active.  Add the block of
                    # jet fan thrust to the list of blocks in this tunnel.
                    t_JF_dict[tun_name].append(JFcalcdata)
                # We always add the jet fan data to the plotting dictionary.
                JF_plot_dict.__setitem__(JF_name, JFplotdata)

                # Check for name clashes.  Can't have two banks of jet fans
                # with the same name!
                line_number, discard, line_text = line_triples[settings[-1]]
                result = CheckEntityName(JF_name, "a bank of fans",
                                         tun_name, line_number, line_text,
                                         entities_dict, file_name, log)
                if result is None:
                    return(None)
                else:
                    entities_dict = result

    # Figure out which tunnels don't have any plumes from jet fans
    # in them and remove their empty lists from the dictionary.
    no_JF = []
    for tun_name, dict in t_JF_dict.items():
        if dict == []:
            no_JF.append(tun_name)
    for tun_name in no_JF:
            t_JF_dict.pop(tun_name)

    # for tun_name, banks_on in t_JF_dict.items():
    #     print("t_JF_dict", tun_name)
    #     for entry in banks_on:
    #         print("  ", entry)
    # for bank_name, plotdata in JF_plot_dict.items():
    #     print("  ", bank_name, plotdata)
    # At this point we assess the blocks of jet fans set in routes.
    # These are not implemented yet.
    pass
    return(t_JF_dict, JF_plot_dict, entities_dict)


def CheckJFBanks(settings, tun_name, tun_defn, in_route,
                 settings_dict, line_triples, log):
    debug1 = settings_dict["debug1"]
    file_name = settings_dict["file_name"]
    units = settings_dict["units"]
    JFcalc_dict = settings_dict["JFcalc_dict"]
    # Check if there are any jet fan types defined and complain
    # if there are none.
    tr_index = settings[-1]
    dist, JF_name, count, JFtype, speed = settings[:5]
    optionals_dict = settings[-2]

    text1 = SIorUSvalue("dist1", dist, 2, units, debug1, log)

    if len(JFcalc_dict) == 0:
        if settings_dict["JFblock_dict"] == {}:
            # There is no "jetfantypes" block.
            text2 = 'a "jetfantypes" block and\n' \
                    '> define'
        else:
            # There is a "jetfantypes" block but it is empty.
            text2 = 'an entry in the "jetfantypes"\n'  \
                    '> block for'
        err = ('> In the file named "' + file_name + '"\n'
               '> tunnel "' + tun_name + '" had a bank of jet fans of type\n'
               '> "' + JFtype + '" at ' + text1 + '.\n'
               '> There are no definitions of jet fan types in\n'
               '> this file.  Please either remove the bank of\n'
               '> jet fans or add ' + text2 + ' the "'
                 + JFtype + '" type.'
              )
        gen.WriteError(2941, err, log)
        gen.ErrorOnLine2(tr_index, line_triples, log)
        return(None)

    # Check if this jet fan type exists and complain if it doesn't.
    if JFtype not in JFcalc_dict:
        err = ('> In the file named "' + file_name + '"\n'
               '> tunnel "' + tun_name + '" had a bank of jet fans of type\n'
               '> "' + JFtype + '" at ' + text1 + '.\n'
               '> Alas, there is no jet fan type with this name.\n'
               '> Please edit the file to remove the bank of jet\n'
               '> fans, change the name of the jet fan type or add\n'
               '> the jet fan type to the "jetfantypes" block.\n'
               '> For what it is worth, the following jet fan type(s)\n'
               '> exist:\n'
                + gen.FormatOnLines(tuple(JFcalc_dict.keys()))
              )
        gen.WriteError(2942, err, log)
        gen.ErrorOnLine2(tr_index, line_triples, log)
        return(None)
    else:
        # Get the properties so we can check if the jet fan definition
        # is suitable for the operation of this bank of jet fans.
        [JF_index, T_f, effic_f, U_f,
                   T_r, effic_r, U_r,
         throw, length, runup, rundown] = JFcalc_dict[JFtype]

    # Check if this bank of jet fans is trying to run a unidirectional
    # jet fan type in reverse.
    if speed < 0.0 and effic_r == -1.:
        # Having 'effic_r' be -1 means this jet fan type is a
        # unidirectional fan, not a reversible one (the user cannot
        # set negative values of installation efficiency).  The
        # jet fan type is not compatible with the way this bank of
        # fans is to be operated.  Complain.
        # Get the line defining the jet fan type and line defining
        # the bank of fans.
        line1_num, discard, line1_text = line_triples[JF_index]
        line2_num, discard, line2_text = line_triples[tr_index]
        err = ('> In the file named "' + file_name + '"\n'
               '> tunnel "' + tun_name + '" had a bank of jet fans of type\n'
               '> "' + JFtype + '" at ' + text1 + '.\n'
               '> Jet fans of type "' + JFtype + '" are unidirectional and\n'
               '> cannot run in reverse, but the bank of fans tries\n'
               '> to run them in reverse (the speed fraction in the\n'
               '> "jetfans1" line is ' + str(speed) + ').\n'
               '> Please edit the file to change the type of jet\n'
               '> fan in the bank or turn the fans off.\n'
               '> Note that if you genuinely want unidirectional\n'
               '> jet fans blow towards the back end of a tunnel,\n'
               '> define a reversible jet fan and put in zero for\n'
               '> the installation efficiency in forwards mode.'
              )
        gen.WriteError(2943, err, log)
        gen.ErrorOnTwoLines(line1_num, line1_text,
                            line2_num, line2_text, log,
                            False, False, "Clashing")
        return(None)

    # Check the count of jet fans.  When projects are in an advanced
    # stage and the jet fans have been selected, we want the counts of
    # jet fans to be integers.  At the early stages of projects
    # (when we are figuring out how much thrust we need) it is more
    # useful to be able to allow fractional numbers of jet fans.
    # This controlled by an entry in the "settings" block, which
    # defaults to requiring integer values of jet fan counts.
    # If you want to use fractional values, include the entry
    # "jetfancounts nonintegers" in the "settings" block (without
    # double quotes).  The default is "jetfancounts integers".
    if (not math.isclose(count, int(count))) and     \
        settings_dict["jetfancounts"] == "integers":
        # The user has tried to use a non-integer count of jet fans
        # in a file that only integer counts are allowed in.
        err = ('> In the file named "' + file_name + '"\n'
               '> tunnel "' + tun_name + '" had a bank of jet fans that\n'
               '> had a fractional number of jet fans operating\n'
               '> (' + str(count) + ').\n'
               '> This is only allowed if you have the line\n'
               '>\n'
               '>   jetfancounts nonintegers\n'
               '>\n'
               '> in the "settings" block.  Please either only use\n'
               '> integer counts of jet fans in this file or add\n'
               '> that line to the "settings" block.'
              )
        gen.WriteError(2944, err, log)
        gen.ErrorOnLine2(tr_index, line_triples, log)
        return(None)

    # Get the extents of the tunnel.
    tun_back = tun_defn["back"][0]
    tun_fwd = tun_defn["fwd"][0]

    if math.isclose(count * speed, 0.0):
        # The input data means that the useful fan thrust is zero.
        # Record that we don't need to process the thrust from
        # this bank of fans, but if someone plots at it we will
        # plot something appropriate to a bank of stopped fans.
        # Note that we set the fan fractional rotational speed
        # to zero just in case the user set zero count instead of
        # zero speed.
        jetspeed = U_r
        thrust = T_r
        effic = effic_r
        speed = 0.0

        # Set an empty list for calculating.  The calling routine
        # will discard it and not include it in the calculation.
        JFcalcdata = []
        # Generate details of this bank of jet fans.  We will write
        # them to the log file later.
        lines = ['  Bank of jet fans "' + JF_name + '" at '
                 + gen.RoundText(dist, 2) + ' m is turned off.']
    else:
        # Figure out the regions over which the fans will apply thrust
        # when running in forwards mode and reverse mode.  We do both
        # because we will be implementing jet fans that can reverse
        # during the run, we might as well cater for them now.
        # fwd_dist1 and fwd_dist2 are the distances along the segment
        # that apply when the jet fan is running in reverse mode.
        # rev_dist1 and rev_dist2 are the distances along the segment
        # that apply when the jet fan is running in forwards mode.
        halflength = length / 2
        fwd_dist1 = dist + halflength
        fwd_dist2 = fwd_dist1 + throw
        rev_dist2 = dist - halflength
        rev_dist1 = rev_dist2 - throw

        # We create body force terms for the calculation by lumping
        # the static thrust, count, installation efficiency, the
        # air density the static thrust is at (1.2 kg/m^3) and the
        # distance over which the jet transfers its thrust.
        # The values have units of m^4/s^2 per metre of tunnel
        # length; they will be divided by the segment area at the
        # jet fan intake later, after we figure out the properties
        # of segments.  We adjust for the background air velocity
        # then add it as a body force in the E term of the
        # characteristic equations.
        # The thrust will be adjusted by multiplying by the square
        # of fractional rotational speed at each timestep.
        E_fwd = -count * T_f * effic_f / ( 1.2 * throw)
        E_rev =  count * T_r * effic_r / ( 1.2 * throw)
        if speed < 0.0:
            # Make some text for the error message we issue if the
            # plume of air coming out of the jet fan is partly outside
            # the tunnel and for the log file.
            text2 = "reverse mode"
            ASCII_art = ('>            back end              fwd end\n'
                         '>            <------extent of tunnel----->\n'
                         '>    <<<<<<<<<<<<<<<<<<<<<<<<<[===]\n'
                         '>       plume from jet fans\n')
            missing = gen.RoundText(tun_back - rev_dist1, 2)
            jet_back = rev_dist1
            jet_fwd = rev_dist2
            # Build some lines to write to the log file.
            perf = BuildJFStats(T_r, effic_r, U_r, speed)
        elif speed > 0.0:
            # Create the error message texts for forwards mode.
            text2 = "forwards mode"
            ASCII_art = ('>      back end              fwd end\n'
                         '>      <------extent of tunnel----->\n'
                         '>              [===]>>>>>>>>>>>>>>>>>>>>>>>>>\n'
                         '>                      plume from jet fans\n')
            missing = gen.RoundText(fwd_dist2 - tun_fwd, 2)
            jet_back = fwd_dist1
            jet_fwd = fwd_dist2
            perf = BuildJFStats(T_f, effic_f, U_f, speed)
        # Check the extents of the jet from the fan.  If this is a
        # jet fan placed in a tunnel, we want the jet to start and
        # end inside the tunnel and have its inlet in the tunnel.
        # If it is a jet fan in a route, we allow the plume to extend
        # into other tunnels in the route and the intake to be in
        # another tunnel.
        if in_route:
            # At this point we figure out which tunnel the jet fan
            # intake is in and the distance in that tunnel it lies at.
            # We figure out if the plume from the bank of fans enters
            # one or more other tunnels and the distances in those
            # tunnels.  But as we haven't implemented jet fans in
            # routes yet, we just pass.
            pass
        elif jet_back < tun_back or jet_fwd > tun_fwd:
                # The plume from the jet fan extends outside the tunnel.
                err = ('> In the file named "' + file_name + '",\n'
                       '> tunnel "' + tun_name + '" had a bank of jet fans\n'
                       '> (called ' + JF_name + ') running in ' + text2 + '\n'
                       '> in which the jet plume extended outside the\n'
                       '> tunnel.  The tunnel extends from\n'
                       '> ' + gen.RoundText(tun_back, 2) + ' to '
                            + gen.RoundText(tun_fwd, 2) + '.\n'
                       '> The plume from the jet fans extends from\n'
                       '> ' + gen.RoundText(jet_back, 2) + ' to '
                            + gen.RoundText(jet_fwd, 2) + '.\n'
                       '> ' + missing
                         + ' m of the jet fan plume is outside the\n'
                       '> tunnel, as depicted in this ASCII art:\n>\n'
                         + ASCII_art + '>'
                      )
                gen.WriteError(2945, err, log)
                gen.ErrorOnLine2(tr_index, line_triples, log)
                return(None)

        # Build the timing of the jet fan operation.  This is similar to
        # how axial fans in Hobyah work, but we pass the runup and rundown
        # times that were set in the jet fan type (either the ones in
        # the optional argument or the default values in ProcessJFTypes).
        # The default is for the jet fans to run up at the start of the
        # run (taking 5 seconds to get to full speed) then continue
        # until the end of the run.  Default rundown time is 10 seconds.
        (times, speeds) = JetFan1Timing(speed, optionals_dict,
                                        runup, rundown, settings_dict)

        # Generate details of this bank of jet fans.  We will write
        # them to the log file after this block ends.
        text3, text4 = gen.AlignListPrint((times, speeds))

        lines = ['  Bank of jet fans "' + JF_name
                   + '" at ' + gen.RoundText(dist, 2)
                   + ' m is running in ' + text2
                   + ' at ',
                 '  ' + gen.RoundText(abs(speed)*100, 1) +
                   '% speed.  It consists of ' + str(count) +
                   ' jet fans of type "' + JFtype + '":'
                ] + perf + \
                ["    Operating times (s): " + text3,
                 "    Fractional speeds:   " + text4,
                 "    When operating in forwards mode, the jet of air"
                   " starts at " + gen.RoundText(fwd_dist1, 3) + " m and"
                   " ends at " + gen.RoundText(fwd_dist2, 3) + " m."]
        if effic_r != -1.:
            # This fan can reverse.  Add an entry for it.
            lines.append("    When operating in reverse mode, the jet of air"
                         " starts at " + gen.RoundText(rev_dist2, 3) + " m and"
                         " ends at " + gen.RoundText(rev_dist1, 3) + " m.")


        # Make a list of properties to calculate with.  We include
        # the jet fan name so we can pull out performance data at
        # the print timesteps while we calculate it and assign it
        # to the name of the jet fan bank.

        JFcalcdata = (JF_name, fwd_dist1, fwd_dist2, E_fwd, U_f,
                               rev_dist1, rev_dist2, E_rev, U_r,
                               times, speeds, tr_index)
    # Make a dictionary entry for plotting.  It contains:
    #  * Name of the jetfantype
    #  * Name of the tunnel the bank of fans is in
    #  * Distance in the tunnel the bank of fans is at
    #  * Count of the jet fans operating
    # All the other data can be taken from the jetfantype or the
    # 'JFcalcdata' array.
    JFplotdata = (JFtype, tun_name, dist, count)
    for line in lines:
        gen.WriteMessage3(line, debug1, log)
    return(JF_name, JFcalcdata, JFplotdata)


def SIorUSvalue(key, value, decpl, units, debug1, log):
    '''Take an SI value, the key to convert it to US units, the
    decimal places to round it to and a string giving the units
    of this file ('si' or 'us').

    Return a string with the relevant number and the relevant units.
    '''
    USval, (SI, US) = USc.ConvertToUS(key, value, debug1, log)
    if units == "si":
        text1 = gen.RoundText(value, 2) + ' ' + SI
    else:
        USval, (SI, x) = USc.ConvertToUS(key, value, debug1, log)
        text1 = gen.RoundText(USval, 2) + ' ' + US
    return(text1)


def BuildJFStats(thrust, effic, jetvel, speed):
    '''Create a list of lines to be written to the log file giving
    details of the performance of each jet fan in a bank of fans.

        Parameters:
            thrust          float           Static thrust of one jet
                                            fan, in Newtons
            effic           float           Installation efficiency of
                                            jet fans, fractional (0-1)
            jetspeed        float           Discharge velocity of a jet
                                            fan, m/s.  May be positive
            speed           float           Speed of operation of a
                                            jet fan.  Fractional value
                                            (usually -1 to +1, but the
                                            absolute value could be
                                            above 1 if the jet fans are
                                            being overclocked for some
                                            reason).
        Returns:
            perf            [str]           A list of lines describing
                                            the operation of the jet
                                            fan.  If the fan is not
                                            running at 100% speed the
                                            thrust and velocity at the
                                            operational speed are included.

    '''
    percspeed = gen.RoundText(abs(100*speed),1)
    perf = ['    Static thrust (at 100% speed & 1.2 kg/m^3): '
              + gen.RoundText(thrust, 1) + ' N per jet fan',
            '    Jet velocity at 100% speed: '
              + gen.RoundText(math.copysign(jetvel, speed), 2) + ' m/s',]
    # If the jet fan is not running at 100% speed, add the
    # thrust and jet velocity at the operational speed.
    if percspeed != "100":
        perf.extend(['    Static thrust at ' + percspeed + '% speed: '
                        + gen.RoundText(thrust * speed**2, 1) + ' N',
                     '    Jet velocity at ' + percspeed + '% speed: '
                        + gen.RoundText(jetvel * speed, 2) + ' m/s',])
    perf.append('    Installation efficiency: ' + gen.RoundText(effic, 3))
    return(perf)


def RebuildOptionals(entry):
    '''Take the entries on a line and pull out the optional arguments.
    Reconstruct the text of the optional arguments.  This is used
    when we are building clones of tunnels and need to put the optional
    arguments into a line of text that we can send to ProcessTunnel.

        Parameters:
            entry           (int, str, str) One line definition returned
                                            by ProcessBlock.  The 2nd-
                                            last entry is a dictionary
                                            of the optional arguments
                                            on the line.
        Returns:
            opt_list        [str]           A list of the optional
                                            entries in a form that can
                                            be processed a second time.
    '''
    opt_args = entry[-2]
    opt_list = []
    # Get the arguments in the optionals dictionary.
    for arg in opt_args:
        opt_list.extend([arg, ":=", opt_args[arg]])
    return(opt_list)


def BuildNameList(numbers, tname, istring):
    '''Take a list of numbers, a base name that contains one asterisk,
    and an integer format string.  Return a list of names with the
    numbers replacing the asterisk.

        Parameters:
            numbers         (float)         A list of numbers to be
                                            used in the names of
                                            procedurally-generated
                                            tunnels or joins.
            tname           str             A one-word name containing
                                            an asterisk.
            istring         str             A number format specifier
                                            that forces leading zeros
                                            be used, like "{:04d}".

        Returns:
            names           [str]           A list of names (all 'tname'
                                            with the asterisk replaced
                                            by one of the entry in
                                            'numbers'.  If (as is
                                            recommended) the numbers
                                            are all integers, they may
                                            have leading zeros.
    '''
    # Check if every entry maps to an integer.  If they all do, get
    # integers into the names.  If some don't, put floats in for all.
    # Note that we don't use is_integer because these may be a mix
    # of base numbers and numpy numbers.
    integers = True
    for number in numbers:
        if not math.isclose(number, int(number)):
            integers = False
    if integers:
        for number in numbers:
            names = [tname.replace("*", istring.format(int(number))) for
                      number in numbers]
    else:
        for number in numbers:
            names = [tname.replace("*", gen.FloatText(number,8)) for
                        number in numbers]
    return(names)


def CheckAsterisks(base, descrip, file_name, line_number, line_text, log):
    '''Take a string that ought to be the base name of something in a set
    of clones (tunnel base name or node base name), like "XP***".
    Check if it has more than one group of "*" characters, like "XP*8*"
    Return a string that contains the text before the first group of
    "*" characters, the correct count of "*" characters in the first
    group and the text before the second group of "*" characters.
    This is used in error messages where we want to give an example
    of a good way to set the names in clone blocks.

        Parameters:
            base            str             A one-word name containing
                                            one or more asterisks.
            descrip         str             A descriptor like "name of
                                            a damper", for errors.
            file_name       str             The file name, for error messages.
            line_number     int             The line number, for error messages.
            line_text       str             The entire line, including comments.
            log             handle          The handle of the logfile.

        Returns:
            touse           str             The text in 'base' with all
                                            the asterisks reduced to
                                            one asterisk.
            istring         str             A number format specifier
                                            that forces leading zeros
                                            be used, like "{:04d}".

        Errors:
            Abort with 2701 if there was more than one group of
            asterisks in the base name.
    '''
    # Simplest way to count these is to use regular expressions.
    asterisk_groups = re.findall("\*+", base)
    if len(asterisk_groups) > 1:
        # Get the text on either side of the first group of asterisks.
        parts = base.split(sep = asterisk_groups[0], maxsplit = 1)
        toshow = parts[0] + asterisk_groups[0] + parts[1].split(sep = "*")[0]
        err = ('> In the file named "' + file_name + '"\n'
               '> the ' + descrip + ' had too\n'
               '> many groups of "*" characters in it.  Please\n'
               '> reduce it to one group, e.g. "' + toshow + '".'
              )
        gen.WriteError(2701, err, log)
        gen.ErrorOnLine(line_number, line_text, log, False)
        return(None)
    else:
        # Turn the asterisks into one asterisk.
        touse = base.replace(asterisk_groups[0], "*")
        # The number of "*" characters in the name tells us whether to
        # use leading zeros and if so, how many.  Something like "CP**"
        # means "use CP01, CP02, ... CP10, CP11 etc."  If the count
        # goes beyond 99 it will give "CP100" without raising an error.
        # We generate a format string to use with the format command,
        # This one is for integer values.
        tstring = "{:0" + str(len(asterisk_groups[0])) + "d}"
    return(touse, tstring)


def ProcessJFTypes(line_triples, tr_index, settings_dict, log):
    '''Read all the data defining jet fan types (static thrusts at air
    density 1.2 kg/m^3, jet speeds, installation efficiencies).

        Parameters:
            line_triples [(int, str, str)]  List of lines in the file.
            tr_index        int             Pointer to where the block starts
                                            in line_triples.
            settings_dict   {}              Dictionary of the run settings.
            log             handle          The handle of the logfile.

        Returns:
            JFblock_dict    {}              Dictionary defining the lines
                                            in the jetfantypes block.
            JFcalc_dict     {}              Dictionary defining the jet
                                            fan types for the calculation.
        Errors:
            Aborts with 2761 if the name of a jet fan type appeared twice
            in the list.
    '''
    debug1 = settings_dict["debug1"]
    file_name = settings_dict["file_name"]
    units = settings_dict["units"]

    valid_settings = {"unidirectional":
                            ("#name",
                             "float  +  Force2  a static thrust",
                             "float  +  null    an installation efficiency",
                             "float  +  speed1  a jet speed",
                             "QAstr"),
                      "reversible":
                            ("#name",
                             "float  +  Force2  a static thrust",
                             "float  +  null    an installation efficiency",
                             "float  +  speed1  a jet speed",
                             "float  +  Force2  a static thrust",
                             "float  +  null    an installation efficiency",
                             "float  -  speed1  a reverse jet speed",
                             "QAstr"),
                     }
    #
    # Define the optional entries allowed in each keyword.
    #
    # The optional entry "jetlength" sets the distance over which the
    # jet decays and transfers its thrust to the air in the tunnel.
    # The jet fan pressure rise is not a jump in pressure at a point,
    # it takes place over a distance.  The default is 80 metres.
    #
    # The optional entry "fanlength" sets the length of the fan carcass
    # and is used to set where the jet of air from the fan starts
    # compared to where the fan is positioned.
    # It is not intended for ordinary jet fans, which are usually so
    # short that the length of the fan carcass might as well be zero.
    # Even the longest jet fans I know about (Lane Cove tunnel, 1.8 m
    # internal diameter with 3D silencers on each end) are only about
    # 12 m long, so setting 12 m for the fan length only moves the point
    # where the jet of air starts by 6 m - not really worth doing.
    # The "fanlength" keyword is intended for weird cases like Epping
    # Services Facility vent on Sydney North West Rail Link (NWRL),
    # which behaves like a jet fan even though it isn't one.
    # Epping SF has a set of axial fans above the tunnel roof that
    # draw air out of the tunnel, then throw it back into the tunnel
    # at high speed.  This vent system behaves like a jet fan, but
    # the intake and discharge are (if I recall correctly) about 35 m
    # apart.  This comes from the reference design of the NWRL (2013);
    # the successful tenderer may have changed it.
    #
    # The "runup" and "rundown" entries are the durations over which the
    # fans go from stationary to the intended speed and vise-versa.
    # "Runup" and "rundown" have defaults (5 seconds and 10 seconds
    # respectively).
    # "Runup" and "rundown" values can be set for individual banks of
    # jet fans in the "jetfans1" keyword.  Those durations will override
    # the ones set here in the jet fan type definition.
    optionals = {"unidirectional":
                        {"jetlength": "float  0+  dist1  a jetfan plume length",
                         "fanlength": "float  0+  dist1  a jet fan length",
                         "runup":   "float  0+  null  a jet fan runup time",
                         "rundown": "float  0+  null  a jet fan rundown time",
                        },
                }
    # Copy these option for unidirectional fans into the settings
    # for reversible fans.
    optionals.__setitem__("reversible", optionals["unidirectional"])
    #
    # Make a list of what entries we must have (none, we allow an
    # empty block even if it's a weird thing for a user to do).
    requireds = ()
    #
    # Make a list of what entries can be duplicated.
    duplicates = ("unidirectional", "reversible")

    settings = (valid_settings, requireds, optionals, duplicates)
    block_name = "jetfantypes"

    if debug1:
        print("Processing jet fan types")

    result = ProcessBlock(line_triples, tr_index, settings_dict,
                          block_name, {}, settings, log)
    if result is None:
        return(None)
    else:
        (discard, JFblock_dict) = result

    # Now go through the contents of the block looking for jet fan
    # types with the same name (the #name entry on each line of)
    # definitions.  We also build the first part of the calculation
    # dictionary.
    maxlen = 0
    JFcalc_dict = {}
    for key, JFdef in JFblock_dict.items():
        line_num, discard, line_text = line_triples[tr_index]
        if key[:14] == "unidirectional" or key[:10] == "reversible":
            JFtype_name = JFdef[0]
            tr_index = JFdef[-1]
            # Check if the width of the name of this jet fan type is the
            # widest.  We need this to be able to format a table of
            # jet fan properties properly.
            new_len = len(JFtype_name)
            if new_len > maxlen:
                maxlen = new_len

            if JFtype_name in JFcalc_dict:
                # We have a duplicate name for the jet fan type.
                # Complain.
                old_entry = JFcalc_dict[JFtype_name]
                line1_num, discard, line1_text = line_triples[old_entry[0]]
                err = ('> In the file named "' + file_name + '"\n'
                       '> the "jetfantypes" block had two jet fan types\n'
                       '> with the same name, ("'
                         + JFtype_name + '").\n'
                       '> Please edit the file to remove one or change\n'
                       '> its name.'
                      )
                gen.WriteError(2761, err, log)
                gen.ErrorOnTwoLines(line1_num, line1_text,
                                    line_num, line_text, log, False)
                return(None)
            else:
                # Get the line number and line text for the limits
                # check.
                tr_index = JFdef[-1]
            # Check if there was an optional argument to set the
            # throw distance of the jet when at full speed.  This is
            # the length over which the jet fan pressure rise is
            # spread (it is not treated as a jump in pressure at the
            # location of the jet fan like in other programs).
            if "jetlength" in JFdef[-2]:
                value = JFdef[-2]["jetlength"]
                throw = CheckForConstant2(str(value), "spoof_it",
                                          settings_dict, line_triples,
                                          line_num, line_text, False, log)
                if throw is None:
                    return(None)
                elif units == "us":
                    (throw, discard) = USc.ConvertToUS("dist1", throw,
                                                       debug1, log)
            else:
                # Use the default jet length of 80 m.
                throw = 80.0

            # Check if there was an optional argument to set the
            # length of the fan carcass.
            if "fanlength" in JFdef[-2]:
                value = JFdef[-2]["fanlength"]
                length = CheckForConstant2(str(value), "spoof_it",
                                          settings_dict, line_triples,
                                          line_num, line_text, False, log)
                if length is None:
                    return(None)
                if units == "us":
                    (length, discard) = USc.ConvertToUS("dist1", length,
                                                        debug1, log)
            else:
                # Use a default jet fan carcass length of 0 m.  In
                # reality, jet fans may be up to 11 m long (a 1.6 m
                # diameter jet fan with 3D long silencers on each
                # end is about 10.8 m).
                length = 0.0

            if "runup" in JFdef[-2]:
                runup = JFdef[-2]["runup"]
            else:
                # Use the default runup time of 5 seconds (DOL).
                runup = 5.0
            if "rundown" in JFdef[-2]:
                rundown = JFdef[-2]["rundown"]
            else:
                # Use the default rundown time of 10 seconds.  This is
                # based on watching jet fans of size 630 mm to 1120 mm
                # being switched off.
                rundown = 10.0


            # The values all check out, let's put them into the calculation
            # arrays.
            if key[:14] == "unidirectional":
                # It's a unidirectional jet fan, we put in three zero
                # values for the properties in the reverse direction.
                T_f, effic_f, U_f = JFdef[1:4]
                result = CheckJFThrust(JFtype_name, T_f, effic_f, U_f,
                                       "forwards", file_name, tr_index,
                                       line_triples, settings_dict, log)
                if result is None:
                    return(None)
                # Make a list of the properties of the jet fan.  We
                # include tr_index so we can use it in error 2761
                # above and in later errors.
                # We set the efficiency of the reverse mode to be -1
                # so that we can distinguish between reversible fan
                # types and unidirectional fan types later (Users
                # can't set negative values of installation efficiency
                # so it is a convenient flag for a unidirectional jet
                # fan).
                calcstuff = [tr_index, T_f, effic_f, U_f,
                                       0.,   -1.,     0.,
                             throw, length, runup, rundown]
            else:
                # It's a reversible jet fan, use all six values.
                T_f, effic_f, U_f, T_r, effic_r, U_r = JFdef[1:7]
                result = CheckJFThrust(JFtype_name, T_f, effic_f, U_f,
                                       "forwards", file_name, tr_index,
                                       line_triples, settings_dict, log)
                if result is None:
                    return(None)
                result = CheckJFThrust(JFtype_name, T_r, effic_r, U_r,
                                       "reverse", file_name, tr_index,
                                       line_triples, settings_dict, log)
                if result is None:
                    return(None)
                calcstuff = [tr_index, T_f, effic_f, U_f,
                                       T_r, effic_r, U_r,
                             throw, length, runup, rundown]
            JFcalc_dict.__setitem__(JFtype_name, calcstuff)
    # print("JFcalc_dict")
    # for entry, details in JFcalc_dict.items():
    #     print("  ", entry, details)
    return(JFblock_dict, JFcalc_dict)


def CheckJFThrust(name, thrust, insteff, velocity, direc, file_name,
                  tr_index, line_triples, settings_dict, log):
    '''Check that a jet fan thrust, air velocity and installation
    efficiency of a jet fan type have sensible values.  Warn if they
    look weird enough to not be available in any of the typical
    jet fan manufacturer's catalogues.  This check is in a routine
    of its own because we use it to check one set of values for
    unidirectional jet fans and two sets of values for reversible
    jet fans.

    We look for jet fan static thrusts (at 1.2 kg/m^3) between 100 N
    (based on a 560 mm diameter jet fan at low pitch angle and 50 Hz)
    and 2300 N (a 1600 mm diameter jet fan at high pitch angle and 60 Hz).
    We look for installation efficiencies above 0.2 and below one.
    We look for jet velocities between 20 m/s and 40 m/s.

    The routine raises warnings and continues rather than raising faults
    and stopping the run.  We may change that in future.
    '''
    units = settings_dict["units"]
    debug1 = settings_dict["debug1"]

    # Set the limits we check for.
    low_T, high_T = (100, 2300)  # Thrusts in N
    low_eta, high_eta = (0.2, 1) # Dimensionless installation efficiency
    low_v, high_v = (20, 40)     # Jet discharge speed in m/s
    if not (low_T <= thrust <= high_T):
        text1, text2 = RangeSnippets(thrust, low_T, high_T, "Force2", 2,
                                    units, debug1, log)
        err = ('> In the file named "' + file_name + '"\n'
               '> the "jetfantypes" block had a jet fan type (called\n'
               '> "' + name + '") with a thrust in ' + direc
                 + ' mode (' + text1 + ')\n'
               "> that's outside the usual range of " + text2 + '.'
              )
        gen.WriteError(2921, err, log)
        gen.ErrorOnLine2(tr_index, line_triples, log, False, True,
                         "Concerning")
    if not (low_eta <= insteff <= high_eta):
        text1, text2 = RangeSnippets(insteff, low_eta, high_eta, "null", 2,
                                    units, debug1, log)
        err = ('> In the file named "' + file_name + '"\n'
               '> the "jetfantypes" block had a jet fan type (called\n'
               '> "' + name + '") with an installation efficiency in '
                 + direc + '\n'
               '> mode (' + text1 + ") that's outside the usual range of "
                 + text2 + '.'
              )
        gen.WriteError(2922, err, log)
        gen.ErrorOnLine2(tr_index, line_triples, log, False, True,
                         "Concerning")
    if not (low_v <= abs(velocity) <= high_v):
        text1, text2 = RangeSnippets(abs(velocity), low_v, high_v, "speed1", 2,
                                    units, debug1, log)
        err = ('> In the file named "' + file_name + '"\n'
               '> the "jetfantypes" block had a jet fan type (called\n'
               '> "' + name + '") with a jet velocity in ' + direc
                 + ' mode (' + text1 + ')\n'
               "> that's outside the usual range of " + text2 + '.'
              )
        gen.WriteError(2923, err, log)
        gen.ErrorOnLine2(tr_index, line_triples, log, False, True,
                         "Concerning")
    return("all good")


def RangeSnippets(actual, low, high, range_units, decpl, units,
                  debug1, log):
    '''Generate a pair of strings that are used in warning messages
    in range checks.  The first is the value being used with the
    units text, e.g. "500 N" or "112.4 lbf".  The second is the text
    of the suggested low and high range limits with units text,
    e.g. "100 to 2300 N" or "22.48 to 517.06 lbf".  The three values
    passed are always in SI units, the text may be SI units or US units
    depending on the units being used in the input file.
    '''
    if units == "si":
        # Get the SI units text.
        discard, twounits = USc.ConvertToSI(range_units, 1.0, debug1, log)
        rtext = twounits[0]
    else:
        # Get the three numbers in US units and get the US units text.
        # We overwrite the three values, but that's OK.
        actual, twounits = USc.ConvertToUS(range_units, actual, debug1, log)
        (low, twounits) = USc.ConvertToUS(range_units, low, debug1, log)
        (high, twounits) = USc.ConvertToUS(range_units, high, debug1, log)
        rtext = twounits[1]
    text1 = gen.RoundText(actual, decpl) + ' ' + rtext
    text2 = gen.RoundText(low, decpl) + ' to '    \
            + gen.RoundText(high, decpl) + ' ' + rtext
    # We return text1.rstrip() and text2.rstrip() as text1 and
    # text2 may have trailing spaces.
    return(text1.rstrip(), text2.rstrip())


def ProcessVehTypes(line_triples, tr_index, settings_dict, log):
    '''Read all the data defining traffic types (vehicle properties and
    tables of emissions at different speeds and gradients).

        Parameters:
            line_triples [(int, str, str)]  List of lines in the file.
            tr_index        int             Pointer to where the block starts
                                            in line_triples.
            settings_dict   {}              Dictionary of the run settings.
            log             handle          The handle of the logfile.

        Returns:
            vehicles_dict   {}              Dictionary defining the vehicle
                                            type definitions.
            vehcalc_dict    {}              Dictionary defining the vehicles
                                            for the calculation.
        Errors:
            Aborts with 2721 if a line reading "calculate with blockage
            correction term" or "calculate without blockage correction
            term" is not found.
            Aborts with 2722 if the name of a vehicle type appeared twice.
            Aborts with 2723 if the name of a vehicle type was "tot",
            which is a reserved word (there is a property, "tot_flow"
            to plot the total vehicle flow on a route.
            Aborts with 2724 if there was a "speedgradpairs" entry for a
            vehicle type that doesn't exist.
            Aborts with 2725 if there was a "speedgradpairs" line with
            no data on it after the name of the vehicle type.
            Aborts with 2726 if there was a "speedgradpairs" line with
            gradients over 1.0 (100%)
            Aborts with 2727 if there was a "speedgradpairs" line with
            gradients over 0.12 (12%), a gradient that seldom appears
            in a road tunnel and is much steeper than modern PIARC
            pollution calculation methods can handle.  The last PIARC
            pollution method that could handle above 6% was PIARC 1991
            (it could handle gradients up to 12%).
    '''
    debug1 = settings_dict["debug1"]
    file_name = settings_dict["file_name"]
    units = settings_dict["units"]

    # Create a dictionary to hold the data for the traffic.  In the
    # "vehicle" setting, "QAstr" is a special that is processed after
    # returning from ProcessBlock.  It may contain additional numbers
    # for speed and PCU equivalents.  This setup lets us use one PCU
    # value for most of the vehicle types (cars), then have a set of
    # PCU values and speeds they apply up to for HGVs.  Two examples
    # of valid input (car first, then HGV):
    #             # name      area  C_d  PCU  speed  PCU  speed  PCU
    #    vehicle  petrol_car    2   0.4   1
    #    vehicle    HGV         6   1.0   3    10    2.5   20     2
    #
    # This means that the "petrol_car" vehicle type always takes up the
    # space of 1 PCU.  The "HGV" type takes up the space of 3 PCUs at
    # speeds up to and including 10 km/h, 2.5 PCUs above 10 km/h up and
    # including 20 km/h, then 2 PCUs at speeds above 20 km/h.  There
    # shouldn't be lots of values; the current PIARC emissions document
    # 2019R05EN (ISBN 978-2-84060-500-3) says to use 3 PCU/HGV below
    # 10 km/h and 2 PCU/HGV above 10 km/h, which would map to
    #   " 3    10   2"
    # at the end of a line of input.
    valid_settings = {"vehicle": ("#name",
                                  "float  +  area   a vehicle area",
                                  "float  +  null   a vehicle drag factor",
                                  "float  +  null   a PCU value",
                                  "QAstr"), # Optional list of speeds + PCU values.
                      "calculate": ("QAstr",),
                 "speedgradpairs": ("#name", "QAstr"),
                     }
    #
    # Define the optional entries allowed in each keyword.  Vehicles
    # have mass in tonnes in files in SI units, and masses in short
    # tons in files in US units.
    optionals = {"vehicle": {"mass": "float  +  null  a mean vehicle mass",
                              },
                }
    # 2019 emissions data on the PIARC website:
    # https://www.piarc.org/ressources/publications/10/30207,2019A02EN.xlsx

    #
    # Make a list of what entries we must have.  We must have either
    # "calculate with...", or "calculate without...", but not both.
    requireds = ("calculate", )
    #
    # Make a list of what entries can be duplicated.
    duplicates = ("vehicle", "speedgradpairs",)

    settings = (valid_settings, requireds, optionals, duplicates)
    block_name = "traffictypes"

    if debug1:
        print("Processing vehicle types")

    result = ProcessBlock(line_triples, tr_index, settings_dict,
                          block_name, {}, settings, log)
    if result is None:
        return(None)
    else:
        (discard, vehicles_dict) = result

    # Process the data we couldn't handle in ProcessBlock.

    # First check for one of two correct blockage ratio instructions.
    # "calculate" is a keyword that must appear, so we don't have to
    # use a try...except block here.
    block_term = vehicles_dict["calculate"]
    # We want one of two sentences that turn the blockage correction in
    # the calculation on or off.
    words = block_term[0].split()
    if words[:4] == ["without", "blockage", "correction", "term"]:
        # The user put an entry in to turn off the blockage adjustment.
        adjust = False
        block_mess = ("Road traffic drag calculation will not include a "
                      "blockage term.", "The vehicle drag factors below "
                      "are assumed to already account for blockage.")
    elif words[:4] == ["with", "blockage", "correction", "term"]:
        # The user put an entry in to turn on the blockage adjustment
        # for vehicles in the tunnnels.
        adjust = True
        block_mess = ("Road traffic drag calculation will include a blockage "
                      "term.", "The vehicle drag factors below"
                      "are assumed to not account for blockage.")
    else:
        # There was a line starting with "calculate", but it didn't
        # have the correct incantation after that.
        line_num, discard, line_text = line_triples[block_term[-1]]
        err = ('> In the file named "' + file_name + '"\n'
               '> the (road vehicle) "traffictypes" block had\n'
               '> no line with the phrase\n'
               '>   "calculate with blockage correction term"\n'
               '> or the phrase\n'
               '>   "calculate without blockage correction term".\n'
               '> on it.\n'
               '> Please edit the file to add one or the other.\n'
               '> There was a line starting with "calculate", but\n'
               "> the remaining words weren't what is required."
              )
        gen.WriteError(2721, err, log)
        gen.ErrorOnLine(line_num, line_text, log, False)
        return(None)

    # Make a new dictionary to be used in the calculation, using the
    # name of the vehicle type ("petrol_car", "HGV" etc). as the key.
    # The tuple they return includes all the data needed in the
    # calculation.  We also catch duplicate names of vehicle types
    # here, and get the maximum length of the names of the vehicle
    # types so we can format the table to be printed to the log file.

    maxlen = 0
    vehcalc_dict = {}
    for key, result in vehicles_dict.items():
        if key[:7] == "vehicle":
            new_key = result[0]
            # Check if the width of the name of the vehicle type is the
            # widest.  We need this to be able to format a table of
            # vehicle properties properly.
            new_len = len(new_key)
            if new_len > maxlen:
                maxlen = new_len

            # Check if there was an optional argument to set the
            # vehicle mass.
            if "mass" in result[-2]:
                mass = result[-2]["mass"]
                if units == "us":
                    (mass, discard) = USc.ConvertToUS("mass3", mass,
                                                      debug1, log)
            else:
                # This vehicle does not use mass in the emissions
                # calculation.  Set something that will cause a crash
                # if a calculation expects mass to be a number.
                mass = "No mass"
                #          tr_index      area       C_d     PCU-value

            if new_key in vehcalc_dict:
                # We have a duplicate name for the traffic type.
                # Complain.
                old_entry = vehcalc_dict[new_key]
                line1_num, discard, line1_text = line_triples[old_entry[0]]
                line2_num, discard, line2_text = line_triples[result[-1]]
                err = ('> In the file named "' + file_name + '"\n'
                       '> the (road vehicle) "traffictypes" block had\n'
                       '> two specifications of the same vehicle type\n'
                       '> ("' + new_key + '").  Please edit the file to remove\n'
                       '> one or change its name.'
                      )
                gen.WriteError(2722, err, log)
                gen.ErrorOnTwoLines(line1_num, line1_text,
                                    line2_num, line2_text, log, False)
                return(None)
            elif new_key.lower() == "tot":
                line_num, discard, line_text = line_triples[result[-1]]
                err = ('> In the file named "' + file_name + '"\n'
                       '> the (road vehicle) "traffictypes" block had\n'
                       '> a vehicle type named "' + new_key
                         + '", which is a\n'
                       '> reserved word.  Please edit the file to remove\n'
                       '> one or change its name.'
                      )
                gen.WriteError(2723, err, log)
                gen.ErrorOnLine(line_num, line_text, log, False)
                return(None)

            # Now check for multiple entries in the list that sets
            # the variation of PCU equivalent value with speed.
            if result[4] == '':
                # The user gave one PCU value for this vehicle type.
                # Build a tuple of two tuples that applies the value to
                # any speed.
                PCU_var = ((result[3],), (math.inf,))
            else:
                # There is at least one number after the PCU value.
                # Figure out how many.
                PCU_list = [str(result[3])] + result[4].split()
                count = len(PCU_list)
                if count == 2:
                    # This is a bit odd, they've only set a vehicle speed
                    # and no other, different PCU values.  We use the one
                    # PCU value up to math.inf.
                    PCU_var = ((result[3],), (math.inf,))
                else:
                    if divmod(count,2)[1] == 1:
                        # The user ended the list with a PCU value.  Put on
                        # an extra value that is very high (later we will
                        # replace this with math.inf).  Pedants may argue
                        # that this could trigger a confusing error if
                        # someone (a pedant, perhaps) included a speed in
                        # their list that is over 10,000,000 km/h.
                        # But I think that it will work fine for any real
                        # road tunnel application.
                        PCU_list.append("10000000.")
                    else:
                        # The user ended the list with a speed.  Leave it
                        # unchanged for the call to process the list of
                        # numbers.
                        pass
                    PCU_texts = "  ".join([num for num in PCU_list])

                    # Process the list of numbers by faking a block.  We
                    # want the PCU values to go down as we move to the right
                    # on the list and we want the speeds to go up.
                    line_num, discard, line_text = line_triples[result[-1]]
                    PCU_res = ProcessNumberList2(line_num, PCU_texts, line_text,
                                                settings_dict, "vehicles",
                                                "PCU values", "speeds",
                                                "<=", ">", log)
                    if PCU_res is None:
                        return(None)
                    else:
                        (discard, PCUs, speeds, PCU_lines, speed_lines) = PCU_res
                        # If the file is in US units, assume that the speed
                        # limits are in mph, not km/h.  Convert the speeds
                        # to km/h.
                        if units == "us":
                            speeds = ListToSI("speed2", speeds, debug1, log)

                    # Replace the last speed with math.inf.
                    speeds[-1] = math.inf
                    # Generate the list of pairs of PCU value and speed.  This
                    # list comprehension flattens the pairs of lists into a
                    # list that has no sublists.
                    PCU_var = (tuple(PCUs), tuple(speeds))

            # Generate an entry that we can use in the calculation loop
            # without to refer to the contents of the dictionary.  PCU_var
            # is a tuple that either has one PCU value followed by math.inf
            # for the speed or a list of PCU values that decrease as the
            # speed goes up, ending in a speed of math.inf.  That ought to
            # be bulletproof.
            new_values = [result[-1], result[1], result[2], PCU_var, mass, adjust]
            vehcalc_dict.__setitem__(new_key, new_values)


    # Make a list of the vehicle types that have lines of speed limit
    # applied to them and lines defining their PCU values at different
    # speeds (this is typically only HGVs).
    lim_speed = []

    # Process the pairs of speeds and gradients, if any.
    for key in vehicles_dict:
        if key[:14] == "speedgradpairs":
            # Check all the entries in the speedgradpairs keyword.  These are
            # a list of pairs of numbers (not likely to be very long so we
            # can assume they are all on one line).  The pairs of numbers
            # are a speed limit for the vehicle and the uphill gradient
            # that the speed limit applies to.  The keyword is a crib
            # for the order they are to be in: speed, then gradient
            entries = vehicles_dict[key]
            veh_type = entries[0]
            sp_index = entries[-1]
            if veh_type not in vehcalc_dict:
                # The name of the vehicle type in the speed limit entry
                # is not the name of a vehicle type.
                err = ('> In the file named "' + file_name + '"\n'
                       '> the (road vehicle) "traffictypes" block has\n'
                       '> a line of speed-gradient pairs for a type of\n'
                       '> vehicle that does not exist ("' + veh_type + '").\n'
                       '> The only valid vehicle types are\n'
                         + gen.FormatOnLines(vehcalc_dict.keys())
                      )
                gen.WriteError(2724, err, log)
                gen.ErrorOnLine2(sp_index, line_triples, log, False)
                return(None)
            elif entries[1] == '':
                # There was no data on the line after the name of the
                # vehicle type.
                err = ('> In the file named "' + file_name + '"\n'
                       '> the (road vehicle) "traffictypes" block has\n'
                       '> a line of speed-gradient pairs for the\n'
                       '> "' + veh_type + '" vehicle type, but there is\n'
                       '> no data on it.\n'
                       '> Please add one or more speed-gradient pairs,\n'
                       '> e.g. "' + veh_type + ' 120  0.04   80 0.06".\n'
                       '> N.B. This means "this vehicle type can travel\n'
                       '> at 120 km/h on upslopes that are equal to or\n'
                       '> less 4% and at 80 km/h on upslopes that are\n'
                       '> over 4% and equal to or less than 6%".'
                      )
                gen.WriteError(2725, err, log)
                gen.ErrorOnLine2(sp_index, line_triples, log, False)
                return(None)
            else:
                lim_speed.append(veh_type)
            # If we get to here, we have something on the rest of the
            # line.  Check it for sanity.  We want pairs of numbers:
            # the first one is a speed, the second a gradient.
            # In each pair of numbers, the first is the speed limit
            # in km/h, the second is the gradient that speed limit
            # applies down to (and including).  The higher the gradient,
            # the lower the speed, as this is all about vehicles trying
            # to climb hills.
            line_num, discard, line_text = line_triples[sp_index]
            # Process the numbers on the rest of the line.  This catches
            # non-numbers, names that are not constants and ensures that
            # the gradients increase and the speeds decrease.
            result = ProcessNumberList2(line_num, entries[1], line_text,
                                        settings_dict, "speedgradpairs",
                                        "speeds", "gradients",
                                        "<", ">", log)
            if result is None:
                return(None)
            else:
                (discard, speeds, gradients,
                 speed_lines, grad_lines) = result
                # If the file is in US units, assume that the speed
                # limits are in mph, not km/h.  Convert the speeds
                # to km/h.
                if units == "us":
                    speeds = ListToSI("speed2", speeds, debug1, log)

            # Check that the gradients are in the range 0 to +1,
            # not 0 to 100.  Then check if the gradient is so high that
            # it probably can't be used with PIARC emission tables.
            for gradient in gradients:
                if gradient > 1.0:
                    err = ('> In the file named "' + file_name + '"\n'
                           '> the (road vehicle) "traffictypes" block has\n'
                           '> a line of speed limits for the "' + veh_type
                             + '"  vehicle\n'
                           '> type, with pairs of numbers (speed limit\n'
                           '> and the gradient it applies up to.  One of\n'
                           '> the gradients ("' + str(gradient)
                             + '") is too high: gradients \n'
                           '> in "traffictypes" blocks are fractional\n'
                           '> (0 to 1), with 0 being a flat tunnel and 1\n'
                           '> being a vertical shaft.  If you gave the\n'
                           '> gradients as percentages, please divide\n'
                           '> them all by 100 to turn them into fractions.\n'
                           '> Otherwise, please modify the value (and any\n'
                           '> others over 1) to be between 0 and 1.'
                          )
                    gen.WriteError(2726, err, log)
                    gen.ErrorOnLine(line_num, line_text, log, False)
                    return(None)
                elif gradient >= 0.11:
                    # This is 11%.  The user might be thinking that the
                    # gradients are in percentage instead of fractions.
                    # Alternatively, someone might be trying to do a
                    # mine decline, which seem to be in the range 11-14%.
                    g_text = str(gradient*100) + "%,\n"
                    err = ('> In the file named "' + file_name + '"\n'
                           '> the (road vehicle) "traffictypes" block has\n'
                           '> a line of speed limits for the "' + veh_type + '"\n'
                           '> vehicle type.  One of the gradients is over\n'
                           '> 11%, which is practically unheard of in a\n'
                           '> road tunnel.  Are you sure that these\n'
                           '> gradients are in fractions (0-1) and not\n'
                           '> in percentages (0 to 100)?  Note that if\n'
                           '> you are trying to model a mine decline\n'
                           "> tunnel as a road tunnel, you're probably\n"
                           '> out of luck, as the standard method of\n'
                           '> estimating emissions in road tunnels\n'
                           '> only goes up to 6%.  If you are rolling\n'
                           '> your own custom emissions and genuinely\n'
                           '> have emissions tables that apply at '
                             + g_text +
                           '> then your best bet is to edit the source\n'
                           '> code of Hobyah.py to temporarily comment\n'
                           '> out the block of code that raises error\n'
                           '> message 2727 and the line of entry with\n'
                           '> "return(None)" on it (the "return(None)\n'
                           '> is what causes the run to stop).'
                          )
                    gen.WriteError(2727, err, log)
                    gen.ErrorOnLine(line_num, line_text, log, False)
                    return(None)
            # If we get to here, all is well.  Add the speed limits
            # to the relevant entry in vehcalc_dict.
            # First add entries that make the last speed extend to
            # the highest possible gradient (vertical, or 1.0).
            speeds.append(speeds[-1])
            gradients.append(1.0)
            entry = vehcalc_dict[veh_type]
            entry.extend([tuple(speeds), tuple(gradients)])
            vehcalc_dict.__setitem__(veh_type, entry)



    # Print the vehicle data to the screen and to the log file.  It's
    # easy for engineers to screw up the input, so we need to make the
    # printout as unambiguous as possible.
    # First print the line about how the blockage correction will be
    # handled, to the log file and to the screen.
    for line in block_mess:
        gen.WriteMessage2(line, log)

    # Write a line of headers for the table of vehicle data.  We do this
    # in SI units only, because I don't think many people with a lot of
    # experience are going to write PIARC calculations in US units.
    width = max(9, maxlen)
    mess = ("Vehicle".center(width)
            + '  Area  Drag factor   PCU       Mass        '
            + '  Vehicle speed limits on uphill grades',
            "name".center(width)
            + '  (m^2)    (-)       value    (tonnes)        '
            + '  Limiting gradient     Speed limit', '-'*(width + 84))
    for line in mess:
        gen.WriteMessage2(line, log)

    # Check for vehicle types with and without speed limits on gradients,
    # definitions of mass, and variation of PCU value with speed.
    for veh_type in vehcalc_dict:
        entry = vehcalc_dict[veh_type]
        masstext = entry[4]
        if masstext != "No mass":
            # There was an optional argument to set the vehicle mass,
            # so 'masstext' may be a number.
            masstext = gen.RoundText(masstext, 3)
        else:
            masstext = "Not used"

        # Check for one PCU value or several.
        PCU_spec = entry[3]
        if len(PCU_spec[0]) == 1:
            # One value of PCU equivalent was given.
            PCU_text = PCU_spec[0][0]
        else:
            # State that the PCU value varies.  We'll put the variation
            # into the transcript later.
            PCU_text = "Varies"
        if units == "us":
            area, C_d = entry[1:3]
            mess = (veh_type.center(width)
                    + str(round(area,4)).center(9)
                    + str(round(C_d,4)).center(8)
                    + str(PCU_text).center(13)
                    + masstext.center(8))
        else:
            mess = (veh_type.center(width) + str(entry[1]).center(9) +
                    str(entry[2]).center(8) + str(PCU_text).center(13)
                    + masstext.center(8))

        if veh_type not in lim_speed:
            # This vehicle type does not have user-generated speed limits.
            # Set things such that any speed is allowed on any gradient.
            speeds = (math.inf,)
            gradients = (1.0,)
            entry.extend([speeds, gradients])
            vehcalc_dict.__setitem__(veh_type, entry)
            # Add a note to the message that there were no speed limits
            # in the vehicle type.
            mess = mess + "         Flat to vertical:     no speed limit"
            gen.WriteMessage2(mess, log)
        else:
            # The user set some speed limits.  Write them.
            speeds = entry[6]
            gradients = entry[7]
            for (index, gradient) in enumerate(gradients):
                speed = speeds[index]
                gradperc = gradient * 100
                grad_text = str(gradient) + ' (' +  \
                            str(round(gradient * 100,2)) + '%)'
                if index == 0:
                    speed_text = (str(round(speed,1)) + ' km/h').rjust(15)
                    text = (('Below & at ' + grad_text + ':').rjust(29) +
                             speed_text)
                    # Add it to the first line
                    mess = [mess + text]
                else:
                    speed_text = (str(round(speed,1)) + ' km/h').rjust(12)
                    text = ('>' + old_text + ' to ' + grad_text
                            + ':').rjust(74 + maxlen) + speed_text
                    # Tab it across a suitable number of spaces.
                    mess.append(text)
                old_text = grad_text.replace('(','(>')
            for line in mess:
                gen.WriteMessage2(line, log)

    # Check for vehicle types with variable PCU values and write out the
    # rules they follow.
    for veh_type in vehcalc_dict:
        entry = vehcalc_dict[veh_type]
        # Get the list of PCU values and speeds.
        (PCUs, speeds) = entry[3]
        if len(PCUs) > 1:
            # There are rules about how to vary PCU with speed for this
            # type of vehicle.  Print them to the screen and the logfile.
            PCU1 = PCUs[0]
            oldspeed = str(round(speeds[0],2))
            mess = ('Vehicle type "' + veh_type + '" occupies '  \
                    + 'different amounts of space at different speeds:',
                    ('<=' + oldspeed + ' km/h,').center(30) \
                    + ' vehicle type occupies ' + str(PCU1) + ' PCU')
            for line in mess:
                gen.WriteMessage2(line, log)

            for PCU, speed in zip(PCUs[1:],speeds[1:]):
                txt_speed = str(round(speed,2))
                if speed is math.inf:
                    # This is the last entry.  We can put in a short phrase.
                    speed_text = ('>' + oldspeed + ' km/h,').center(30)
                else:
                    speed_text = ('>' + oldspeed + ' km/h & <='\
                                  + txt_speed + ' km/h,').center(30)
                mess = speed_text + ' vehicle type occupies ' + str(PCU) + ' PCU'
                gen.WriteMessage2(mess, log)
                # Save the speed for the next line of printout.
                oldspeed = txt_speed
    # vehicles_dict has all the input in the "traffictypes" block.
    # vehcalc_dict has all the values needed for the calculation.
    if False:
        for key in vehcalc_dict:
            print(key, vehcalc_dict[key])
    return(vehicles_dict, vehcalc_dict)


def ProcessTraffic1(line_triples, tr_index, settings_dict,
                    routes_dict, route_indices, log):
    '''Read the data putting steady-state traffic into routes.
    This is the preferred way of defining traffic (point to point)
    because it matches well to how traffic modellers like to present
    their data (a grid of vehicle flows from point to point).
    We may add others later, like applying traffic to sectypes (how the
    MM Roads program does it) or applying traffic to portals and defining
    how they split at joins (how IDA does it).

        Parameters:
            line_triples [(int, str, str)]  List of lines in the file.
            tr_index        int             Pointer to where the block starts
                                            in line_triples.
            settings_dict   {}              Dictionary of the run settings.
            routes_dict     {}              Dictionary of the routes, for
                                            error checking.
            route_indices   {}              Dictionary of the indices of
                                            the blocks that have put
                                            traffic into routes.  The keys
                                            are the names of routes, the
                                            result are values of tr_index.
                                            Used in an error message.
            log             handle          The handle of the logfile.

        Returns:
            routes_used     []              List of route names used in this
                                            block.
            traffic1_dict   {}              Dictionary defining the contents
                                            of the traffic definition: vehicle
                                            flows, speeds and densities.
        Errors:
            Aborts with 2741 if the user tried to put traffic in
            without defining any traffictypes.
            Aborts with 2742 if the user tried to put traffic into a
            route that does not exist.
            Aborts with 2743 if the user tried to put traffic into the
            same route a second time in this block.
            Aborts with 2744 if the user tried to put traffic into a
            route that has already been used in an earlier block.
            Aborts with 2745 if one or more of the routes have traffic
            numbers but no associated "standstill" or "moving" line
            of entry.
    '''
    debug1 = settings_dict["debug1"]
    file_name = settings_dict["file_name"]
    units = settings_dict["units"]
    vehcalc_dict = settings_dict["vehcalc_dict"]
    vehicles_dict = settings_dict["vehicles_dict"]

    # Check if the user has tried to put in traffic without defining
    # any traffic types.
    if vehcalc_dict == {}:
        err = ('> In the file named "' + file_name + '",\n'
               '> there was a "trafficsteady" block (a block that\n'
               '> puts traffic into routes) without there being a\n'
               '> "traffictypes" block to define the properties of\n'
               '> the traffic.\n'
               '> Please edit the file to add a "traffictypes" block\n'
               '> or remove all the blocks that put traffic in routes.'
              )
        gen.WriteError(2741, err, log)
        return(None)

    # Get some data out that we need for printing.
    veh_names = list(vehcalc_dict.keys())
    maxlen = max([len(name) for name in veh_names])

    # Check if there is a line defining the routes, the speed and the
    # density at standstill and any limits on the extent of traffic.
    # We skip all the other lines and handle them later.
    valid_settings = {"routes": ("#name",  # We want at least one route name
                                 "QAstr"), # This holds the rest of the names
                      "standstill": ("#name",
                                     "float  0+  null   a vehicle density",
                                     ("PCU/lane-km", "veh/lane-km",
                                      "PCU/lane-mile", "veh/lane-mile"),
                                      "QAstr"),
                      "moving": ("#name",
                                 "float  any  speed2   a vehicle speed",
                                 "QAstr"),
                      "#skip": "discard"  # This catches all other lines
                     }
    # Define the optional entries allowed in each keyword.
    # One optional argument allows the traffic density to be above
    # 165 PCU/km (PIARC has recommended a limit of 165 PCU/km for many
    # years).  The other allows traffic to exceed 120 km/h (PIARC's
    # 2019 emission tables go up to 120 km/h).
    optionals = {"standstill": {"highdensity": ("allowed",)},
                 "moving":      {"hoons": ("allowed",)},
                }

    # Make a list of what entries we must have.  There are two
    # arguments we need one or more of (speed and standstill) but
    # we can't tell how many we need (depends on how many routes have
    # traffic set in them).
    requireds = ("routes", )

    # Make a list of what entries can be duplicated.
    duplicates = ["standstill", "moving"]

    settings = (valid_settings, requireds, optionals, duplicates)
    block_name = "trafficsteady"

    if debug1:
        print("Processing steady-state traffic")

    result = ProcessBlock(line_triples, tr_index, settings_dict,
                          block_name, {}, settings, log)
    if result is None:
        return(None)
    else:
        (dict_name, traffic1_dict) = result

    data = traffic1_dict["routes"]
    # Build a list of the routes used.
    routes_used = [data[0]] + data[1].lower().split()
    # Get some data for error messages.
    line_num, discard, line_text = line_triples[data[-1]]
    route_names = list(routes_dict.keys())
    route_names.sort()

    for name in routes_used:
        if name not in route_names:
            err = ('> In the file named "' + file_name + '",\n'
                   '> the "trafficsteady ' + dict_name + '" block\n'
                   '> tried to put traffic into a route that does not\n'
                   '> exist, "' + name + '".\n'
                   '> Please edit the file to either add a route with\n'
                   '> that name, remove the route name from the block,\n'
                   "> or correct the name of the route.  For what it's\n"
                   '> worth, these are the valid route names:\n'
                     + gen.FormatOnLines(route_names)
                  )
            gen.WriteError(2742, err, log)
            gen.ErrorOnLine(line_num, line_text, log, False)
            return(None)
        elif routes_used.count(name) > 1:
            # The route appears more than once in this block.
            err = ('> In the file named "' + file_name + '",\n'
                   '> there is a "trafficsteady" block that puts\n'
                   '> traffic into the same route, "'
                     + name + '" twice.\n'
                   '> Please edit the file so that each route name\n'
                   '> appears only once in all the "trafficsteady"\n'
                   '> blocks.'
                  )
            gen.WriteError(2743, err, log)
            gen.ErrorOnLine(line_num, line_text, log)
            return(None)
        elif name in route_indices:
            # The route appears more than once.  The first entry
            # is in another "trafficsteady" block, the second is in
            # this block.
            line_num2, line_data2, line_text2 = line_triples[tr_index]
            other_index = route_indices[name]
            line_num, line_data, line_text = line_triples[other_index]
            other_name = line_data.split()[-1]
            err = ('> In the file named "' + file_name + '",\n'
                   '> there are two "trafficsteady" blocks that put\n'
                   '> traffic into the same route, "' + name + '".  The \n'
                   '> first trafficsteady block is "' + other_name + '" and\n'
                   '> the other is "' + dict_name + '".\n'
                   '> Please edit the file so that each route name\n'
                   '> appears only once.'
                  )
            gen.WriteError(2744, err, log)
            gen.ErrorOnTwoLines(line_num, line_text,
                                line_num2, line_text2, log,
                                False, False, "Relevant")
            return(None)
    # When we get to here, we know all the route names are valid.  Now
    # we build a new instance of valid_settings that lets us process the
    # other lines in the block (the ones we ignored before).  We want
    # the name of a valid traffic type and as many counts of vehicle
    # numbers as we have route names in the block.  This is easier to
    # explain by showing an example and showing which lines are processed
    # first and which are processed second:
    #
    #     routes EBmain   EBbranch   # This line processed first (code above).
    #     car_p   1800      300      # These next five lines processed second.
    #     car_d    450       80      # Their first words are the names of
    #     LCV_p     20        5      # vehicle types and the count of
    #     LCV_d     90       20      # numbers after them is the count of
    #     HGV      300       80      # names after the "routes" keyword

    # Get the count of the routes and use it to generate a spec for
    # the lines of vehicle flowrates that is that length.  Then
    # reprocess the block with the vehicle types as a keyword and
    # a count of vehicle flowrates equal to the count of routes.
    route_count = len(routes_used)
    flowspec = ("float 0+ null  a vehicle flowrate",) * route_count
    for veh_type in vehcalc_dict:
        valid_settings.__setitem__(veh_type, flowspec)

    # Add "allroutes" to the list of valid names.  "Allroutes" is a
    # shortcut meaning 'all the routes listed in this trafficsteady
    # block's "route" keyword'.  A useful shortcut when you have
    # many routes in a block and want them all to have the same
    # traffic speed or the same stationary vehicle density.
    route_names.append("allroutes")
    # We remove the entry that skips lines and alter the entries for
    # the "standstill" and "moving" keywords to let them apply to
    # a particular route and set the extents over which stationary
    # traffic extends.
    valid_settings.pop("#skip")
    valid_settings.__setitem__("standstill",
                               (tuple(route_names),
                                "float  0+  null   a vehicle density",
                                ("PCU/lane-km", "veh/lane-km",
                                "PCU/lane-mile", "veh/lane-mile"),
                                # Can't process these two entries yet as we
                                # don't know the route's portal chainages.
                                # "float  any  dist1  a traffic start chainage",
                                # "float  any  dist1  a traffic stop chainage"),
                                "QAstr"))
    valid_settings.__setitem__("moving",
                               (tuple(route_names),
                                "float  any  speed2   a vehicle speed"))

    result = ProcessBlock(line_triples, tr_index, settings_dict,
                          block_name, {}, settings, log)
    if result is None:
        return(None)
    else:
        (dict_name, traffic1_dict) = result

    # Generate a list of lists of zeros for the traffic flowrates.  Each
    # time we find a line of entries for the vehicle flowrates, we set
    # the appropriate sub-list with the entries on the relevant line.
    # This means that we assign the vehicle flows correctly even if
    # the order of entry of the vehicle types in the 'trafficinroute'
    # blocks is not the same as the order of entries in the
    # 'traffictypes' block.
    veh_types = list(vehcalc_dict.keys())
    veh_flows = [[0] * route_count] * len(veh_types)

    for veh_index, veh_type in enumerate(veh_types):
        if veh_type in traffic1_dict:
            # Get all the numbers in the entries and turn them into
            # vehicle flowrates for each route.
            flowrates = traffic1_dict[veh_type][:route_count]
            veh_flows[veh_index] = flowrates

    # We now have a list of lists in which the sub-lists are the flows
    # for each vehicle type.  We want to get a new list of lists in
    # which the sub-lists are the flows in each route.
    route_flows = list(zip(*veh_flows))

    # Now check the speed entry/traffic density entries.  We want either
    # a line setting the speed or a line stating that the traffic is at
    # standstill and setting the traffic density and extents.
    # We make dictionary to hold the properties.  The key is the route
    # name and the result is:
    #  * For moving traffic, the traffic speed in m/s and the index
    #    of the line_triple that holds it (for error messages).
    #  * For stationary traffic, a tuple of zero traffic speed, the
    #    density at rest, the units the density is expressed in, the
    #    chainage the traffic starts at, the chainage the traffic
    #    stops at and the index of the line_triple that holds it
    #    (for error messages)
    traffic_stuff = {}
    # Set a Boolean that lets us pick up if the user has defined
    # "moving" or "standstill for "allroutes" and for one named route.
    # Set an invalid value for the tr_index of the line that sets
    # all routes.
    all_used = False
    tr_allroutes = math.nan
    for key in traffic1_dict.keys():
        traffic_data = traffic1_dict[key]
        if key[:6] == "moving":
            route = traffic_data[0]
            # Now check if we need to loop over all the routes or
            # just set values for one route.
            if route == "allroutes":
                for loc_route in routes_used:
                    result = MovingSpeeds(loc_route, routes_dict,
                                          route_flows, traffic_data,
                                          settings_dict, routes_used,
                                          traffic_stuff, dict_name,
                                          all_used, tr_allroutes, True,
                                          line_triples, settings, log)
                    if result is None:
                        return(None)
                    else:
                        # We have updated traffic_stuff with the new
                        # entry.
                        traffic_stuff = result
                all_used = True
                # Set tr_index for the "allroutes" entry in case there
                # are other lines defining traffic.
                tr_allroutes = traffic_data[-1]
            else:
                # Just set an entries for one new route in traffic_stuff.
                result = MovingSpeeds(route, routes_dict,
                                      route_flows, traffic_data,
                                      settings_dict, routes_used,
                                      traffic_stuff, dict_name,
                                      all_used, tr_allroutes, False,
                                      line_triples, settings, log)
                if result is None:
                    return(None)
                else:
                    traffic_stuff = result

        elif key[:10] == "standstill":
            route = traffic_data[0]
            # Now check if we need to loop over all the routes or
            # just set values for one route.
            if route == "allroutes":
                for loc_route in routes_used:
                    result = StationaryDists(loc_route, routes_dict,
                                             route_flows, traffic_data,
                                             settings_dict, routes_used,
                                             traffic_stuff, dict_name,
                                             all_used, tr_allroutes, True,
                                             line_triples, settings, log)
                    if result is None:
                        return(None)
                    else:
                        traffic_stuff = result
                all_used = True
                # Set tr_index for the "allroutes" entry in case there
                # are other lines defining traffic.
                tr_allroutes = traffic_data[-1]
            else:
                # Just set an entries for one new route in traffic_stuff.
                result = StationaryDists(route, routes_dict,
                                         route_flows, traffic_data,
                                         settings_dict, routes_used,
                                         traffic_stuff, dict_name,
                                         all_used, tr_allroutes, False,
                                         line_triples, settings, log)
                if result is None:
                    return(None)
                else:
                    traffic_stuff = result

    # Check that all the routes that traffic numbers have been set in
    # have an associated "standstill" or "moving" keyword and complain
    # about those that do not have one.
    unused = []
    for route_name in routes_used:
        # Get a list of the names of routes that do not have a "standstill"
        # or "moving" keyword.
        if route_name not in traffic_stuff.keys():
            unused.append(route_name)
    count = len(unused)
    if count > 0:
        # At least one route has traffic numbers but no "standstill"
        # or "moving" keyword.
        snippet1 = 'route' + gen.Plural(count) + ':\n'
        if count == 1:
            snippet2 = "this route."
        else:
            snippet2 = "these routes."
        # Get the data for the line at the start of the block.
        line_number, discard, line_text = line_triples[tr_index]
        err = ('> In the file named "' + file_name + '",\n'
               '> the "trafficsteady ' + dict_name + '" block set\n'
               '> traffic numbers in the following ' + snippet1
                 + gen.FormatOnLines(unused) + '\n'
               '> Please edit the file to add lines of "standstill"\n'
               '> or "moving" for ' + snippet2
              )
        gen.WriteError(2745, err, log)
        gen.ErrorOnLine(line_number, line_text, log, False, True, "Relevant")
        return(None)

    # Remove the entry in trafficstuff for "allroutes", we don't need
    # it any more and leaving it in may cause problems later.
    if "allroutes" in traffic_stuff:
        traffic_stuff.pop("allroutes")
    # If we get to here, the traffic speeds and traffic flows have been
    # set for all routes are fine and we have no conflicts.
    # Now add the flowrate, speeds and tunnel occupancies for each route
    # to the dictionary.
    for route in traffic_stuff:
        traffic1_dict.__setitem__("#flow_" + route, traffic_stuff[route])
    return(routes_used, traffic1_dict)


def StationaryDists(route, routes_dict, route_flows, traffic_data,
                    settings_dict, routes_used, traffic_stuff, dict_name,
                    all_used1, tr_allroutes, all_used2, line_triples,
                    settings, log):


    '''Take the name of a route and data for the routes and vehicles.
    Check if there is a clash between setting a route while having already
    set traffic in this route.
    Get and check the vehicle density and the extents of the traffic.
    Build a list of data that is used in the calculation for this route
    and return it.

        Parameters:
            route           str             Name of a route.
            routes_dict     {}              Dictionary of the routes, for
                                            error checking.
            route_flows     [[]]            List of list of traffic flows in
                                            the routes in this block.
            traffic_data    []              List of data that sets stationary
                                            traffic in the route.
            settings_dict   {}              Dictionary of the run settings.
            routes_used     []              List of route names used in this
                                            block.
            traffic_stuff   {}              A dictionary of the traffic data
                                            set in routes.
            all_used1       Bool            If True, the "allroutes" entry
                                            has been used already.
            all_used2       Bool            If True, this call to this
                                            routine is from an "allroutes"
                                            entry.
            line_triples [(int, str, str)]  List of lines in the file.
            log             handle          The handle of the logfile.

        Returns:
            traffic_data    []              An updated dictionary with the
                                            entries from this route added.
        Errors:
            Aborts with 3041 if the user tried to use a traffic density
            that is above 165 PCU/lane-km or above 165 veh/lane-km.
            Aborts with 3042 if the user tried to use traffic extents
            that were less than 0.1 m apart.
            Aborts with 3043 if the user tried to set the extents of
            a route that does not have a count of lanes in it.

    '''
    debug1 = settings_dict["debug1"]
    file_name = settings_dict["file_name"]
    units = settings_dict["units"]
    vehcalc_dict = settings_dict["vehcalc_dict"]
    vehicles_dict = settings_dict["vehicles_dict"]

    # Get some data out that we need for printing.
    veh_names = list(vehcalc_dict.keys())
    maxlen = max([len(name) for name in veh_names])

    # Check if we have already set traffic in this route.
    result = TrafficClash(route, line_triples, traffic_stuff, traffic_data,
                          all_used1, tr_allroutes, all_used2, dict_name,
                          file_name, log)
    if result is None:
        return(None)

    (discard, density, inp_units,
                QAstr, opt_args, stat_index) = traffic_data
    line_number, discard, line_text = line_triples[stat_index]

    # Spoof a block with this line and call ProcessBlock to read
    # it.
    (valid_settings, requireds, optionals, duplicates) = settings

    spoof_block = [(1, "begin trafficsteady " + dict_name,
                    "begin trafficsteady " + dict_name),
                   line_triples[stat_index],
                   (3, "end trafficsteady", "end trafficsteady") ]
    valid_settings = {"standstill":
                       (tuple(routes_used + ["allroutes"]),
                        "float  0+  null   a vehicle density",
                        ("PCU/lane-km", "veh/lane-km",
                        "PCU/lane-mile", "veh/lane-mile"),
                        "float  any  dist1  a traffic start chainage",
                        "float  any  dist1  a traffic stop chainage")}
    tunnel_chs = routes_dict[route]["tunnel_chs"]
    # First, we add the chainages of the up and down portals
    # to the list of constants so that the user can put in the
    # phrase "up_ptl" and "down_ptl" and the program will treat
    # it as a constant and use whatever chainage is at the
    # relevant portal.
    up_ptl = tunnel_chs[0]
    down_ptl = tunnel_chs[-1]
    settings_dict.__setitem__("#up_ptl", [str(up_ptl), str(up_ptl), 1])
    settings_dict.__setitem__("#down_ptl", [str(down_ptl),
                                            str(down_ptl), 1])
    # Set the required entries to an empty list.
    settings = (valid_settings, [], optionals, duplicates)
    result = ProcessBlock(spoof_block, 0, settings_dict,
                          "trafficsteady", {}, settings, log)
    if result is None:
        return(None)
    start_ch, stop_ch = result[1]["standstill#1"][3:5]
    # Remove the temporary constants so that they can't be
    # used anywhere else.
    settings_dict.pop("#up_ptl")
    settings_dict.pop("#down_ptl")

    route_dict = routes_dict[route]

    # Process the vehicle density at standstill and turn it into
    # a vehicle density in either PCU/lane-km or veh/lane-km.
    # We get it from the text of the line so that the case is
    # preserved.
    if "mile" in inp_units:
        # Get the value as a string in case we need it for an error
        # message later.
        permiletext = str(density)
        # Changes 'per mile' to per km.
        density = density / 1.609344

    # Check the vehicle density.  PIARC has recommended
    # 165 PCU/lane-km as good figure to use for stationary
    # traffic for years.
    maxdens = 165.0
    # There is an optional argument, "highdensity:=allowed" that
    # prevents this error from being raised.  We don't need the
    # value (there can be only one) so we just check if the
    # argument is not present in the dictionary of optional
    # entries.
    if density > maxdens and "highdensity" not in opt_args:
        maxtext = str(maxdens)
        if "mile" not in inp_units:
            dens_text1 = maxtext + ' ' + inp_units + ' (it was '  \
                         +  str(density) + ' ' + inp_units + ').\n'
            dens_text2 = maxtext + ' ' + inp_units + '.'
        else:
            mod_units = inp_units.replace("mile", "km")
            maxtextmph = str(round(maxdens * 1.609344, 2))

            dens_text1 = maxtextmph + ' ' + inp_units + ' (it was '  \
                         +  permiletext + ' ' + inp_units + ').\n'
            dens_text2 = maxtextmph + ' ' + inp_units + ' ('  \
                         + maxtext + ' ' + mod_units + ').'

        err = ('> In the file named "' + file_name + '",\n'
               '> the "trafficsteady ' + dict_name + '" block had\n'
               '> stationary traffic with a traffic density above\n'
               '> ' + dens_text1 +
               '> Please edit the file so that the density is at\n'
               '> or below ' + dens_text2 + '\n'
               '> Alternatively, add the optional argument\n'
               '> "highdensity := allowed" to the line.'
              )
        gen.WriteError(3041, err, log)
        gen.ErrorOnLine(line_number, line_text, log, False)
        return(None)
    else:
        # Make a Boolean to tell whether the density is set as PCUs
        # or vehicles.
        if "PCU" in inp_units.upper():
            PCUperkm = True
        else:
            PCUperkm = False

    if math.isclose(start_ch, stop_ch, abs_tol = 1e-1):
        err = ('> In the file named "' + file_name + '",\n'
               '> the "trafficsteady ' + dict_name
                 + '" block tried to\n'
               '> limit the extent of traffic in route "'
                 + route + '",\n'
               '> but the extent of the traffic is too short to\n'
               '> matter (less than 0.1 m).\n'
               '> Please edit the file to either remove the line\n'
               "> or adjust the chainages."
              )
        gen.WriteError(3042, err, log)
        gen.ErrorOnLine(line_number, line_text, log, False)
        return(None)
    elif start_ch > stop_ch:
        # We can handle the start chainage being above the stop
        # chainage.  We just reverse them.
        start_ch, stop_ch = stop_ch, start_ch

    if 'lanes' not in route_dict:
        # The user set stationary traffic in a route but the
        # count of lanes not set in the route dictionary.
        # Get the line that starts the traffic1 block.
        route_name, UC_name, tr_index = route_dict["block_index"]
        line_num, discard, line_text = line_triples[tr_index]
        err = ('> In the file named "' + file_name + '",\n'
               '> the "trafficsteady "' + dict_name
                + '" block tried to\n'
               '> set traffic in route "' + route_name
                 + '" but the\n'
               '> route definition does not have a count of\n'
               '> lanes for it.  Stationary traffic cannot be\n'
               '> set without knowing lane counts.\n'
               '> Please edit the file to add an entry for the\n'
               '> count of lanes to the definition of route\n'
               '> "' + route_name + '".'
              )
        gen.WriteError(3043, err, log)
        gen.ErrorOnLine(line_num, line_text, log, False,
                        True, "Relevant")
        return(None)
    # Get the traffic for this route.
    flow_index = routes_used.index(route)
    traffic_flow = route_flows[flow_index]

    # We take the vehicle flows and turn them into fractions
    # of the density at standstill to use.  This is handy
    # because it means we can set (say) traffic flowrates
    # for each vehicle type at speed 50 km/h and then (in
    # another file) change the 'speed' entry to 'standstill'
    # and the program will figure out what the density of
    # traffic at stationary is.
    # Let's say we have 3000 cars/hr and 400 HGVs/hr.  Cars
    # are 1 PCU (by definition).  HGVs are 3 PCU in traffic
    # at standstill.  We want 165 PCU/lane-km at rest.
    # The program does the following calculation:
    #   3000 cars/hr = 3000 * 1 = 3000 car-PCUs at rest.
    #   400 HGVs/hr = 400 * 3 = 1200 HGV-PCUs at rest.
    #   Total is 4200 PCUs at rest.
    #   Car PCUs make up 3000/4200 = 71.4% of the mix
    #   HGV PCUs make up 1200/4200 = 28.6% of the mix
    #   We have 165 * 0.714 / 1 = 117.9 cars/lane-km
    #   We have 165 * 0.286 / 3 = 15.7 HGVs/lane-km
    # Getting the program to do it is much safer than getting
    # graduate engineers to do that kind of tedious calculation.
    tot_vehs = sum(traffic_flow)
    # We need to multiply each vehicle flowrate by the PCU
    # value at rest of its vehicle type before we calculate
    # the density (which is stored in veh/lane-km or in
    # PCU/lane-km).  We also make a list of the PCU values
    # for printing later.
    tot_PCUs = 0
    PCU_vals = []
    for index, (veh_type, veh_dict) in enumerate(vehcalc_dict.items()):
        PCU_at_rest = veh_dict[3][0][0]
        tot_PCUs += traffic_flow[index] * PCU_at_rest
        if math.isclose(int(PCU_at_rest), PCU_at_rest):
            PCU_vals.append(int(PCU_at_rest))
        else:
            PCU_vals.append(PCU_at_rest)
    mean_PCU = tot_PCUs / tot_vehs
    # Get the vehicle density in veh/lane-km.  Write slightly
    # different text to the log file depending on whether the
    # user expressed the vehicle density in PCUs or vehicles.
    if PCUperkm:
        PCU_dens = density
        veh_dens = density / mean_PCU
        text1 = gen.RoundText(PCU_dens, 3) + ' PCU/lane-km'
        text2 = '  So ' + text1 + ' is equivalent to ' \
                + gen.RoundText(veh_dens, 3)  \
                + ' veh/lane-km:'
    else:
        PCU_dens = density * mean_PCU
        veh_dens = density
        text1 = gen.RoundText(veh_dens, 3) + ' veh/lane-km'
        text2 = '  So ' + text1 + ' is equivalent to ' \
                + gen.RoundText(PCU_dens, 3)  + ' PCU/lane-km:'
    gen.WriteMessage2('', log)
    gen.WriteMessage2('Route "' + route + '" has stationary'
                      + ' traffic at ' + text1, log)

    # These lists hold the numbers on the numerator and the
    # denominator of the fraction printed below.
    mult_str = []
    flow_str = []
    for PCU, flow in zip(PCU_vals, traffic_flow):
        flow_str.append(gen.RoundText(flow, 3))
        mult_str.append(str(PCU) + "*" + flow_str[-1])
    gen.WriteMessage2("  Mean PCU value calculation:", log)
    # Write the calculation of the mean PCU value, because
    # I always have problems calculating it when I haven't
    # calculated it for a while.
    numerator = "         "                          \
            + ' + '.join([mult for mult in mult_str])
    count = len(numerator) - 9
    gen.WriteMessage2(numerator, log)
    # Dividing line and mean PCU value
    divisor = "         " + '-' * count  \
               + ' = ' + str(round(mean_PCU, 3)) + ' PCU/veh'
    gen.WriteMessage2(divisor, log)
    # Write the text of the denominator.
    denom = "         "                        \
            + (' + '.join([flow for flow in flow_str]).center(count))
    gen.WriteMessage2(denom, log)
    gen.WriteMessage2(text2, log)

    loc_dens = []
    for veh_index3, flow in enumerate(traffic_flow):
        veh_dens2 = veh_dens * flow / tot_vehs
        PCU_dens2 = veh_dens2 * PCU_vals[veh_index3]
        loc_dens.append(veh_dens2)
        mess = ("    " + veh_names[veh_index3].rjust(maxlen) + ':'
                 + ( gen.FloatText(round(veh_dens2,2))).rjust(9)
                 + " veh/lane-km "
                 + ("(" + gen.FloatText(round(PCU_dens2,2))).rjust(8)
        + " PCU/lane-km)")
        gen.WriteMessage2(mess, log)

    # Print the extents of the tunnels and traffic in the route.
    PrintExtents(up_ptl, down_ptl, start_ch, stop_ch, log)

    # Make a tuple of the type of traffic, density, traffic flows,
    # names of tunnels occupied and tr_index that we store in
    # traffic_stuff.
    tun_chs = route_dict["tunnel_chs"]
    tun_names = []
    for index, up_ch in enumerate(tun_chs[:-1]):
        down_ch = tun_chs[index + 1]
        if start_ch <= down_ch and stop_ch >= up_ch:
            # The stopped traffic overlaps this tunnel.
            tun_names.append(route_dict["tunnel_names"][index])
    # A later routine (TrafficInTunnels) applies lane counts and
    # turns them into vehicle densities.  TrafficInTunnels also
    # merges overlapping stationary traffic in different routes.
    result = ["stationary",
              PCU_dens, veh_dens, PCUperkm,
              start_ch, stop_ch,
              tun_names, np.array(traffic_flow), stat_index]
    traffic_stuff.__setitem__(route, result)
    return(traffic_stuff)


def MovingSpeeds(route, routes_dict, route_flows, traffic_data,
                 settings_dict, routes_used, traffic_stuff,
                 dict_name, all_used1, tr_allroutes, all_used2,
                 line_triples, settings, log):
    '''Take the name of a route and data for the routes and vehicles.
    Check if there is a clash between setting a route while having already
    set traffic in this route.
    Get and check the vehicle speed.  Build a list of data that is used
    in the calculation for this route and return it.

        Parameters:
            route           str             Name of a route.
            routes_dict     {}              Dictionary of the routes, for
                                            error checking.
            route_flows     [[]]            List of list of traffic flows in
                                            the routes in this block.
            traffic_data    []              List of data that sets stationary
                                            traffic in the route.
            settings_dict   {}              Dictionary of the run settings.
            routes_used     []              List of route names used in this
                                            block.
            traffic_stuff   {}              A dictionary of the traffic data
                                            set in routes.
            all_used1       Bool            If True, the "allroutes" entry
                                            has been used already.
            all_used2       Bool            If True, this call to this
                                            routine is from an "allroutes"
                                            entry.
            line_triples [(int, str, str)]  List of lines in the file.
            log             handle          The handle of the logfile.

        Returns:
            traffic_data    []              An updated dictionary with the
                                            entries from this route added.
        Errors:
            Aborts with 3021 if the user tried to use a speed higher
            than the figure allowed (currently 120 km/h).
            Aborts with 3022 if the user tried to set very slow-moving
            traffic to simulate stationary traffic (I have seen this
            done in other programs).  The error message tells the user
            to use the "standstill" keyword instead.

    '''
    debug1 = settings_dict["debug1"]
    file_name = settings_dict["file_name"]
    units = settings_dict["units"]
    vehcalc_dict = settings_dict["vehcalc_dict"]
    vehicles_dict = settings_dict["vehicles_dict"]

    # Get some data out that we need for printing.
    veh_names = list(vehcalc_dict.keys())
    maxlen = max([len(name) for name in veh_names])

    # Check if we have already set traffic in this route.
    result = TrafficClash(route, line_triples, traffic_stuff, traffic_data,
                          all_used1, tr_allroutes, all_used2, dict_name,
                          file_name, log)
    if result is None:
        return(None)

    # If we get to here, we haven't set traffic in this route before.
    discard, speed, opt_args, mov_index = traffic_data
    line_number, discard, line_text = line_triples[mov_index]

    # Check if the speed entry is a constant and fail if the check
    # returns a list.
    speed = CheckForConstant2(str(speed), "spoof_it", settings_dict,
                  line_triples, line_number, line_text, False, log)
    if speed is None:
        return(None)

    route_dict = routes_dict[route]
    tunnel_chs = route_dict["tunnel_chs"]
    # Moving traffic fills the tunnel complex from end to end,
    # as you can't have traffic appearing in the middle of a
    # tunnel.
    up_ch = tunnel_chs[0]
    down_ch = tunnel_chs[-1]

    # Check the speed value.  Complain if it is above 120 km/h unless
    # there was an extra argument on the line that permits speeding.
    # Note that we don't have to get the value in the optional
    # argument because only one value can be assigned to the option
    # argument.
    maxspeed = 120
    if abs(speed) > maxspeed and "hoons" not in opt_args:
        maxtext = str(maxspeed)
        if units == "si":
            speed_text1 = str(speed) + ' km/h).\n'
            speed_text2 = maxtext + ' km/h'
        else:
            # Get the original speed text (the speed in mph).
            speed_mph = line_text.split()[2]
            speed_text1 = speed_mph + ' mph/' + str(round(speed,1)) + ' km/h).\n'
            maxtextmph = str(round(maxspeed / 1.609344, 2))
            speed_text2 = maxtextmph + ' mph (' + maxtext + ' km/h)'
        err = ('> In the file named "' + file_name + '",\n'
               '> the "trafficsteady ' + dict_name + '" block had a\n'
               '> speed over ' + maxtext + ' km/h (it was ' + speed_text1 +
               '> Please edit the file so that the speed is at\n'
               '> or below ' + speed_text2 + ' or add the optional\n'
               '> argument "hoons := allowed" (without quotes) to\n'
               '> the line of input.'
              )
        gen.WriteError(3021, err, log)
        gen.ErrorOnLine(line_number, line_text, log, False)
        return(None)
    elif abs(speed) <= 0.1:
        # Looks like the user is trying to model stationary traffic
        # by using a low speed.  Tell them to use the "standstill"
        # keyword instead.
        if all_used2 is True:
            # This line of entry has "allroutes" for the route name.
            # Change the route name for the error message.
            route = "allroutes"
        tun_chs = routes_dict[route]["tunnel_chs"]
        err = ('> In the file named "' + file_name + '",\n'
               '> the "trafficsteady ' + dict_name + '" block had a\n'
               '> speed near zero instead of a "standstill" keyword\n'
               '> giving the density of stationary traffic.  Please\n'
               '> comment out the "moving" line of entry and add a\n'
               '> "standstill" line of entry starting with a density\n'
               '> of traffic and the units it is given in, e.g.\n'
               '>    standstill  ' + route + '  165  PCU/lane-km\n'
               '>    standstill  ' + route + '  130  veh/lane-km\n'
               '>    standstill  ' + route + '  265  PCU/lane-mile\n'
               '> or\n'
               '>    standstill  ' + route + '  209  veh/lane-mile\n'
               '> You can use other numbers if you can justify them\n'
               "> to your project's reviewers: 165 PCU/lane-km\n"
               "> is just the value recommended by PIARC.\n"
               '> Note that the rest of the line must include the\n'\
               '> route chainage that traffic starts at and the\n'
               '> route that chainage traffic stops at, e.g.\n'
               '>    standstill  ' + route + '  165  PCU/lane-km  '
                 + '  ' + gen.RoundText(up_ch, 0) +
                 '  ' + gen.RoundText(down_ch, 0)
              )
        if units == "us":
            err = err + '\n'\
               '> Note that all four of the above can be used in\n'  \
               '> files in US units.'
        gen.WriteError(3022, err, log)
        gen.ErrorOnLine(line_number, line_text, log, False)
        return(None)

    # Get the traffic for this route.
    route_index = routes_used.index(route)
    traffic_flow = route_flows[route_index]

    # Write the information to the screen and to the logfile.
    mess = ('', 'Route "' + route + '" has '
             + gen.FloatText(speed)
             + ' km/h traffic in it as follows:')
    # Write it to the screen and logfile.
    for line in mess:
        gen.WriteMessage2(line, log)
    tot_veh = 0
    tot_PCU = 0
    # print("entering loop", traffic_flow)
    for veh_index, flow in enumerate(traffic_flow):
        veh_name = veh_names[veh_index]
        tot_veh += flow
        # print(veh_name, tot_veh, flow, veh_index)
        # Now figure out what the PCU value is for this
        # vehicle type at this speed.
        PCU_vals, PCU_speeds = vehcalc_dict[veh_name][3]
        PCU_val = PCU_vals[0]
        count = len(PCU_vals)
        if count > 1:
            # This type of vehicle has more than one PCU
            # value, so we have to figure out which one to
            # use depending on traffic speed.  We use the
            # PCU value if the PCU speed is at or below
            # given value.  We start from the last value
            # in the list (infinity) and count down.
            for index in range(count-1, -1, -1):
                if PCU_speeds[index] <= abs(speed):
                    # The PCU value for speeds above this
                    # speed is the one we use.  The last
                    # value in PCU_speeds is infinity, so
                    # this works.
                    break
                else:
                    PCU_val = PCU_vals[index]

        tot_PCU += flow * PCU_val
        # Print the vehicle flowrate and, if the PCU value
        # is not 1, the PCU value too.
        if math.isclose(PCU_val, 1.0):
            mess = ("   " + veh_names[veh_index].rjust(maxlen)
                      + ': ' + gen.FloatText(flow).rjust(7)
                      + " veh/hr")
        else:
            mess = ("   " + veh_names[veh_index].rjust(maxlen)
                     + ': ' + gen.FloatText(flow).rjust(7)
                     + " veh/hr (using "
                     + gen.FloatText(PCU_val) + " PCU/veh at "
                     + gen.FloatText(speed) + " km/h)")
        gen.WriteMessage2(mess, log)
    mess = ("  Total vehicle flowrate: "
             + gen.FloatText(tot_veh).rjust(7) + " veh/hr")
    gen.WriteMessage2(mess, log)
    mess = ("  This is equivalent to:  "
             + gen.FloatText(tot_PCU).rjust(7) + " PCU/hr")
    gen.WriteMessage2(mess, log)

    # Print the extents of the tunnels and traffic in the route.
    # This is a bit redundant because the moving traffic has to
    # fill the tunnel complex from end to end (what goes in
    # must come out) but it's always sensible to give users
    # as much information as possible.
    PrintExtents(up_ch, down_ch, up_ch, down_ch, log)

    # Make a tuple of the type of traffic, speed, traffic
    # flows and tr_index that we store in traffic_stuff.
    # A later routine (TrafficInTunnels) applies speed
    # limits (if necessary) and turns them into vehicle
    # densities at those speeds.
    result = ["moving", speed, up_ch, down_ch,
              np.array(traffic_flow), mov_index]
    traffic_stuff.__setitem__(route, result)
    return(traffic_stuff)


def TrafficClash(route, line_triples, traffic_stuff, traffic_data,
                 all_used1, tr_allroutes, all_used2, dict_name,
                 file_name, log):
    # Get the keyword that has been used with the "allroutes"
    # keyword.  It will either be "moving" or "standstill".
    if all_used1 is True:
        # Get the line numbers of the entries that set traffic in the
        # the routes for this error message.
        line_number2, line_text2, line_text2 = line_triples[tr_allroutes]
        line_number, discard, line_text = line_triples[traffic_data[-1]]
        if all_used2 is True:
            # We have used "moving allroutes" twice, "standstill allroutes"
            # twice or "standstill allroutes" and "moving allroutes".
            err = ('> In the file named "' + file_name + '",\n'
                   '> the trafficsteady "' + dict_name
                     + '" block had\n'
                   '> two entries setting traffic in all the routes in\n'
                   '> the block.  Please edit the file to resolve the\n'
                   '> the clash by removing one of the entries.'
                  )
            gen.WriteError(3001, err, log)
            gen.ErrorOnTwoLines(line_number2, line_text2,
                                line_number, line_text, log)
            return(None)
        else:
            # We have already set traffic in all routes, we can't set
            # traffic in individual routes too.  First figure out if
            # the "allroutes" entry set moving or stationary traffic.
            keyword = line_text2.split()[0]
            err = ('> In the file named "' + file_name + '",\n'
                   '> the trafficsteady "' + dict_name
                     + '" block had\n'
                   '> an entry setting traffic in all routes and an\n'
                   '> entry setting traffic in route ' + route + '.\n'
                   '> If you a have "' + keyword + ' allroutes" entry you\n'
                   "> can't set traffic in all routes and then set\n"
                   '> traffic routes.  Please edit the file to resolve\n'
                   '> the clash by removing one of the entries.'
                  )
            gen.WriteError(3002, err, log)
            gen.ErrorOnTwoLines(line_number2, line_text2,
                                line_number, line_text, log)
            return(None)
    # Check if we have already set traffic in this route in this
    # block and complain if we have.
    if route in traffic_stuff:
        # Get the line numbers of the entries that set traffic in the
        # the routes for this error message.
        old_result = traffic_stuff[route]
        line_number2, discard, line_text2 = line_triples[old_result[-1]]
        line_number, discard, line_text = line_triples[traffic_data[-1]]
        # Figure out whether each entry sets moving or stationary
        # traffic and set the line of text.
        if (old_result[0] == "moving" and len(traffic_data) == 4) or  \
           (old_result[0] == "stationary" and len(traffic_data) != 4):
            err = ('> In the file named "' + file_name + '",\n'
                   '> the trafficsteady "' + dict_name
                     + '" block had\n'
                   '> two entries setting ' + old_result[0]
                     + " traffic twice\n"
                   '> in the same route (' + route + ').\n'
                   '> Please edit the file to resolve the clash\n'
                   '> by removing one of the entries.'
                  )
            gen.WriteError(3003, err, log)
            gen.ErrorOnTwoLines(line_number2, line_text2,
                                line_number, line_text, log)
            return(None)
        else:
            err = ('> In the file named "' + file_name + '",\n'
                   '> the trafficsteady "' + dict_name
                     + '" block had\n'
                   '> two entries setting stationary and moving\n'
                   '> traffic in the same route (' + route + ').\n'
                   '> Please edit the file to resolve the clash\n'
                   '> by removing one of the entries.'
                  )
            gen.WriteError(3004, err, log)
            gen.ErrorOnTwoLines(line_number2, line_text2,
                                line_number, line_text, log)
            return(None)
    return("all is well")


def PrintExtents(up_ptl, down_ptl, start_ch, stop_ch, log):
    '''Take four chainages and the handle of the log file.  Print
    them to the screen and the log file.  This is in a routine of
    its own because it is called from two locations.

        Parameters:
            up_ptl          float           Chainage of the up portal
                                            of the first tunnel in the
                                            route.
            down_ptl        float           Chainage of the down portal
                                            of the last tunnel in the
                                            route.
            start_ch        float           Chainage that the traffic
                                            starts at.
            stop_ch         float           Chainage that the traffic
                                            stops at.
            log             handle          The handle of the logfile.

        Returns:
            Nothing useful.
    '''
    mess = ('  * The tunnels start at chainage '
             + gen.RoundText(up_ptl, 3) + ' and stop at chainage '
             + gen.RoundText(down_ptl, 3) + '.')
    gen.WriteMessage2(mess, log)

    # Print the extents of the traffic in the route.
    mess = ('  * The vehicles start at chainage '
             + gen.RoundText(start_ch, 3) + ' and stop at chainage '
             + gen.RoundText(stop_ch, 3) + '.')
    gen.WriteMessage2(mess, log)
    # Add an extra line if the traffic is not inside the tunnel
    # complex.
    if stop_ch <= up_ptl or start_ch >= down_ptl:
        mess = ' * None of the vehicles are inside the tunnel.'
        gen.WriteMessage2(mess, log)
    return("Written")


def TrafficInTunnels(line_triples, settings_dict, routes_dict,
                     route_indices, r_traffic_dict, log):
    '''Take the traffic data in routes.  Sort out conflicts between
    traffic in routes that are in the same trafficsteady block.
    All the routes in a given trafficsteady block share the same
    set of road lanes, so the following need to be checked:
     * No moving traffic in one route while there's stationary traffic
       in another (can't have that when they share the same lanes).
     * speed limits in moving traffic match.
     * lane counts in the different routes along the tunnels match.
     * densities of stationary traffic are close enough to be
       considered the same.

    We complain with detailed error messages if a rule is breached.
    Lots of detail is necessary because the fixes can become very
    confusing when the route chainages are mismatched (this is
    surprisingly easy).

    Note that none of the abpve restrictions apply between routes in
    different "trafficsteady" blocks.  That's the reason for having
    "trafficsteady" blocks in the first place: to put traffic in
    many routes through a shared set of lanes (and catch mismatches
    in them).  Different traffic (stationary in one set of lanes
    vs. moving in another set, eastbound in one set of lanes vs.
    westbound in another set) is handled by using multiple
    "trafficsteady" blocks.

    If everything in the routes in each trafficsteady block matches up,
    generate a set of dictionaries with the names of the tunnels as
    keys and the traffic in each tunnel as a list of sub-lists.
    Each sub-list has the following:
      * Distance along the tunnel that a block of traffic data starts at.
      * Distance along the tunnel that a block of traffic data stops at.
      * Speed of traffic in the block (km/h).
      * Density of each type of traffic in the block (veh/km).  If
        a type of traffic has zero density, the sub-list is removed.
      * Gradient of the block (fraction -1 to +1).  Accounts for the
        direction of travel of this vehicle type.
      * Index of the vehicle type in vehcalc_dict.
    In each block along the tunnel, the densities, speeds and
    gradients are constant.  The densities are in a list that is in
    the same order as the entries in the "traffictypes" block.

        Parameters:
            line_triples [(int, str, str)]  List of lines in the file.
            settings_dict   {}              Dictionary of the run settings.
            routes_dict     {}              Dictionary of the routes, for
                                            error checking.
            route_indices   {}              Dictionary of the indices of
                                            the blocks that have put
                                            traffic into routes.  The keys
                                            are the names of routes, the
                                            result are values of tr_index.
            r_traffic_dict  {}              Dictionary of the traffic blocks,
                                            indexed by line number.
            log             handle          The handle of the logfile.

        Returns:
            t_traffic_dict  {}              Dictionary defining the traffic
                                            densities in each tunnel that
                                            has traffic in it (as opposed
                                            to in routes).

        Errors:
            Aborts with 2841 if a trafficsteady block puts moving and
            stationary traffic into the same tunnel.  This is not allowed
            because the routes in a trafficsteady block share the same
            set of lanes.
    '''
    debug1 = settings_dict["debug1"]
    file_name = settings_dict["file_name"]
    units = settings_dict["units"]
    vehcalc_dict = settings_dict["vehcalc_dict"]
    vehicles_dict = settings_dict["vehicles_dict"]

    # It's useful to calculate how much of each tunnel is occupied by
    # traffic and the traffic speed here, so that we can pass a bunch
    # of tunnel-centric data (tunnel distances) instead of route-centric
    # data (route chainages) to the calculation routines.  We make a
    # dictionary to hold this data.
    # The keys are the route names and the results are a list of tuples
    # (one tuple for each tunnel in the route).  They give the distance
    # along the tunnel at which traffic starts, ditto for where traffic
    # stops and the traffic speed in m/s, adjusted such that +ve values
    # of speed means traffic goes from back end to forward end.
    # The two distances only really matter for stationary traffic, as
    # the queue of traffic can start or stop partway along a tunnel.
    # Moving traffic has to go from portal to portal, so the distances
    # are just the distances at the back end and forward end of the
    # tunnel.  But it's easier to include them because it may make it
    # easier to write the Fortran code that will probably be processing
    # the data.
    t_traffic_dict = {}

    # Make a list of all the tunnels in routes that have traffic in them.
    # and dictionaries that identifies which traffic1 blocks pass through
    # which tunnels.  Tunnels in routes that do not have traffic in them
    # are not added to the list.
    tunnels_used = []
    tuns2routes = {}
    tuns2traffic1 = {}
    for tr_index in r_traffic_dict:
        traffic1_dict = r_traffic_dict[tr_index]
        # Get out the keys in the route traffic dictionary that point
        # to the routes that contain traffic.  This list comprehension
        # gets the names of the routes.
        route_keys = [key for key in traffic1_dict if key[:6] == "#flow_"]

        for r_key in route_keys:
            route_name = r_key[6:]
            state = traffic1_dict[r_key]
            route_dict = routes_dict[route_name]
            tunnel_names = route_dict["tunnel_names"]
            for ind, tun_name in enumerate(tunnel_names):
                # Check if this is stationary traffic.  If it is,
                # check if the block of traffic extends into this
                # tunnel.
                traf_type = state[0]
                if (traf_type == "moving") or   \
                   (traf_type == "stationary" and tun_name in state[6]):
                    if tun_name not in tunnels_used:
                        tunnels_used.append(tun_name)
                    if tun_name in tuns2traffic1:
                        others = tuns2traffic1[tun_name]
                        if tr_index not in others:
                            others.append(tr_index)
                        tuns2traffic1.__setitem__(tun_name, others)
                    else:
                        tuns2traffic1.__setitem__(tun_name, [tr_index])
                    if tun_name in tuns2routes:
                        others = tuns2routes[tun_name]
                        if route_name not in others:
                            others.append(route_name)
                        tuns2routes.__setitem__(tun_name, others)
                    else:
                        tuns2routes.__setitem__(tun_name, [route_name])
    # Sort the tunnel names so that the entries in the log file are
    # in a suitable order.
    tunnels_used.sort()
    # print("tunnels_used", tunnels_used)
    # print("tuns2traffic1", tuns2traffic1)
    # print("tuns2routes", tuns2routes)
    # print("")

    # Now loop over all the tunnels and figure out what traffic each
    # route contributes to each tunnel.
    # In stationary traffic, we fault if an overlap with moving
    # traffic occurs from traffic in the same trafficsteady block.
    # We also apply the lane counts to the traffic  densities, turning
    # them from veh/lane-km (or PCU/lane-km) to veh/km.
    # In moving traffic, we apply speed limits from the routes and turn
    # them into veh/km.
    # Make a dictionary that we populate with lines describing what
    # routes put what traffic in this tunnel.  We use this to set
    # the correct traffic mix for stationary traffic when multiple down
    # routes (or multiple up routes) pass through the same tunnel and
    # to catch cases where both moving and stationary traffic are put
    # in the same direction.
    #
    contents = {}

    for tun_name in tunnels_used:
        mess = 'Tunnel "' + tun_name + '" has traffic:'
        gen.WriteMessage2(mess, log)
        # Make lists to hold the specifications of moving and stationary
        # traffic for this tunnel.
        moving = []
        stationary = []

        # Make a list of lists of traffic for the calculation.  Each
        # sub-list holds the traffic data in the following form:
        #  * Back distance of a traffic block
        #  * Forward distance of a traffic block
        #  * Vehicle speed in m/s
        #  * Density (veh/km) for the first traffic type.
        #  * Density (veh/km) for the 2nd traffic type.
        #  ...
        #  * Density (veh/km) for the last traffic type.
        calc_densities = []

        for tr_index in tuns2traffic1[tun_name]:
            # Make lists for moving and stationary traffic in this
            # "trafficsteady" block.  If we have moving and stationary
            # traffic in the same tunnel, we raise a fault.  Note that
            # this only applies to routes in the same "trafficsteady"
            # block: if traffic is put into a tunnel in the same route
            # in different "trafficsteady" blocks, that is OK (so that
            # we can model bidirectional tunnels).
            moving_loc = []
            stationary_loc = []
            traffic1_dict = r_traffic_dict[tr_index]
            dict_name, UC_name, tr_index = traffic1_dict["block_index"]

            route_keys = [key for key in traffic1_dict if key[:6] == "#flow_"]

            for r_key in route_keys:
                route_name = r_key[6:]
                if route_name not in tuns2routes[tun_name]:
                    # This is a route in the block that does not
                    # pass through the current tunnel.  We don't need
                    # to write anything, the user shouldn't expect
                    # to have traffic from this route in this tunnel.
                    pass
                else:
                    # Get out the traffic data, depending on whether it
                    # is moving or stationary.
                    state = traffic1_dict[r_key]
                    # Put the name of the trafficsteady block into the
                    # state, so we can use it in information messages
                    # later.
                    state.insert(-1, dict_name)
                    if state[0] == "stationary":
                        # Check if the stationary traffic extends into
                        # this tunnel (it may not).
                        if tun_name in state[6]:
                            # mess =('  It has stationary traffic from route "'
                            #        + route_name + '" in block "'
                            #        + dict_name + '".')
                            # gen.WriteMessage2(mess, log)
                            stationary_loc.append((route_name, state))
                    else:
                        # It is moving traffic, so it must go through
                        # this tunnel.
                        mess =('  It has moving traffic from route "'
                               + route_name + '" in block "'
                               + dict_name + '".')
                        gen.WriteMessage2(mess, log)
                        moving_loc.append((route_name, state))
                if moving_loc != []:
                    moving.extend(moving_loc)
                if stationary_loc != []:
                    stationary.extend(stationary_loc)

            # Check for conflicts with stationary and moving traffic
            # in this "trafficsteady" block.
            if moving_loc != [] and stationary_loc != []:
                # We have moving and stationary traffic in the same
                # tunnel from the same "trafficsteady" block.
                # Get the data for the line at the start of the block.
                line_number, discard, line_text = line_triples[tr_index]
                if len(stationary_loc) == 1:
                    text1 = '>  * The following route puts stationary\n' \
                            '>    traffic in the tunnel: ' \
                            + stationary_loc[0][0]
                else:
                    stationaries = [entry[0] for entry in stationary_loc]
                    text1 = '>  * The following routes put stationary\n' \
                            '>    traffic in the tunnel:' \
                            + gen.FormatOnLines(stationaries)
                if len(moving_loc) == 1:
                    text2 = '>  * The following route puts moving traffic\n' \
                            '>    in the tunnel: ' + moving_loc[0][0]
                else:
                    movers = [entry[0] for entry in moving_loc]
                    text2 = '>  * The following routes put moving traffic\n' \
                            '>    in the tunnel:' + gen.FormatOnLines(movers)
                err = ('> In the file named "' + file_name + '",\n'
                       '> the trafficsteady block named "' + dict_name + '"\n'
                       '> sets moving traffic and stationary traffic in\n'
                       '> the same tunnel, "' + tun_name +
                         '".\n'
                       '> This is not allowed when traffic is put into\n'
                       '> routes in the same "traffictypes" block, as\n'
                       '> the routes share the same lanes.\n'
                       '> If you need to put both stationary and moving\n'
                       '> traffic into two routes that pass through the\n'
                       '> same tunnel, put them into different blocks,\n'
                       '> as the program then assumes that their lanes\n'
                       '> are segregated, much like the eastbound and\n'
                       '> westbound lanes are segregated in single-tube\n'
                       '> bidirectional tunnels.\n'
                       '> Please edit the file to correct the clash.\n'
                       '> In trafficsteady block "' + dict_name + '":\n'
                         + text1 + '\n' + text2
                      )
                gen.WriteError(2841, err, log)
                gen.ErrorOnLine(line_number, line_text, log, False, True,
                                "Relevant")
                return(None)
            elif len(stationary_loc) > 0:
                # At least one route in this trafficsteady block puts
                # stationary traffic into this tunnel.  Even if the blocks
                # don't overlap, we want them to have the same value of
                # PCU/lane-km for traffic density at rest.
                # Check for conflicting stationary traffic densities
                # and conflicting lane counts in the routes block.
                result = DoRoutesMatch(settings_dict, routes_dict,
                                       tun_name, dict_name, stationary_loc,
                                       line_triples, log)
                if result is None:
                    return(None)
                # Store the lists of distances, speed (zero) and traffic
                # densities for this tunnel.
                calc_densities.extend(result)
            elif len(moving_loc) > 0:
                # More than one route in this trafficsteady block puts
                # moving traffic into this tunnel.
                # Check for conflicting traffic speeds and conflicting
                # speed limits in the routes.
                # Different vehicle types may have different speed
                # limits due to restrictions on speeds on steep
                # upgrades.  We need to check each vehicle type
                # and generate a set of distances, speeds and
                # densities for each vehicle type.

                for veh_type in vehcalc_dict:
                    result = DoSpeedsMatch(settings_dict, routes_dict,
                                           tun_name, dict_name, veh_type,
                                           moving_loc, line_triples, log)
                    if result is None:
                        return(None)
                    # Store the list of distances, speeds and traffic
                    # densities for this traffic type in this tunnel.
                    calc_densities.extend(result)
            # If we get to here, we can add the traffic from this
            # "trafficsteady" block to the lists of traffic passing
            # through this tunnel.

        # Go through the traffic definitions and swap the traffic start
        # and stop distances if the start distance is above the stop
        # distance.
        for line in calc_densities:
            if line[0] > line[1]:
                line[0], line[1] = line[1], line[0]
        # Add this set of densities to the dictionary, but only if
        # it has vehicles in it (the user could have set zero values
        # of traffic flow for all vehicle types).
        if calc_densities != []:
            contents.__setitem__(tun_name, calc_densities)
    return(contents)


def DoSpeedsMatch(settings_dict, routes_dict, tun_name, dict_name,
                  veh_type, moving_loc, line_triples, log):
    '''Take a list of entries that set moving traffic in a tunnel.
    Check that the speeds of traffic match and that the speed limits
    along the length of the tunnel match.  Print an informative
    error message if the speed limits don't match (as those can be
    tricky to figure out).

    If all is OK, build lists of the traffic densities and speeds
    (accounting for speed limits in the tunnels) in a form that is
    suitable for merging with the lists of stationary traffic.

        Parameters:
            settings_dict   {}              Dictionary of the run settings.
            routes_dict     {}              Dictionary of the routes, for
                                            error checking.
            tun_name        str             Name of the tunnel that traffic
                                            is being put into.
            dict_name       str             Name of the trafficsteady block.
            moving_loc      []              A list of lists from lines that
                                            set moving traffic in a
                                            trafficsteady block.
            line_triples [(int, str, str)]  List of lines in the file.
            log             handle          The handle of the logfile.

        Returns:
            calc_dens        [[]]           A list of sub-lists.  Each
                                            sub-list has two distances
                                            setting the extent of the
                                            traffic in this part of the
                                            tunnel followed by a vehicle
                                            speed (km/h) and vehicle
                                            density (veh/km) for this
                                            vehicle type.  If a vehicle
                                            type has zero density, its
                                            sub-list is excluded from
                                            the list.



        Errors:
            Aborts with 2881 if the same trafficsteady block put
            moving traffic at different speeds into the tunnel.
            Aborts with 2882 if two routes pass through the same
            tunnel but the speed limits in the tunnel are different.
            Aborts with 2883 if one route has speed limits in a tunnel
            and another route does not have a matching set of speed
            limits (or no speed limits).  This error can be raised
            by route speed limits or by vehicles being unable to
            climb differing gradients in routes.
            Aborts with 2884 if neither route has speed limits, one
            route has gradients that cause certain vehicle types to
            slow down, and the other does not have gradients.
    '''
    vehcalc_dict = settings_dict["vehcalc_dict"]
    b_route_name, base_state = moving_loc[0]
    flow_index = list(vehcalc_dict.keys()).index(veh_type)
    base_spd = base_state[1]
    totflow = base_state[4][flow_index]
    file_name = settings_dict["file_name"]
    b_route = routes_dict[b_route_name]

    # Get the details of the speed limits along the base route.  Note
    # that if there was no block of speed limits in the route definition
    # and no vehicles that had to slow on steep upgrades, the desired
    # traffic speed will be used all the way through the tunnel.
    # Each speed limit could be from the speed limits in the route or
    # the vehicle type's inability to climb a particular gradient.
    # So the source of each speed limit is given in a list of sources.
    (base_dists, base_chs,
     base_spds, base_grads,
     base_sources) = SpeedsInTunnel(b_route, tun_name, vehcalc_dict,
                                    veh_type, base_spd, log)
    # Get the conversion factors from chainage to distance in this
    # tunnel.
    b_index = b_route["tunnel_names"].index(tun_name)
    b_mult, b_offset = b_route["route2tun"][b_index]

    # Iterate over the other routes in the block and fault if their
    # base speed or speed limits differ from the ones in the first
    # route.
    for route_name, state in moving_loc[1:]:
        # Check the speeds.  These vehicles share the same lanes,
        # so they can't be told to move at different base speeds.
        if state[1] != base_spd:
            index_1 = base_state[-1]
            index_2 = state[-1]
            line_num1, discard, line_text1 = line_triples[index_1]
            line_num2, discard, line_text2 = line_triples[index_2]
            err = ('> In the file named "' + file_name + '",\n'
                   '> the trafficsteady block named "'
                     + dict_name + '"\n'
                   '> had two routes that set moving traffic in\n'
                   '> the same tunnel ("' + tun_name +'").\n'
                   '> The "moving" keywords gave different values\n'
                   '> for the traffic speed, which is not allowed\n'
                   '> when they are in the same "traffictypes"\n'
                   '> block.  If you need to put traffic flows at\n'
                   '> different speeds in the same tunnel (this\n'
                   '> only happens when the traffic flows are in\n'
                   '> segregated lanes), them put the traffic into\n'
                   '> different "trafficsteady" blocks.\n'
                   '> Please edit the file to correct the clash.'
                  )
            gen.WriteError(2881, err, log)
            gen.ErrorOnTwoLines(line_num1, line_text1,
                                line_num2, line_text2, log, False)
            return(None)
        # Now build a combined list of the distances along the
        # tunnel for this vehicle type.
        route_dict = routes_dict[route_name]
        (dists, chs, spds,
            grads, sources) = SpeedsInTunnel(route_dict, tun_name,
                                             vehcalc_dict, veh_type,
                                             base_spd, log)
        all_dists = []
        for dist in base_dists + dists:
            rdist = round(dist, 2)
            if rdist not in all_dists:
                all_dists.append(rdist)
        all_dists.sort()
        # Now turn these into chainages in the two routes.
        t_index = route_dict["tunnel_names"].index(tun_name)
        mult, offset = route_dict["route2tun"][t_index]
        # Get the counts of lanes midway between each pair of chainages
        # in the two lists.  Each time we find a mismatch, we add the
        # chainages up the routes and the speed limits for the error
        # message.
        fails = []
        for index, dist1 in enumerate(all_dists[:-1]):
            dist2 = all_dists[index + 1]
            dist = (dist1 + dist2) / 2
            ch1 = (dist - b_offset) / b_mult
            ch2 = (dist - offset) / mult
            spd1 = PickStepValue(base_chs, base_spds, ch1, log)
            spd2 = PickStepValue(chs, spds, ch2, log)
            prevch1 = PickStepValue(base_chs, base_chs, ch1, log)
            prevch2 = PickStepValue(chs, chs, ch2, log)
            # print("speed check", prevch1, spd1, prevch2, spd2)
            if spd1 != spd2:
                fails.append((prevch1, spd1, prevch2, spd2))

        if len(fails) != 0:
            # One of the two routes set different speed limits in this
            # tunnel.
            ch1, spd1, ch2, spd2 = fails[0]
            # Get the lines of input to use in the error message.  If
            # a route has speed limits, the result of a call to the
            # SpeedLimitsErr routine is the line number and line text
            # of the "begin speedlimits" block in that route.
            # If the route does not have speed limits the result is
            # the string "Unset".
            line1 = SpeedLimitsErr(route_dict, line_triples)
            line2 = SpeedLimitsErr(b_route, line_triples)
            if line1 != "Unset":
                # Speed limits are implemented in the current route.
                # Complain about changing from spd1 to spd2 at ch2
                # in the current route.
                route1 = route_name
                route2 = b_route_name
                ch3 = ch2
                dist = ch2 * mult + offset
                ch4 = (dist - b_offset) * b_mult
                spd3 = spd2
                spd4 = spd1
                index_1 = route_dict["speedlimits"][-1]
                line_num1, line_text1 = line1
                if line2 == "Unset":
                    # Get the start of the route block as the second line.
                    index = b_route["block_index"][-1]
                    line_num2, discard, line_text2 = line_triples[index]
                else:
                    # Get the start of the "speedlimits" block as the
                    # second line.
                    line_num2, line_text2 = line2

            elif line2 != "Unset":
                # The current route does not have speed limits, but
                # the base route does.
                # Complain about changing from spd2 to spd1 at ch1
                # in the base route.
                route1 = b_route_name
                route2 = route_name
                ch3 = ch1
                dist = ch1 * b_mult + b_offset
                ch4 = (dist - offset) * mult
                spd3 = spd1
                spd4 = spd2
                index_1 = b_route["profile"][-1]
                line_num1, line_text1 = line2
                if line1 == "Unset":
                    # Get the start of the route block as the second line.
                    index = route_dict["profile"][-1]
                    line_num2, discard, line_text2 = line_triples[index]
                else:
                    # Get the start of the "speedlimits" block as the
                    # second line.
                    line_num2, line_text2 = line1
            else:
                # Neither route has speed limits.  The difference in
                # speed can only be due to differing gradients in the
                # routes and a vehicle type slowing down on one of
                # them.
                route1 = b_route_name
                route2 = route_name
                ch3 = ch1
                dist = ch1 * b_mult + b_offset
                ch4 = (dist - offset) * mult
                spd3 = spd1
                spd4 = spd2
                line_num1, line_text1 = ProfileLine(b_route, "profile",
                                                   line_triples)
                line_num2, line_text2 = ProfileLine(route_dict, "profile",
                                                   line_triples)

            # Check if any of the speed limits are set by vehicles
            # slowing down on upgrades and if so, generate a block
            # of error text discussing it.  Also get the line numbers
            # and text on the "begin gradients" or "begin elevations"
            # lines, if present.

            if "gradient" in base_sources + sources:
                text1 = '> The mismatches may spring from speed limits\n'  \
                        '> set in the routes or speed limits imposed by\n' \
                        '> the inability of the "' + veh_type  \
                          + '" vehicle type to\n'  \
                        '> climb steep upgrades.\n'
                line_num3, line_text3 = ProfileLine(b_route, "profile",
                                                   line_triples)
                line_num4, line_text4 = ProfileLine(route_dict, "profile",
                                                   line_triples)
            else:
                # Put an empty paragraph in the error message and
                # set line numbers/text that won't be printed.
                text1 = ''
                line_num3, line_text3 = line_num2, line_text2
                line_num4, line_text4 = line_num2, line_text2

            # Now we choose a message depending on whether one or both
            # routes had speed limits.
            if line1 != "Unset" and line2 != "Unset":
                # Both routes have speed limits.  Write a message that
                # reflects this.
                err = ('> In the file named "' + file_name + '",\n'
                       '> the trafficsteady block named "'
                         + dict_name + '" had\n'
                       '> two routes that set moving traffic in the\n'
                       '> same tunnel ("' + tun_name
                         + '").  The two routes\n'
                       '> have mismatched speed limits in that tunnel.\n'
                         + text1 +
                       '> Mismatches are not allowed when two routes are\n'
                       '> in the same trafficsteady block; they share\n'
                       '> the same lanes, so their speed limits in the\n'
                       '> tunnel must match.\n'
                         + ShowSpeeds(b_route, tun_name, veh_type,
                                      base_state[1], base_dists, base_chs,
                                      base_spds, base_grads, base_sources)
                         + ShowSpeeds(route_dict, tun_name, veh_type,
                                      state[1], dists, chs,
                                      spds, grads, sources) +
                         # + ShowSpeeds(b_route, tun_name, base_state[1])
                         # + ShowSpeeds(route_dict, tun_name, state[1]) +
                       '> Please study the speed limit and gradient\n'
                       '> transcripts above, then edit the file to\n'
                       '> correct the clash by editing the speed\n'
                       '> limits or the gradients in one or both\n'
                       '> routes.')
                gen.WriteError(2882, err, log)
                gen.ErrorOnManyLines(line_num1, line_text1,
                                     line_num2, line_text2,
                                     line_num3, line_text3,
                                     line_num4, line_text4,
                                     log, False, True, "Relevant")
                return(None)
            elif (line1 == "Unset") != (line2 == "Unset"):
                # A note for future me about the logic test above:
                # in this context the two pairs of brackets and the
                # '!=' between them makes this a xor test on two
                # Booleans.
                # One of the routes has speed limits and the other
                # does not.  We write a suitable error message for
                # this case.
                err = ('> In the file named "' + file_name + '",\n'
                       '> the trafficsteady block named "'
                         + dict_name + '"\n'
                       '> had two routes that set moving traffic in the\n'
                       '> same tunnel ("' + tun_name
                         + '").  One route set\n'
                       '> speed limits in the tunnel and the other did\n'
                       '> not.\n'
                         + text1
                         + ShowSpeeds(b_route, tun_name, veh_type,
                                      base_state[1], base_dists, base_chs,
                                      base_spds, base_grads, base_sources)
                         + ShowSpeeds(route_dict, tun_name, veh_type,
                                      state[1], dists, chs,
                                      spds, grads, sources) +
                       '> Route "' + route1 + '" sets a speed limit of '
                         + str(spd3) + ' km/h\n'
                       '> at chainage ' + gen.RoundText(ch3, 3)
                         + ' m.  Route "' + route2 + '" wants\n'
                       '> ' + str(spd4)
                         + ' km/h along the length of the tunnel.\n'
                       '> Please edit the file to correct the clash by\n'
                       '> editing the speed limits in the route or by\n'
                       '> adding speed limits to the "' + route2
                         + '" route.')
                gen.WriteError(2883, err, log)
                gen.ErrorOnManyLines(line_num1, line_text1,
                                     line_num2, line_text2,
                                     line_num3, line_text3,
                                     line_num4, line_text4,
                                     log, False, True, "Relevant")
                return(None)
            else:
                # Neither of the routes has speed limits.  The speed
                # restriction must be due to differing gradients
                # affecting one or more vehicle types.
                err = ('> In the file named "' + file_name + '",\n'
                       '> the trafficsteady block named "'
                         + dict_name + '"\n'
                       '> had two routes that set moving traffic in the\n'
                       '> same tunnel ("' + tun_name
                         + '").  Neither route set\n'
                       '> speed limits in the tunnel, but speed limits\n'
                       '> were imposed by a vehicle type needing to\n'
                       "> slow down on one or both routes' gradients:\n"
                         + ShowSpeeds(b_route, tun_name, veh_type,
                                      base_state[1], base_dists, base_chs,
                                      base_spds, base_grads, base_sources)
                         + ShowSpeeds(route_dict, tun_name, veh_type,
                                      state[1], dists, chs,
                                      spds, grads, sources) +
                       '> Please edit the file to correct the clash by\n'
                       '> making the gradients in the routes match.')
                gen.WriteError(2884, err, log)
                gen.ErrorOnManyLines(line_num1, line_text1,
                                     line_num2, line_text2,
                                     line_num3, line_text3,
                                     line_num4, line_text4,
                                     log, False, True, "Relevant")
                return(None)
        # If we get to here, the desired speeds and the speed limits
        # match.  Add the traffic flow in this route to the array that
        # holds the total.
        totflow += state[4][flow_index]

    # If we get to here, the routes all have the same speed limits for
    # this vehicle type, whether set by a block of speed limits or
    # by gradients that this vehicle type has to slow on.

    start_texts = ["  Start distance:"]
    stop_texts = ["Stop distance:"]
    grad_texts = ["Gradients:"]
    speed_texts = ["Traffic speed:"]
    flows_texts = [veh_type + ' flow:']
    dens_texts = [veh_type + ' density:']
    settings = vehcalc_dict[veh_type]
    # All the speeds and speed limits in the routes match.
    # Make a list that holds the data the calculation needs:
    #  * The start and stop distances in each section of the tunnel,
    #  * The traffic speed in m/s,
    #  * The net traffic density (veh/km) from all the vehicles
    #    flowing in the routes.
    calc_dens = [] # A list of lists of data for the calculation.
    for index, back_dist in enumerate(base_dists[:-1]):
        fwd_dist = base_dists[index + 1]
        grad = base_grads[index + 1]
        speed = base_spds[index]
        # Build a new list of the data for this vehicle type.  First
        # two numbers are the distances at the back end and forward
        # end of the traffic block, third number is speed, fourth
        # number is the vehicle density. fifth entry is the gradient
        # (adjusted to account for direction of travel) and the
        # sixth entry is index of the vehicle type in vehcalc_dict.
        traffic = [back_dist, fwd_dist, speed, totflow/abs(speed),
                   grad, flow_index]
        if not math.isclose(totflow, 0.0):
            # This vehicle type has non-zero flow in this part of the
            # tunnel.  Check if it can be merged with the previous
            # entry.  If it can't (different vehicle type, different
            # speed, different density, different gradient) add it
            # as a separate entry. If it can, extend the previous
            # entry to the down distance of this entry.
            if len(calc_dens) != 0:
                previous = calc_dens[-1]
                if (previous[1] == traffic[0]) and previous[3:] == traffic[3:]:
                    # This is the next traffic segment in the tunnel and
                    # the vehicle types, speeds and densities match.
                    # Extend the previous entry to the new down distance
                    # instead of making a new entry
                    calc_dens[-1] = [previous[0]] + traffic[1:]
                else:
                    calc_dens.append(traffic)
            else:
                calc_dens.append(traffic)

        # Now set the data for printing in the table.
        start_texts.append(gen.RoundText(back_dist, 3))
        stop_texts.append(gen.RoundText(fwd_dist, 3))
        grad_texts.append(gen.RoundText(grad, 5))
        speed_texts.append(gen.RoundText(speed, 3))
        flows_texts.append(gen.RoundText(totflow, 3))
        dens_texts.append(gen.RoundText(traffic[3], 3))


    # Now figure out how to format the table properly.  We figure
    # out which row has the widest width in each row.  Build a
    # list of them all.
    names = ['"' + name + '"' for name, discard in moving_loc]
    mess = (' Trafficsteady block "' + base_state[-2]
             + '" has moving traffic in',
            ' tunnel "' + tun_name + '" from the following route'
             + gen.Plural(len(names)) + ':',
             gen.FormatOnLines(names, indent = 6)[2:])
    for line in mess:
        gen.WriteMessage2(line, log)

    # Add the units to the lists of printable values.
    start_texts.append('m')
    stop_texts.append('m')
    grad_texts.append('fraction')
    speed_texts.append('km/h')
    flows_texts.append("veh/hr")
    dens_texts.append("veh/km")

    # Build a line of header text for the table that makes it clear that
    # each boundary is along the length of this tunnel.
    header_texts = ["Traffic block:"]
    for count in range(1, len(start_texts) - 1):
        header_texts.append(gen.Enth(count))
    header_texts.append(' ')
    monstrosity = [header_texts, start_texts, stop_texts, grad_texts,
                   speed_texts, flows_texts, dens_texts]
    maxwidths = GetMaxWidths(monstrosity)
    for line in monstrosity:
        # Format the entries in this line according to the maximum
        # width of each entry in the column.  We don't
        printable = []
        for index, entry in enumerate(line):
            printable.append(entry.center(maxwidths[index]))
        mess = ' | '.join(printable[:-1]) + ' ' + printable[-1]
        gen.WriteMessage2(mess, log)

    # Write a message to the log file describing what's been put into
    # this portion of this tunnel.
    return(calc_dens)


def ProfileLine(route_dict, block_name, line_triples):
    '''Take a route definition, the name of a sub-block and the lines
    in the file.  If the block has been defined in the route, return
    the line number and text of the start of the sub-block.  If it
    has not been defined, return the line number and text of the
    start of the route.
    '''
    index = route_dict[block_name][-1]
    if index is math.nan:
        # There was no definition of the block in the route.
        # Use the route definition instead.
        index = route_dict["block_index"][-1]
    line_num, line_data, line_text = line_triples[index]
    return(line_num, line_text)


def SpeedLimitsErr(route_dict, line_triples):
    '''Take a route dictionary and check if it has speed limits set in
    it.  If it does, return the line number and line text to be used
    in an error message.  If it doesn't return the string "Not set"
    so that the error message can be written differently.

        Parameters:
            route_dict      {}              Dictionary of the settings in
                                            a route.
            line_triples [(int, str, str)]  List of lines in the file.

        Returns:
            result          str or (,)      The string "Not set" or a tuple
                                            of line number and text of that
                                            line.
    '''
    speedlimits = route_dict["speedlimits"]
    if speedlimits == "Not set":
        # There is not a block setting speed limits in this
        # dictionary.  Return a string to flag that.
        result = "Unset"
    else:
        # Return the line at the start of the "speedlimits" block in
        # the route, to be used in the error message.
        index = speedlimits[-1]
        line_num, discard, line_text = line_triples[index]
        result = (line_num, line_text)
    return(result)


def DoRoutesMatch(settings_dict, routes_dict, tun_name, dict_name,
                  stationary_loc, line_triples, log):
    '''Take a list of entries that set stationary traffic in a tunnel.
    Check that the densities of traffic at rest are close together
    (within 0.001 PCU/lane-km).  Then check that the lane counts in
    the two routes are the same.  If all is well, figure out the
    densities of traffic in the (possibly overlapping) blocks of
    traffic, using the density at rest and the count of lanes.

        Parameters:
            settings_dict   {}              Dictionary of the run settings.
            routes_dict     {}              Dictionary of the routes, for
                                            error checking.
            tun_name        str             Name of the tunnel that traffic
                                            is being put into.
            dict_name       str             Name of the trafficsteady block.
            stationary_loc  []              A list of lists from lines that
                                            set stationary traffic in a
                                            trafficsteady block.
            line_triples [(int, str, str)]  List of lines in the file.
            log             handle          The handle of the logfile.

        Returns:
            calc_dens       [[]]            A list of sub-lists.  Each
                                            sub list has two distances
                                            setting the extent of the
                                            traffic in this part of the
                                            tunnel, a zero (representing
                                            vehicle speed), a vehicle
                                            density in veh/km, zero for
                                            gradient and an index number
                                            representing vehicle type.
                                            If a vehicle type has zero
                                            density, its sub-list is
                                            excluded from the list.

        Errors:
            Aborts with 2861 if the same trafficsteady block put moving
            traffic at different densities into the same tunnel.
            Aborts with 2862 if two routes have different counts of
            lanes in the same tunnel.
    '''
    b_route_name, base_state = stationary_loc[0]
    base_dens = base_state[1]
    file_name = settings_dict["file_name"]
    b_route = routes_dict[b_route_name]
    # Get the details of the lane counts in the base route.
    base_dists, base_chs, base_lanes = MonosInTunnel(b_route, tun_name,
                                                     "lanes", log)
    # Get the conversion factors from chainage to distance in this
    # tunnel.
    b_index = b_route["tunnel_names"].index(tun_name)
    b_mult, b_offset = b_route["route2tun"][b_index]
    # Get the distances along the tunnel.  Each time a route adds
    # traffic to the tunnel we add the up and down chainage of the
    # block of traffic.  After we finish this loop we pick out
    # duplicates.  To make it easier to pick duplicate numbers
    # that may be affected by slight floating point mismatches
    # we round the distances to eight decimal places and compare
    # with an allowable mismatch to six digits.
    #
    boundaries = [round(dist, 8) for dist in base_dists]
    boundaries = AddTrafficEnd(boundaries, base_state, b_mult, b_offset)

    # Iterate over the other routes in the block and fault if they
    # have a different density at rest or a different count of lanes.
    for route_name, state in stationary_loc[1:]:
        # We allow differences of up to 0.005 PCU/lane-km so that we
        # don't have to have set a ridiculously large amount of decimal
        # places when setting densities in units other than PCU/lane-km.
        if not math.isclose(state[1], base_dens, abs_tol=0.005):
            index_1 = base_state[-1]
            index_2 = state[-1]
            line_num1, discard, line_text1 = line_triples[index_1]
            line_num2, discard, line_text2 = line_triples[index_2]
            # Get some optional helpful text for the two routes,
            # describing the values if one or both is not set in
            # veh/lane-km.
            text1 = DensityText("first", base_state)
            text2 = DensityText("second", state)
            err = ('> In the file named "' + file_name + '",\n'
                   '> the trafficsteady block named "'
                     + dict_name + '"\n'
                   '> had two routes that set stationary traffic in\n'
                   '> the same tunnel ("' + tun_name +'").\n'
                   '> The "standstill" keywords gave different values\n'
                   '> for the density of stationary traffic, which\n'
                   '> is not allowed when they are in the same\n'
                   '> "traffictypes" block because they share the\n'
                   '> same lanes.  If you need to put blocks of\n'
                   '> stationary traffic of different base densities\n'
                   '> in the same tunnel, put them into different\n'
                   '> "traffictypes" blocks.\n' + text1 + text2 +
                   '> As an aside, why do you think you need to have\n'
                   '> different base densities in different routes?\n'
                   '> Much safer to use one figure and play around\n'
                   '> with PCU values.\n'
                   "> Note that the densities don't need to be an\n"
                   '> exact match: a mismatch of 0.005 PCU/lane-km\n'
                   '> is accepted.\n'
                   '> Please edit the file to correct the clash.'

                  )
            gen.WriteError(2861, err, log)
            gen.ErrorOnTwoLines(line_num1, line_text1,
                                line_num2, line_text2, log, False)
            return(None)
        # The routes are in the same trafficsteady block so they
        # are assumed to share the same set of lanes.  Check if
        # they set compatible counts of lanes in this tunnel.
        # First build a combined list of the distances along the
        # tunnels.
        route_dict = routes_dict[route_name]
        dists, chs, lanes = MonosInTunnel(route_dict, tun_name, "lanes", log)
        # Build a list of all the changes of lanes in this route.  We
        # allow mismatches of 0.05 m in the chainages.
        all_dists = []
        for dist in base_dists + dists:
            rdist = round(dist, 2)
            if rdist not in all_dists:
                all_dists.append(rdist)
        all_dists.sort()
        # Now turn these into chainages in the two routes.
        # t_index = route_dict["tunnel_names"].index(tun_name)
        # mult, offset = route_dict["route2tun"][t_index]
        mult, offset = GetMultOffset(route_dict, tun_name)
        # Get the counts of lanes midway between each pair of chainages
        # in the two lists.  Each time we find a mismatch, we add the
        # chainages up the routes and the counts of lanes for the error
        # message
        fails = []
        for index, dist1 in enumerate(all_dists[:-1]):
            dist2 = all_dists[index + 1]
            dist = (dist1 + dist2) / 2
            ch1 = (dist - b_offset) / b_mult
            ch2 = (dist - offset) / mult
            lane1 = PickStepValue(base_chs, base_lanes, ch1, log)
            lane2 = PickStepValue(chs, lanes, ch2, log)
            # if b_mult != mult:
            #     # One route has the chainages going down from low
            #     # distance to high distance, the other has goes up.
            #     # We switch one pair of lane counts.
            #     lane1.reverse()
            prevch1 = PickStepValue(base_chs, base_chs, ch1, log)
            prevch2 = PickStepValue(chs, chs, ch2, log)
            if lane1 != lane2:
                fails.append((prevch1, lane1, prevch2, lane2))

        if len(fails) != 0:
            # The two routes set different counts of lanes in this
            # tunnel.  This is a problem because the two routes appear
            # in the same "trafficsteady" block (it would be fine
            # if they were in different "trafficsteady" blocks).
            # We complain about the first one, because fixing the
            # first one may correct the others.  We give a lot of
            # information in this error message because lane counts
            # in routes are confusing in cases where the tunnel is
            # reversed in one of the routes and not in the other.
            index_1 = routes_dict[route_name]["lanes"][-1]
            index_2 = routes_dict[b_route_name]["lanes"][-1]
            line_num1, discard, line_text1 = line_triples[index_1]
            line_num2, discard, line_text2 = line_triples[index_2]
            ch1, lanes1, ch2, lanes2 = fails[0]
            if ch1 >= ch2:
                # Complain about changing from lanes1 to lanes2 at ch2
                # in the current route.
                route1 = route_name
                route2 = b_route_name
                ch3 = ch2
                dist = ch2 * mult + offset
                ch4 = (dist - b_offset) * b_mult
                lanes3 = lanes2
                lanes4 = lanes1
            else:
                # Complain about changing from lanes2 to lanes1 at ch1
                # in the base route.
                route1 = b_route_name
                route2 = route_name
                ch3 = ch1
                dist = ch1 * b_mult + b_offset
                ch4 = (dist - offset) * mult
                lanes3 = lanes1
                lanes4 = lanes2
            # Get pairs of lanes on each side of the chainages.
            # Note that we overwrite existing variables "base_dists",
            # "base_chs", and "base_lanes" but that's OK because
            # we're about to stop processing.
            base_dists, base_chs, base_lanes = \
                        PropsInTunnel(b_route, tun_name, "lanes", log)
            dists, chs, lanes = PropsInTunnel(route_dict, tun_name, "lanes", log)
            err = ('> In the file named "' + file_name + '",\n'
                   '> the trafficsteady block named "'
                     + dict_name + '"\n'
                   '> had two routes that set counts of lanes in the\n'
                   '> same tunnel ("' + tun_name +'").\n'
                     + ShowLanes(b_route_name, base_chs, base_dists,
                                 base_lanes)
                     + ShowLanes(route_name, chs, dists, lanes) +
                   '> Route "' + route1 + '" sets a mismatched count of lanes\n'
                   '> at chainage ' + gen.RoundText(ch3, 3)
                     + ' m (distance ' + gen.RoundText(dist, 3)
                     + ' in the tunnel).\n'
                   '> The mismatch is ' + str(lanes3) + ' lane'
                     + gen.Plural(lanes3) + ' in "' + route1 + '" vs '
                     + str(lanes4) + ' lane' + gen.Plural(lanes4)
                     + ' in\n'
                   '> "' + route2 + '".\n'
                   '> The corresponding chainage in route "' + route2
                     + '" is\n'
                   '> ' + gen.RoundText(ch4, 3) + ' m (the error'
                     ' may be in that route instead).\n'
                   '> Please edit the file to correct the clash by\n'
                   '> editing the count of lanes or the chainages at\n'
                   '> which they change in one or both routes.'
                  )
            gen.WriteError(2862, err, log)
            gen.ErrorOnTwoLines(line_num1, line_text1, line_num2,
                                line_text2, log, False, True, "Relevant")
            return(None)
        else:
            # The changes of lanes in this route match those in the
            # base route.  Check if we need to add the chainages at
            # which the stationary traffic in this route starts and
            # stops.
            boundaries = AddTrafficEnd(boundaries, state, mult, offset)

    # Sort the list of boundary distances.
    boundaries.sort()

    # All the densities and counts of lanes in the routes match.
    # Make a list that holds the data the calculation needs:
    #  * The start and stop distances in each section of the tunnel,
    #  * Zero for the traffic speed,
    #  * The traffic densities (veh/km) in that interval for all
    #    traffic types
    # We start by putting zero values for traffic flows in all the
    # intervals we know about
    # Get the count of traffic types.
    traftypes = len(base_state[7])
    # Build a list of numpy arrays with zero for each traffic type.
    zeroflows = np.array([0.0] * traftypes)
    stationary = [ zeroflows ] * (len(boundaries) - 1)
    # Build lists of the distances at which each block starts and
    # stops, for printing.
    start_texts = ["Start distance:"]
    stop_texts = ["Stop distance:"]
    for index, back1 in enumerate(boundaries[:-1]):
        fwd1 = boundaries[index + 1]
        # loc_flowtext = []
        for route_name, state in stationary_loc:
            route_dict = routes_dict[route_name]
            mult, offset = GetMultOffset(route_dict, tun_name)
            (back2, fwd2) = TrafficEnds(state, mult, offset)

            # Check for reversed chainages and adjust for them.
            if fwd2 < back2:
                # Switch the two route chainages so that the test
                # below works.
                back2, fwd2 = fwd2, back2
            flows = state[7]
            # The test below includes math.isclose() terms because
            # >= and <= sometimes don't catch cases that they ought
            # to catch, due to different arithmetic being done on
            # back1 & back2 and fwd1 & fwd2.
            if ( (back2 < back1 or math.isclose(back1, back2)) and
                 (fwd2 > fwd1  or math.isclose(fwd1, fwd2)) ):
                # The block of stationary traffic lies in this tunnel.
                # Add them to the flowrate of vehicles.
               stationary[index] = stationary[index] + flows
        # Make a list that we can join together in the printed table.
        # route_flows.append(loc_flows)
        start_texts.append(gen.RoundText(back1, 3))
        stop_texts.append(gen.RoundText(fwd1, 3))

    # Build a line of text with the route extents expressed
    # as distances in the tunnel.  We want a block of text like:

    # >  Start distance:      11020  |  13400  |  21304     (m)
    # >  Stop distance:       13400  |  21304  |  21502     (m)
    # >  In "EBmain":         fills  |         |            (m)
    # >  In "EBbranch":              |         |            (m)
    # >  car_p flow (1 PCU):   1500  !    0    |   1920   (veh/hr)
    # >  HGV flow (3 PCU):      670  !         |    750   (veh/hr)
    # >  vehicle totals:       2170  |
    # >  mean PCU/veh         1.675  !         |  1.562  (PCU/veh)
    # >  car_p PCU-flows:      1500  !         |   1920   (PCU/hr)
    # >  HGV   PCU-flows:      2010  !         |   2250   (PCU/hr)
    # >  Vehicle totals:       3510  !         |   4170   (PCU/hr)
    # >  Lane counts:           2    |    3    |    3        (-)
    # >  car_p density:      141.03  !         | 227.91   (veh/km)
    # >  HGV density:         62.99  !         |  89.03   (veh/km)
    # >   Calculated using density 165 PCU/lane-km.

    # Make a list to hold the text of the summed-up vehicle flows for
    # each interval in the tunnel and each traffic type.  We print two
    # views of the traffic flows: vehicles per hour and PCU per hour,
    # to make it easier for the engineer to check the arithmetic.
    # Then at the end we turn them into densities in veh/km using
    # the count of lanes.

    PCU_vals = []
    flows1_texts = []
    flows2_texts = []
    dens1_texts = []
    # Get the names of the vehicle types and build the first entry
    # in the lists of flows and densities.  While we're at it, build
    # a list of the PCU values that we can use in one of the loops
    # below to calculate mean PCU values for blocks of traffic that
    # overlap one another.
    vehcalc_dict = settings_dict["vehcalc_dict"]
    for name in vehcalc_dict:
        settings = vehcalc_dict[name]
        PCUs, speeds = settings[3]
        # Get the count of PCUs per vehicle at stationary.
        PCUperveh = PickStepValue(speeds, PCUs, 0, log)
        # Build the text we want at the left side of the table: the
        # name of the vehicle type and how many PCUs each occupies at
        # rest.
        text1 = name + ' flow (' + gen.RoundText(PCUperveh, 4) + ' PCU):'
        text2 = name + ' PCU-flow:'
        text3 = name + ' density:'
        PCU_vals.append(PCUperveh)
        flows1_texts.append([text1])
        flows2_texts.append([text2])
        dens1_texts.append([text3])

    route_texts = []
    for index, (route_name, state) in enumerate(stationary_loc):
        route_dict = routes_dict[route_name]
        mult, offset = GetMultOffset(route_dict, tun_name)
        (back2, fwd2) = TrafficEnds(state, mult, offset)

        route_line = ['"' + route_name + '" traffic:']
        for index2, back_dist in enumerate(boundaries[:-1]):
            fwd_dist = boundaries[index2 + 1]
            # Add the lines of traffic
            if fwd_dist <= back2 or back_dist >= fwd2:
                # The block of traffic either hasn't started yet or
                # has already stopped.
                route_line.append(" ")
            else:
                # This block has traffic in it.
                if route_line[-1] not in ('Is here', '& here'):
                    route_line.append('Is here')
                else:
                    route_line.append('& here')
        route_texts.append(route_line)
    # Get the vehicle flows in this stretch of the tunnel.  They
    # may be percentage values or fractional values but it's
    # easier to think of them as flowrates.  In this loop we
    # get the total flowrates in vehicles/hour and PCU/hour.
    # to use to calculate the mean PCU value, get the vehicle
    # densities of each traffic type in veh/km.
    PCU_flows = [] # A list of lists
    calc_dens = [] # A list of lists of data for the calculation.
    tot_vehflows = [] # A list
    tot_PCUs = [] # A list
    mean_PCUs = [] # A list
    # Get the counts of lanes at all the boundaries in this tunnel.
    # It's easier to pick one route, get the chainages of the distance
    # at the traffic blocks and lanes, call the routine then figure
    # out whether to reverse the list depending on the route's
    # orientation in the tunnel.
    lanes = []
    for dist in boundaries[:-1]:
        # Get the lane counts on both sides of the location.  We do
        # this with chainage because my brain hurts from all the
        # conversions.
        ch = (dist - b_offset) / b_mult
        counts = BothSidesProps(base_chs, base_lanes, ch, log)
        if b_mult < 0:
            # We want the count of lanes on the up side of this
            # boundary because the distances in the tunnel go up
            # in the opposite direction to the chainage.
            lanes.append(counts[0])
        else:
            # We want the count of lanes on the down side.
            lanes.append(counts[1])

    veh_texts = ["Vehicle flow totals:"]
    PCU1_texts = ["PCU flow totals:"]
    PCU2_texts = ["Mean PCU value:"]
    for index, dist in enumerate(boundaries[:-1]):
        loc_PCU_flows = []
        lane_count = lanes[index]
        # Store the distances of the traffic in this block for
        # the calculation along with a traffic speed of 0 m/s.
        # I think it's OK to store the distances as lowest first
        # because the calculation routine reverses the distances
        # if the distances reduce from back end to forward end.
        # We append the vehicle densities (veh/km) for each
        # traffic type.
        loc_calc_dens = [dist, boundaries[index + 1], 0.0]
        for index2, veh_flow in enumerate(stationary[index]):
            PCU_flow = veh_flow * PCU_vals[index2]
            loc_PCU_flows.append(PCU_flow)
            flows1_texts[index2].append(gen.RoundText(veh_flow, 4))
            flows2_texts[index2].append(gen.RoundText(PCU_flow, 4))
        tot_veh = sum(stationary[index])
        tot_vehflows.append(tot_veh)
        tot_PCU = sum(loc_PCU_flows)
        if math.isclose(tot_veh, 0.0):
            # The user set all the flowrates to zero.  An unlikely
            # case but worth catching the divide by zero.
            mean_PCU = '-'
            factor = base_dens
        else:
            mean_PCU = tot_PCU / tot_veh
            factor = base_dens / (tot_veh * mean_PCU)
        mean_PCUs.append(mean_PCU)
        # Now that we know the mean PCU value in this block of
        # traffic get the count of lanes in it and calculate the
        # vehicle densities for each vehicle type.  We store the
        # values as a list of numbers for the calculation and
        # as a list of text for the log file.
        for index2, veh_flow in enumerate(stationary[index]):
            dens = factor * lanes[index] * veh_flow
            loc_calc_dens.append(dens)
            dens1_texts[index2].append(gen.RoundText(dens, 2))
        # Store the data that the calculation will use in this part
        # of the tunnel.  loc_calc_dens already has two distances
        # and zero traffic speed followed by vehicle densities in
        # veh/km) for each traffic type.  We don't bother storing
        # it if there are no vehicles, though.
        # Note that we give a zero for the gradient because
        # gradients don't matter when traffic is at rest.
        for index, dens in enumerate(loc_calc_dens[3:]):
            if not math.isclose(dens, 0.0):
                calc_dens.append(loc_calc_dens[:3] +
                                 [dens, 0.0, index])
        veh_texts.append(gen.RoundText(tot_veh, 4))
        PCU1_texts.append(gen.RoundText(tot_PCU, 4))
        PCU2_texts.append(gen.RoundText(mean_PCU, 3))

    # Add the units to the lists of printable values.
    start_texts.append('m')
    stop_texts.append('m')
    for texts in route_texts:
        texts.append(" ")
    for texts in flows1_texts:
        texts.append("veh/hr")
    veh_texts.append("veh/hr")
    for texts in flows2_texts:
        texts.append("PCU/hr")
    PCU1_texts.append("PCU/hr")
    PCU2_texts.append("PCU/veh")
    lanes_texts = ["Lane counts:"] + [str(count) for count in lanes] + ["lanes"]
    for texts in dens1_texts:
        texts.append("veh/km")

    # Now figure out how to format the table properly.  We figure
    # out which row has the widest width in each row.  Build a
    # list of them all.
    text1 = 'route' + gen.Plural(len(route_texts)) + ') using\n'
    mess = '>    Trafficsteady block "' + base_state[-2]  \
             + '" has stationary traffic\n'    \
           '>    in tunnel "' + tun_name   \
             + '".  Densities are calculated as\n'\
           '>    follows (for each block of lanes in the ' + text1 +  \
           '>    a base traffic density of '\
             + gen.RoundText(base_dens, 3) + ' PCU/lane-km:'
    gen.WriteMessage(mess, log)

    # Build a line of header text for the table that makes it clear that
    # each boundary is along the length of this tunnel.
    header_texts = [" Lane/traffic block:"]
    for count in range(1, len(start_texts) - 1):
        header_texts.append(gen.Enth(count))
    header_texts.append(' ')
    monstrosity = [header_texts, start_texts, stop_texts]
    monstrosity.extend(route_texts)
    monstrosity.extend(flows1_texts)
    monstrosity.append(veh_texts)
    monstrosity.append(PCU2_texts)
    monstrosity.extend(flows2_texts)
    monstrosity.append(PCU1_texts)
    monstrosity.append(lanes_texts)
    monstrosity.extend(dens1_texts)
    maxwidths = GetMaxWidths(monstrosity)
    for line in monstrosity:
        # Format the entries in this line according to the maximum
        # width of each entry in the column.
        printable = []
        for index, entry in enumerate(line):
            printable.append(entry.center(maxwidths[index]))
        mess = '> ' + ' | '.join(printable[:-1]) + ' ' + printable[-1]
        gen.WriteMessage(mess, log)

    return(calc_dens)


def SimplifyTraffic(t_traffic_dict, vehcalc_dict, debug1, log):
    '''Take a dictionary of road traffic data, remove the gradients,
    and merge adjacent blocks of similar traffic.  Return a block
    of simplified data for use in the aerodynamic calculation with
    entries for each traffic type.

    This routine also writes the traffic data to the log file so that
    engineers can check it.  If the debug1 switch is True, the
    transcripts are also written to the screen.
    '''
    calc_traffic = {}
    for name, calc_dens in t_traffic_dict.items():
        calc_dens.sort(key=operator.itemgetter(-1))
        # Build a list without the gradients.
        mod_dens1 = []
        for dens in calc_dens:
            short_dens = dens[:4] + [dens[5]]
            mod_dens1.append(short_dens)

        # Build a list that holds the (potentially merged) sub-lists.
        # We either modify the lastmost entry or add a new one,
        # depending on the density, traffic type and distances.
        # We also buid a second list of the start and stop distances of
        # traffic in the tunnel and a third list of the traffic speeds
        # in the tunnel.
        mod_dens2 = [mod_dens1[0]]
        block_ends = []
        speeds = []
        for index, dens in enumerate(mod_dens1[1:]):
            # Check if the speed, density and vehicle type are the
            # same and if the blocks of traffic are adjacent.
            prev_dens = mod_dens2[-1]
            if dens[-3:] == prev_dens[-3:]:
                # The speed, density and vehicle type all match.
                if dens[0] == prev_dens[1]:
                    # The stop distances match.
                    # Overwrite the stop distance of the last traffic
                    # block in the modified list.
                    prev_dens[1] = dens[1]
                elif dens[1] == prev_dens[0]:
                    # The start distances match.
                    # Overwrite the start distance of the last traffic
                    # block in the modified list.
                    prev_dens[0] = dens[0]
            else:
                # This entry has a different density or a different
                # vehicle type.  Add it to the list.
                mod_dens2.append(dens)
            # Now check if this entry has start and stop distances
            # we haven't seen before.
            start_dist, stop_dist, speed = dens[:3]
            if start_dist not in block_ends:
                block_ends.append(start_dist)
            if stop_dist not in block_ends:
                block_ends.append(stop_dist)
            # Now check if this entry has speeds we haven't seen yet.
            if speed not in speeds:
                speeds.append(speed)
        calc_traffic.__setitem__(name, mod_dens2)


    # Get the names of the traffic, a count of how many there are
    # and the maximum width.
    names = tuple(vehcalc_dict.keys())
    namewidth = max([len(name) for name in names])
    zeros = [0.0] * len(names)

    new_calc_traffic = {}
    for name, calc_dens in calc_traffic.items():
        # calc_dens is a list of sub-lists.  Some sub-lists will
        # have the same start distance, stop distance and speed but
        # different vehicle types.  If the start distance, stop
        # distance and speed of adjacent sub-lists are the same
        # we can build a new sub-list that has the start distance,
        # stop distance, speed and zero values for the vehicle
        # densities of all vehicle types.  Then we loop over the
        # sub-lists looking for matching distances and speeds.
        # If we find a match we add the vehicle density to the
        # appropriate entry in the new sub-list.  The advantage
        # of this approach is that when we turn the drag of vehicles
        # travelling at the same speed in segments we can combine
        # their C_d * area factor into one number, which will
        # speed up the calculations at each timestep.
        mod_dens4 = []
        for dens in calc_dens:
            short_dens = dens[:3] + zeros
            if short_dens not in mod_dens4:
                mod_dens4.append(short_dens)
        new_calc_traffic.__setitem__(name, mod_dens4)

    # Now extend the values in new_calc_traffic to include the
    # vehicle densities.
    for name, calc_dens in calc_traffic.items():
        new_lists = new_calc_traffic[name]
        for dens1 in calc_dens:
            startstopspeed = dens1[:3]
            veh_dens, veh_type = dens1[3:5]
            veh_index4 = veh_type + 3
            for index, dens2 in enumerate(new_lists):
                if dens2[:3] == startstopspeed:
                    # The distances and speeds for this traffic
                    # type match the current existing type
                    dens2[veh_index4] = veh_dens
                    new_lists[index] = dens2
        new_calc_traffic.__setitem__(name, new_lists)

    # Now write the complex data and simple data to the log file
    # for debugging purposes.

    # Build a list of headers for the entries.  First block is
    # for the full-blown data (includes gradient).  Second block
    # is for the reduced data (no gradient).
    headers1 = [[ "Start",     "Stop",      "",     "Vehicle",
                    "Gradient",    "Vehicle"],
                ["distance", "distance", "Speed",   "density",
                 "vehicles", "type"],
                ["(m)",         "(m)",   "(km/h)", "(veh/km)",
                   "are on",      ""]
               ]
    headers2 = [[ "Start",      "Stop",      "",     "Vehicle", "Vehicle"],
                ["distance",  "distance", "Speed",   "density",  "type"],
                [  "(m)",        "(m)",   "(km/h)", "(veh/km)",   ""]
               ]

    for tun_name in calc_traffic:
        gen.WriteMessage3('Road traffic in tunnel "'
                          + tun_name + '":', debug1, log)
        # Build a list of lists of the contents of t_traffic_dict
        # with the vehicle type names replacing the vehicle type
        # index as the last entry.
        detailed = []
        for traffic_block in t_traffic_dict[tun_name]:
            veh_name = list(vehcalc_dict.keys())[traffic_block[-1]]
            detailed.append(traffic_block[:-1] + [veh_name])
        # print(detailed)
        # sys.exit()
        # Get the widths so we can line up the columns.
        maxwidths1 = GetMaxWidths2(detailed + headers1, 6)
        maxwidths1.append(namewidth)
        for entries in headers1:
            strs = [entry.center(maxwidths1[index])
                            for index, entry in enumerate(entries)]
            write = "  " + ' | '.join(strs)
            gen.WriteMessage3(write, debug1, log)
        for entry in detailed:
            for value in entry[:-1]:
                strs = [gen.RoundText(num, 6).center(maxwidths1[index])
                                for index, num in enumerate(entry)]
            gen.WriteMessage3("  " + ' | '.join(strs), debug1, log)
    return(new_calc_traffic)


def GetMaxWidths(listoflists):
    '''Take a list of lists of strings that are to be printed in a
    table format, one sublist per line of entry.  Figure out the widest
    entry in each column.  Return a list of the widest widths in
    each column.

        Parameters:
            listoflists     [[]]            A list of lists of strings.
        Returns:
            maxwidths       [int]           A list of maximum widths
                                            from all the
    '''
    count = len(listoflists[0])
    maxwidths = [0] * count
    for mylist in listoflists:
        for index, entry in enumerate(mylist):
            new_width = len(entry)
            if new_width > maxwidths[index]:
                maxwidths[index] = new_width
    return(maxwidths)


def GetMaxWidths2(listoflists, width):
    '''Take a list of lists of numbers or strings that are to be printed
    in a table format, one sublist per line of entry.  Figure out the
    widest entry in each column.  Return a list of the widest widths in
    each column.

        Parameters:
            listoflists     [[]]            A list of lists of strings
                                            or numbers, or a mix of
                                            the two
        Returns:
            maxwidths       [int]           A list of maximum widths
                                            from all the
    '''
    # Make a list of zeros, then find the widest width in each column.
    maxwidths = [0] * len(listoflists[0])
    for mylist in listoflists:
        for index, entry in enumerate(mylist):
            if type(entry) in (float, np.float64):
                new_width = len(gen.RoundText(entry, width))
            elif type(entry) is str:
                new_width = len(entry)
            else:
                new_width = len(str(entry))
            if new_width > maxwidths[index]:
                maxwidths[index] = new_width
    return(maxwidths)


def DensityText(ordinal, state):
    '''Take a string that is an ordinal ("first", "second", etc.)
    and the state of a block of stationary traffic.  If the traffic
    block was set in anything other than PCU/lane-km, return a line
    that describes the setting in veh/lane-km and its equivalent in
    PCU/lane-km.  This makes it easy for a user to adjust the setting
    by applying a ratio.

        Parameters:
            ordinal         str             'first' or 'second'
        Returns:
            state           str             A string used in an error
                                            message to help out users.
    '''
    if state[3] is False:
        text = '> The ' + ordinal + ' route set density ' \
                 + gen.RoundText(state[2], 4)  \
                 + ' veh/lane-km,\n'  \
               '> equivalent to '     \
                 + gen.RoundText(state[1], 4)  \
                 + ' PCU/lane-km.\n'
    else:
        text = ""
    return(text)


def GetMultOffset(route_dict, tunnel_name):
    '''Take a route dictionary and the name of a tunnel.  Get the
    multiplier and offset used to turn chainages in the tunnel into
    distances in the tunnel.  It's been stated a lot in the code
    comments but it's worth mentioning it again:
      distance = chainage * mult + offset
      chainage = (distance / mult) - offset
    Distances apply in tunnels, chainages apply in routes.

        Parameters:
            route_dict      {}              A route definition.
            tunnel_name     str             The tunnel name.
        Returns:
            mult            float           A multiplier used to turn
                                            a chainage into a distance.
            offset          float           An offset added to a
                                            chainage to turn it into
                                            a distance.
    '''
    t_index = route_dict["tunnel_names"].index(tunnel_name)
    mult, offset = route_dict["route2tun"][t_index]
    return(mult, offset)


def TrafficEnds(state,  mult, offset):
    '''Get the back and forward end distances of a block of traffic.
    Sort them into ascending order and return them.

        Parameters:
            state           [float]         A list of entries that
                                            define a block of traffic.
            mult            float           A multiplier used to turn
                                            a chainage into a distance.
            offset          float           An offset added to a
                                            chainage to turn it into
                                            a distance.
        Returns:
            back_dist       float           The distance in the tunnel
                                            of the back end of a block
                                            of traffic.
            fwd_dist        float           The distance in the tunnel
                                            of the forward end of a
                                            block of traffic.
    '''
    back_dist =  state[4] * mult + offset
    fwd_dist =  state[5] * mult + offset
    if fwd_dist < back_dist:
        # Sort the distances in ascending order.
        back_dist, fwd_dist = fwd_dist, back_dist

    return(back_dist, fwd_dist)




def AddTrafficEnd(boundaries, state,  mult, offset):
    '''Take a list of the distances in a tunnel.  These are of three
    types:
      * The distances at the back end and forward end of the tunnel
        (based on the conversion of the chainages in the first route).
      * The distances at which lanes change.
      * The distances at which traffic starts or stops.
    This routine adds the chainage of the ends of a block of traffic
    (rounded to 8 decimal places) if that distance is not already in
    the list.

        Parameters:
            boundaries      [float]         A list of distances in a
                                            tunnel.
            state           [float]         A list of entries that
                                            define a block of traffic.
            mult            float           A multiplier used to turn
                                            a chainage into a distance.
            offset          float           An offset added to a
                                            chainage to turn it into
                                            a distance.
        Returns:
            boundaries      [float]         A modified list of distances.

    '''
    # Get the back and forward end distances of this tunnel in the
    # route.  We need to choose depending on whether the up end
    # of the tunnel in the route is the back end or the forward end.
    if boundaries[0] < boundaries[-1]:
        # The back end of this tunnel is at the up end in the route.
        start = boundaries[0]
        stop = boundaries[-1]
    else:
        # The tunnel is reversed in the route.
        start = boundaries[-1]
        stop = boundaries[0]
    for index, ch in enumerate(state[4:6]):
        candidate = round(ch * mult + offset, 8)
        not_found = True
        for dist in boundaries:
            if math.isclose(candidate, dist, abs_tol = 1e-7):
                not_found = False
        # Check if the traffic starts or stops outside the tunnel.
        if candidate < start or candidate > stop:
            # This is a new distance, but it lies outside the
            # extents of the tunnel.
            pass
        elif not_found:
            # Add the distance to the list, but not at the end
            # so that the test above continues to work.
            boundaries.append(candidate)
    boundaries.sort()
    return(boundaries)


def ShowLanes(route_name, chs, dists, lanes):
    '''Build lines that we use in a few different error messages with
    different lane counts in different routes.  It shows a formatted
    list of the chainages, distances along the tunnels and the count
    of lanes.

        Parameters:
            route_name      str             Name of a route.
            chs             [float]         A list of chainages in the
                                            route that are in one tunnel.
            dists           [float]         A list of distances in the
                                            tunnel that correlate to the
                                            route chainages above.
            lanes           [int]           A list of pairs of counts
                                            of lanes on each side of
                                            the chainages.

        Returns:
            mess            [str]           A list of strings giving the
                                            counts of lanes and the
                                            chainages and tunnel distances
                                            they apply at.
    '''
    # First get the lane counts into a human-readable format.  If
    # there is no change in the count of lanes at a chainage we give
    # the count, if there is we show the transition.
    lanes_texts = []
    for up, down in lanes:
        if up == down:
            # The count of lanes does not change.
            lanes_texts.append(str(up) + ' lane' + gen.Plural(up))
        else:
            lanes_texts.append(str(up) + ' lane' + gen.Plural(up)
                               + ' to ' + str(down))
    # Figure out the widths we need to centre the text on.
    ch_lines = []
    dist_lines = []
    lane_lines = []
    for ch, dist, text in zip(chs, dists, lanes_texts):
        ch_text = gen.RoundText(ch, 2)
        dist_text = gen.RoundText(dist, 2)
        width = max(len(ch_text), len(dist_text), len(text))
        ch_lines.append(ch_text.center(width))
        dist_lines.append(dist_text.center(width))
        lane_lines.append(text.center(width))
    mess = ('> Route "' + route_name + '" has the following count of\n'
            '> lanes along the length of this tunnel:\n'
            '>   Route chainages:    ' + ' | '.join(ch_lines) + '\n'
            '>   Tunnel distances:   ' + ' | '.join(dist_lines) + '\n'
            '>   Lane at that point: ' + ' | '.join(lane_lines) + '\n')
    return(mess)

def ShowSpeeds(route_dict, tun_name, veh_type, speed, dists, chs, spds, grads,
               sources):
    '''Build lines that we use in a few different error messages with
    different speed limits/gradients in different routes.

        Parameters:
            route_dict      {}              A route definition.
            tun_name        str             The name of a tunnel that is
                                            in the route.
            speed           float           A traffic speed to show.
            dists           [float]         A list of distances along
                                            the tunnel, at which speed
                                            limits are set by either
                                            the route speed limits or
                                            a vehicle type's inability
                                            to climba steep upgrade.
            chs             [float]         A list of chainages in the
                                            route that correspond to
                                            the distances.
            speeds          [float]         A list of speed limits.
            sources         [str]           A list of strings (either
                                            "speed" or "gradient") to
                                            indicate where a particular
                                            speed limit came from.

        Returns:
            mess            [str]           A list of strings giving
                                            either the speed limits in
                                            the tunnel or the statement
                                            that there are no speed
                                            limits.
    '''
    # First get the lane counts into a human-readable format.  If
    # there is no change in the speed at a chainage we give the speed,
    # if there is we show the transition.
    route_name = route_dict["block_index"][0]
    if (route_dict["speedlimits"] == "Not set" and
        route_dict["profile"][0] == "none"):
        mess = ('> Route "' + route_name + '" has no speed limits or\n'
                '> gradients in this tunnel and tried to put\n'
                '> traffic at ' + gen.RoundText(speed, 2)
                  + ' km/h into the tunnel.\n')
    else:
        # Figure out the widths we need to centre the text on.
        ch1_lines = []
        ch2_lines = []
        dist1_lines = []
        dist2_lines = []
        spd_lines = []
        grad_lines = []
        source_lines = []
        # print("ShowSpeeds", len(grads), len(spds), len(chs), len(dists))
        # print(grads, spds, chs, dists)
        for index, speed in enumerate(spds[1:],start = 1):
            indexm1 = index - 1
            ch1_text = gen.RoundText(chs[indexm1], 2)
            ch2_text = gen.RoundText(chs[index], 2)
            dist1_text = gen.RoundText(dists[indexm1], 2)
            dist2_text = gen.RoundText(dists[index], 2)
            grad_text = gen.RoundText(grads[index], 5)
            speed_text = gen.RoundText(speed, 2)
            source_text = sources[index]
            width = max(len(ch1_text), len(ch2_text),
                        len(dist1_text), len(dist2_text),
                        len(grad_text), len(speed_text),
                        len(source_text))


            ch1_lines.append(ch1_text.center(width))
            ch2_lines.append(ch2_text.center(width))
            dist1_lines.append(dist1_text.center(width))
            dist2_lines.append(dist2_text.center(width))
            grad_lines.append(grad_text.center(width))
            spd_lines.append(speed_text.center(width))
            source_lines.append(source_text.center(width))
        mess = ('> Route "' + route_name + '" has the following speed\n'
                '> limits along the length of this tunnel:\n'
                '>    Start distance: ' + ' | '.join(dist1_lines) + '\n'
                '>     Stop distance: ' + ' | '.join(dist2_lines) + '\n'
                '>       Up chainage: ' + ' | '.join(ch1_lines) + '\n'
                '>     Down chainage: ' + ' | '.join(ch2_lines) + '\n'
                '>          Gradient: ' + ' | '.join(grad_lines) + '\n'
                '>       Speed limit: ' + ' | '.join(spd_lines) + '\n'
                '>    Cause of limit: ' + ' | '.join(source_lines) + '\n')
    return(mess)


def ShowSpeeds2(route_dict, tun_name, veh_type, default):
    '''Build lines that we use in a few different error messages with
    different speed limits in different routes.

        Parameters:
            route_dict      {}              A route definition.
            tun_name        str             The name of a tunnel that is
                                            in the route.
            default         float           A traffic speed to use if
                                            no speed limits are set.

        Returns:
            mess            [str]           A list of strings giving
                                            either the speed limits in
                                            the tunnel or the statement
                                            that there are no speed
                                            limits.
    '''
    # First get the lane counts into a human-readable format.  If
    # there is no change in the speed at a chainage we give the speed,
    # if there is we show the transition.
    route_name = route_dict["block_index"][0]
    if route_dict["speedlimits"] == "Not set":
        mess = ('> Route "' + route_name + '" has no speed limits in this\n'
                '> tunnel and tried to put traffic at '
                  + gen.RoundText(default, 2) + ' km/h\n'
                '> into the tunnel.\n')
    else:
        # Get pairs of speed limits on each side of the chainages.
        dists, chs, spds = PropsInTunnel(route_dict, tun_name, "speedlimits", log)
        spds_texts = []
        for up, down in spds:
            if up == down:
                # The speed limit did not change.
                spds_texts.append(gen.RoundText(up, 2) + ' km/h')
            else:
                spds_texts.append(gen.RoundText(up, 2) + ' to '
                                  + gen.RoundText(down, 2) + ' km/h')
        # Figure out the widths we need to centre the text on.
        ch_lines = []
        dist_lines = []
        spd_lines = []
        for ch, dist, text in zip(chs, dists, spds_texts):
            ch_text = gen.RoundText(ch, 2)
            dist_text = gen.RoundText(dist, 2)
            width = max(len(ch_text), len(dist_text), len(text))
            ch_lines.append(ch_text.center(width))
            dist_lines.append(dist_text.center(width))
            spd_lines.append(text.center(width))
        mess = ('> Route "' + route_name + '" has the following speed\n'
                '> limits along the length of this tunnel:\n'
                '>   Route chainages:  ' + ' | '.join(ch_lines) + '\n'
                '>   Tunnel distances: ' + ' | '.join(dist_lines) + '\n'
                '>   Speed limits:     ' + ' | '.join(spd_lines) + '\n')
    return(mess)


def PropsInTunnel(route_dict, tun_name, key, log):
    '''Take a route dictionary, a tunnel name and a key to a route
    feature like lane count or gradient.  Figure out how what value
    each has in each subdivision of the tunnel.  Return the data
    as lists of tunnel distances and route chainages and a list of
    tuples of counts of the property on both sides of each location.

        Parameters:
            route_dict      {}              A route definition.
            tun_name        str             The name of a tunnel that is
                                            in the route.
            key             str             A key in the route dictionary
                                            to a property.
            log             handle          The handle of the logfile.

        Returns:
            dists           [float]         A list of distances in this
                                            tunnel (including the back
                                            end distance and forward end
                                            distance) at which features
                                            occur.
            chs             [float]         A list of chainages in the
                                            route that correlate to the
                                            distances.
            props           [(,)]           A list of properties (count
                                            of lanes, gradient or speed
                                            limit) in the route.  Each
                                            entry in the list is a tuple
                                            of values on each side of
                                            each chainage.
    '''
    # Get the tunnel index in the route, the tunnels up and down
    # chainages, and the conversion factors from chainage to distance.
    index = route_dict["tunnel_names"].index(tun_name)
    up_ch, down_ch = route_dict["tunnel_chs"][index:index + 2]
    mult, offset = route_dict["route2tun"][index]
    (props_list, chs_list) = route_dict[key][:2]
    # Get a list of the counts of lanes along this tunnel and
    # the chainages at which each count of lanes starts and stops.
    # This is step interpolation (as opposed to linear interpolation).
    chs, props = PickStepRanges(chs_list, props_list, up_ch, down_ch, log)
    # Turn the chainages into distances along the tunnel.
    dists = [ch * mult + offset for ch in chs]
    return(dists, chs, props)


def SpeedsInTunnel(route_dict, tun_name, vehcalc_dict, veh_type, def_spd, log):
    '''Take a route dictionary, a tunnel name, data on the vehicle
    types (including any speed restrictions on upgrades), the name
    of a vehicle type and a default traffic speed.
    Figure out the setting of speed limits from the route and any
    limitations of the vehicle type.
    If there was a block of speed limits, cap them at the default
    speeds and return them.  If the vehicle type has its speed
    limited by gradients, apply those too.
    If there were no speed limits, return the extents of the tunnel
    and the desired traffic speed.

        Parameters:
            route_dict      {}              A route definition.
            tun_name        str             The name of a tunnel that is
                                            in the route.
            vehcalc_dict    {}              The properties of traffic
                                            types, including their speed
                                            limits.
            veh_type        str             The name of a vehicle type,
                                            a key to vehcalc_dict.
            def_spd         float           A traffic speed to use if
                                            no speed limits are set.
            log             handle          The handle of the logfile.

        Returns:
            dists           [float]         A list of distances in this
                                            tunnel (including the back
                                            end distance and forward end
                                            distance) at which speed
                                            limits or gradients are set.
            chs             [float]         A list of chainages in the
                                            route that correlate to the
                                            distances.
            speeds          []              A list of speed limits in the
                                            route, which apply up to and
                                            including the corresponding
                                            chainage/distance.
            gradients       []              A list of gradients in the
                                            route, which apply up to and
                                            including the corresponding
                                            chainage/distance.
            sources         []              A list of words (either
                                            "speeds" or "gradient").
                                            This indicates whether the
                                            speed came from the speed
                                            limits or the vehicle's
                                            inability to climb a given
                                            gradient.

    '''
    route_name = route_dict["block_index"][0]
    # print("SpeedsInTunnels", route_name)
    loc_spd = abs(def_spd)

    if route_dict["speedlimits"] == "Not set":
        # Pretend that the speed limits in the route are the traffic
        # speed for this route in this trafficsteady block.
        index = route_dict["tunnel_names"].index(tun_name)
        dists = list(route_dict["tunnel_dists"][index])
        chs = route_dict["tunnel_chs"][index:index + 2]
        mono_spds = [def_spd] * 2
        # print("No speed limits")
    else:
        (dists, chs, speeds) = PropsInTunnel(route_dict, tun_name,
                                             "speedlimits", log)
        mono_spds = []
        for (up_spd, down_spd) in speeds[:-1]:
            if loc_spd < down_spd:
                mono_spds.append(def_spd)
            else:
                mono_spds.append(math.copysign(down_spd, def_spd))
        mono_spds.append(min(loc_spd, speeds[-1][0]))
        # print("Speed limits set", mono_spds)


    # Get the speed limits on upgrades for this vehicle type.
    veh_spds, veh_grads = vehcalc_dict[veh_type][6:8]

    if route_dict["gradients"] == "Not set":
        # There are no gradients in this route definition.  Spoof the
        # gradients in this tunnel as being all flat.
        # print("No gradients")
        count = len(dists)
        grads = [0.0] * count
        sources = ["speeds"] * count
    else:
        # There are gradients in this route.  We need to check
        # whether any vehicles slow down on upgrades, and we alter
        # the contents of 'limits' (set above) to include the
        # changes of gradient along the length of this tunnel.

        # Get the gradients and chainages in the route.
        (dists2, chs2, grads) = MonosInTunnel(route_dict, tun_name,
                                              "gradients", log)
        # If the desired speed is negative, negate the gradients in
        # the route (because the vehicles are travelling up the route,
        # not down the route).
        if def_spd < 0.:
            calc_grads = [-grad for grad in grads]
            reverse_traf = True
        else:
            calc_grads = grads
            reverse_traf = False

        # First check if the maximum speed at the lowest gradient is
        # lower than the desired speed or any of the speed limits
        # in the route.  Select a maximum speed to use based on that.
        if veh_spds[0] < loc_spd:
            base_spd = veh_spds[0]
        else:
            base_spd = loc_spd
        # Cap the speeds in the tunnel at or below the new speed.
        loc_spds = [min(base_spd, speed) for speed in mono_spds]

        # There may be speed limits on upgrades that are lower than
        # the new base speed.  If the user had a "speedgradpairs"
        # entry for this vehicle type, 'veh_spds' will have more
        # than one entry.
        if len(veh_spds) > 1:
            # Figure out if there are any gradients on which this
            # vehicle type needs to slow down on.
            capped_spds = []
            for grad in calc_grads:
                candidate = PickStepValue(veh_grads, veh_spds, grad, log)
                if reverse_traf:
                    capped_spds.append(-candidate)
                else:
                    capped_spds.append(candidate)
        else:
            # There are no limits on upgrades.  Get a list of the
            # speed limits for the chainage of each change of
            # gradient (using the speed limits in the route instead
            # of the nonexistent "speedgradpairs" entry).
            capped_spds = []
            for ch in chs2:
                candidate = PickStepValue(chs, mono_spds, ch, log)
                capped_spds.append(candidate)

        # Make a list of the chainages of the speed limits and
        # the chainages of the gradients, remove duplicates and
        # sort the list into ascending order.
        comb_chs = list(set(chs + chs2))
        comb_chs.sort()
        # print("SpeedsInTunnel", veh_type, veh_spds, veh_grads)
        # print("route limits\n", chs, "\n", loc_spds)
        # print("gradient limits\n", chs2, "\n", capped_spds, "\n", grads)
        # Get the speeds of the vehicles dictated by the speed
        # limits and the gradients and choose the lower of the
        # two.
        comb_spds = []
        comb_grads = []
        sources = []
        for ch in comb_chs:
            speed1 = PickStepValue(chs, loc_spds, ch, log)
            speed2 = PickStepValue(chs2, capped_spds, ch, log)
            if reverse_traf:
                if (speed1 >= speed2):
                    comb_spds.append(speed1)
                    sources.append("speeds")
                else:
                    comb_spds.append(speed2)
                    sources.append("gradient")
            else:
                if (speed1 <= speed2):
                    comb_spds.append(speed1)
                    sources.append("speeds")
                else:
                    comb_spds.append(speed2)
                    sources.append("gradient")
            comb_grads.append(PickStepValue(chs2, grads, ch, log))
        # print("Combined limits", "\n", comb_chs, "\n", comb_spds,
        #       "\n", comb_grads)
        # print(sources)
        # Get the distances that correlate to the chainages in
        # the combined list.
        mult, offset = GetMultOffset(route_dict, tun_name)
        dists = [ch * mult + offset for ch in comb_chs]

        # Now overwrite the original values for return.
        chs = comb_chs
        mono_spds = comb_spds
        grads = comb_grads
    return(dists, chs, mono_spds, grads, sources)


def MonosInTunnel(route_dict, tun_name, key, log):
    '''Take a route dictionary and a tunnel name and figure out the
    setting of a property like the lane count, speed limit or
    gradients in the tunnel, including at the two ends.  The selection
    of the down side of the up end chainage and the up side of the
    down end chainage mean that if a change of lane counts coincides
    with a tunnel end we don't choose the wrong value to return.

        Parameters:
            route_dict      {}              A route definition.
            tun_name        str             The name of a tunnel that is
                                            in the route.
            key             str             A key in the route dictionary
                                            to a property.
            log             handle          The handle of the logfile.

        Returns:
            dists           [float]         A list of distances in this
                                            tunnel (including the back
                                            end distance and forward end
                                            distance) at which features
                                            occur.
            chs             [float]         A list of chainages in the
                                            route that correlate to the
                                            distances.
            mono_props      []              A list of properties (count
                                            of lanes or gradient) in
                                            the route.  Each entry in
                                            the list is a tuple of
                                            values on each side of each
                                            chainage.
    '''
    (dists, chs, props) = PropsInTunnel(route_dict, tun_name, key, log)
    mono_props = []
    # for (up_props, down_props) in props[:-1]:
    #     mono_props.append(up_props)
    # mono_props.append(props[-1][0])
    mono_props = [up_ch for up_ch, down_ch in props]
    return(dists, chs, mono_props)


def PickStepRanges(ch_list, prop_list, up_ch, down_ch, log):
    '''Take a list of chainages and a list of properties.  The properties
    apply up to and including the corresponding chainage.  Take a pair
    of chainages that represent something of interest (typically the
    extents of a tunnel) and return lists of the properties and
    chainages bounded by the pair of chainages.  It is used for things
    like gradients, counts of lanes and speed limits: they all apply
    up to and including a chainage.  A quick ASCII art example, using
    the count of lanes in a road tunnel:

     3-lanes in route        2-lanes               3-lanes

    ----------------\                      /------------------------
    ____________     \____________________/    _____________________

    ----------------------------------------------------------------

    ----------------------------------------------------------------
          ^         ^                     ^       ^
          |         |                     |       |
        up_ch      ch1                   ch2    down_ch

    This would return the information that there are three lanes from
    up_ch up to and including ch1, two lanes from ch1 up to and
    including ch2 and three lanes from ch2 up to and including down_ch.

        Parameters:
            ch_list         [float]         A list of chainages in the
                                            in the route.
            prop_list       [float]         A list of properties that
                                            apply up to and including
                                            the correlating chainage.
            up_ch           float           The chainage of the up end
                                            we want to start at (usually
                                            the chainage of the up end
                                            of a tunnel).
            down_ch         float           The chainage of the down
                                            end we want to stop at.
            log             handle          The handle of the logfile.

        Returns:
            chs             [float]         A list of chainages in the
                                            route that correlate to the
                                            distances.
            props           [(,)]           A list of properties (count
                                            of lanes, gradient or speed
                                            limit) in the route.  Each
                                            entry in the list is a tuple
                                            of values on each side of
                                            each chainage.
    '''
    # Get the value of the property at both sides of the up end.
    ch_list2 = [up_ch]
    prop_counts = BothSidesProps(ch_list, prop_list, up_ch, log)
    prop_list2 = [prop_counts]
    for ch in ch_list:
        if up_ch < ch <= down_ch:
            ch_list2.append(ch)
            prop_counts = BothSidesProps(ch_list, prop_list, ch, log)
            prop_list2.append(prop_counts)
        elif ch > down_ch:
            # We have passed beyond down_ch and don't need to
            # process any more entries.
            break
    # Check if we need to add down_ch to the list.
    if not math.isclose(down_ch, ch_list2[-1]):
        ch_list2.append(down_ch)
        prop_counts = BothSidesProps(ch_list, prop_list, down_ch, log)
        prop_list2.append(prop_counts)
    return(ch_list2, prop_list2)


def BothSidesProps(ch_list, prop_list, ch, log):
    '''Take a list of chainages, a list of associated properties
    and a chainage.  Get the values of the property at each side
    of the chainage and return them as a tuple of two.

        Parameters:
            ch_list         [float]         A list of chainages in the
                                            in the route.
            prop_list       [float]         A list of properties that
                                            apply up to and including
                                            the correlating chainage.
            ch              float           The chainage we want the
                                            values on each side of.
            log             handle          The handle of the logfile.

        Returns:
            prop_counts     [(,)]           A tuple of properties on
                                            each side of each chainage.
    '''
    # Get the counts of properties on either side of the location.
    # We allow a fuzziness of 1 m either way.
    prop_up = PickStepValue(ch_list, prop_list, ch - 1, log)
    prop_down = PickStepValue(ch_list, prop_list, ch + 1, log)
    # We store the count of lanes on the up side first, then the
    # down side.
    prop_counts = (prop_up, prop_down)
    return(prop_counts)


def PickStepValue(ch_list, prop_list, ch, log):
    '''Take a list of chainages and a list of properties that apply
    up to and at those chainages.  Take a chainage that we want
    the value of the property at and return it.

    This routine assumes that the chainages increase.  It is not
    suitable for calling with lists of tunnel distances instead of
    route chainages; if the tunnels are reversed in a route, the
    distances decrease with increasing chainage.

    Behaviour:
     1) If the chainage is below the first entry in the list of
        chainages, we use the first entry in the property list.
     2) If the chainage is above the last entry in the list of
        chainages, we use the last entry in the property list.
     3) If the chainage is exactly equal to an entry in the list
        of chainages, we use previous entry in the list.
     4) If the chainage lies between two chainages in the list, we
        take the property at the lower chainage.

        Parameters:
            ch_list         [float]         A list of chainages in the
                                            in the route.
            prop_list     fas  [float]         A list of properties that
                                            apply up to and including
                                            the correlating chainage.
            ch              float           The chainage we want the
                                            values on each side of.
            log             handle          The handle of the logfile.

        Returns:
            value           float           A value at the chainage or
                                            on the up side of it.
    '''
    # Surprisingly this code handles all four behaviours.
    value = prop_list[0]
    maxcount = len(ch_list) - 1
    if len(ch_list) != len(prop_list):
        print("Mismatched list lengths passed to PickStepValue\n",
              len(ch_list), len(prop_list),
              ch_list, prop_list)
        raise()
        gen.OopsIDidItAgain(log)
    # print("In PickStepValue", ch_list, prop_list, ch)
    for index, test_ch in enumerate(ch_list):
        if ch <= test_ch:
            # We keep the current value (because the value applies up
            # to and including the chainage) and break from the loop.
            # print("breaking", ch, test_ch, value, index)
            break
        else:
            # We choose the corresponding value and loop around.
            value = prop_list[min(maxcount, index + 1)]
            # print("iterating", ch, test_ch, value, index)
    return(value)


def TrafficStat(line_triples, settings_dict, routes_dict, state,
                route_name, tun_name, log):
    '''Take the definition of traffic in a route that this tunnel
    passes through.  Turn it into a set of traffic flows bounded by
    start and stop chainages and tunnel start and stop distances.
    Write the details to the log file and to the screen.

        Parameters:
            line_triples [(int, str, str)]  List of lines in the file.
            settings_dict   {}              Dictionary of the run settings.
            routes_dict     {}              Dictionary of the route
                                            definitions.
            state           [float]         A list of entries that
                                            define a block of traffic.
            route_name      str             Name of a route.
            tun_name        str             Name of a tunnel that lies
                                            in the route.
            log             handle          The handle of the logfile.

        Returns:
            Nothing useful.
    '''
    (state, PCU_dens, veh_dens, PCUperkm, start_ch, stop_ch,
                                traffic_flow, stat_index) = flowrates

    route_dict = routes_dict[route_name]
    # Get some data we will need for both stationary and
    # moving traffic.
    index = route_dict["tunnel_names"].index(tun_name)
    mult, offset = route_dict["route2tun"][index]
    up_ch, down_ch = route_dict["tunnel_chs"][index:index + 2]
    if route_dict["signed_names"][index][0] == "-":
        t_rev = True
    else:
        t_rev = False
    # Set the extents of traffic in the tunnel to the
    # distances at the back end and forward end.
    start_dist, stop_dist = route_dict["tunnel_dists"][index]

    # Check to see if stationary traffic is in this tunnel.
    # If it isn't, there's no need to process this route
    # any further.
    if start_ch >= down_ch:
        mess =("  No stationary traffic in the tunnel in route "
              + route_name + " because", "  traffic start ch. "
              + gen.FloatText(start_ch) + " >= tunnel down ch. "
              + gen.FloatText(down_ch))
        for line in mess:
            gen.WriteMessage2(line, log)
        fill_frac = (0, 0)
        return("No traffic", fill_frac)
    elif stop_ch <= up_ch:
        mess =('  No stationary traffic in the tunnel in route "'
              + route_name + '" because', "  traffic stop ch. "
              + gen.FloatText(stop_ch) + " <= tunnel up ch. "
              + gen.FloatText(up_ch))
        for line in mess:
            gen.WriteMessage2(line, log)
        fill_frac = (0, 0)
        return("No traffic", fill_frac)

    # Figure out the extent of traffic in the tunnel.
    if start_ch <= up_ch and stop_ch >= down_ch:
        # Traffic fills the tunnel completely.
        mess = ['  Stationary traffic in route "' + route_name
                + '" fills the tunnel from end to end:']
        start_ch = up_ch
        stop_ch = down_ch
    elif start_ch > up_ch and stop_ch < down_ch:
        # Traffic starts and stops in the tunnel.  No
        # need to adjust the chainages.
        mess = ['  Stationary traffic in route "' + route_name
                + '" starts and stops in the tunnel:']
    elif start_ch > up_ch and stop_ch >= down_ch:
        # Traffic starts in the tunnel and fills to
        # the down end.
        mess = ['  Stationary traffic in route "' + route_name
                + '" starts in the tunnel:']
        stop_ch = down_ch
    elif start_ch <= up_ch and stop_ch < down_ch:
        # Traffic starts at the up end of the tunnel
        mess = ['  Stationary traffic in route "' + route_name
                + '" stops in the tunnel:']
        # and ends before the down end.
        start_ch = up_ch
    else:
        # Not sure if we can get here, but you never know.
        # This prints an informative message and calls a
        # routine that stops the run.
        print('> Fouled up setting traffic chainage extents\n'
              '> in tunnels in PROC TrafficInTunnels.  The\n'
              '> tunnel was "' + tun_name + '" in route "'
                + route_name + '",\n'
              '> and the four chainages involved were:\n'
              '>   Tunnel extents: ', up_ch, down_ch, '\n'
              '>   Traffic extents: ', start_ch, stop_ch)
        gen.OopsIDidItAgain(log)

    # Give the chainages (in the route) and the distances
    # (along the tunnel) at which traffic starts and stops.
    # Also calculate the fractional occupancies with the base
    # at the back end.
    length = up_ch - down_ch
    if t_rev:
        # The back end of the tunnel is at the down end in the route.
        if math.isclose(stop_ch, down_ch):
            # Traffic starts at or before the down end (back end) of
            # the tunnel
            fill1 = 0
        else:
            fill1 = abs(stop_ch - down_ch) / length
        if math.isclose(start_ch, up_ch):
            # Traffic continues to or after the up end (forward
            # end) of the tunnel.
            fill2 = 1
        else:
            fill2 = abs(start_ch - up_ch) / length
        mess.append("    Up end of traffic in this tunnel is"
                    + " at ch. " + gen.RoundText(start_ch, 2))
        mess.append("    Down end of traffic in this tunnel is"
                    + " at ch. " + gen.RoundText(stop_ch, 2))
        mess.append("    The tunnel is reversed in the route.")
        mess.append("    Back end of traffic is at tunnel"
                    + " distance "
                    + gen.RoundText(start_dist, 2))
        mess.append("    Forward end of traffic is at tunnel"
                    + " distance "
                    + gen.RoundText(stop_dist, 2))
    else:
        # The back end of the tunnel is at the up end in the route.
        if math.isclose(start_ch, up_ch):
            # Traffic starts at or before the up end (back end) of
            # the tunnel
            fill1 = 0
        else:
            fill1 = abs(start_ch - up_ch) / length
        if math.isclose(stop_ch, down_ch):
            # Traffic continues to or after the down end (forward
            # end) of the tunnel.
            fill2 = 1
        else:
            fill2 = abs(stop_ch - down_ch) / length
        mess.append("    Up end of traffic in this tunnel is"
                    + " at ch. " + gen.RoundText(start_ch, 2))
        mess.append("    Down end of traffic in this tunnel is"
                    + " at ch. " + gen.RoundText(stop_ch, 2))
        mess.append("    The tunnel is not reversed in the route.")
        mess.append("    Back end of traffic is at tunnel"
                    + " distance "
                    + gen.RoundText(stop_dist, 2))
        mess.append("    Forward end of traffic is at tunnel"
                    + " distance "
                    + gen.RoundText(start_dist, 2))
    mess.append("    Fractional occupancies for use in the"
                + " calculation are (", gen.RoundText(fill1, 4)
                + ',', gen.RoundText(fill2, 4) + ').')

    for line in mess:
        gen.WriteMessage2(line, log)
    return()

def ProcessRoute(line_triples, tr_index, settings_dict, tunnels_dict,
                 trtypes_dict, log):
    '''Read all the data defining a route and add an entry for it into the
    route dictionary.  Return the updated route dictionary.

        Parameters:
            line_triples [(int, str, str)]  List of lines in the file.
            tr_index        int             Pointer to where the block starts
                                            in line_triples.
            settings_dict   {}              Dictionary of the run settings.
            tunnels_dict    {}              The tunnels, as a dictionary, for
                                            checking the names we use.
            trtypes_dict    {}              The train types, because we may
                                            refer to them in schedules blocks.
            log             handle          The handle of the logfile.

        Returns:
            route_name      str             Name of the new route, used as
                                            a dictionary key.
            new_route_dict  {}              Dictionary defining a new route.

        Errors:
            Aborts with 2301 if the name of a route is also the name of a
            tunnel.
            Aborts with 2302 if the name of a tunnel given is not valid.
            Aborts with 2303 if the name of a tunnel appears twice.
            Aborts with 2304 if a route keyword has been used as a tunnel
            name.
            Aborts with 2305 if a route has a gradients block and an
            elevations block.
            Aborts with 2306 if the value of a gradient was less than -1 or
            greater than +1.
            Aborts with 2307 if the change in elevation between two points
            was greater than the distance between two points.
            Aborts with 2308 if the "begin tunnels" block was absent.
            Aborts with 2309 if the user put in a "tunnels" keyword instead
            of a "begin tunnels" block by mistake.
            Aborts with 2310 if a train schedule tried to spawn trains at
            a chainage before the route origin.
            Aborts with 2311 if a train schedule tried to spawn trains at
            a chainage so far down the route that the alignment definition
            doesn't exist there.
            Aborts with 2312 if two adjacent tunnels in a route do not
            have a common node.
            Aborts with 2313 if two adjacent tunnels in a route have a
            the same nodes at each end (can't tell which way round to
            put them).
            Aborts with 2314 if a speed limit was zero or less.
            Aborts with 2315 if the "begin gradients" sub-block did not
            specify if the gradients were percentages or fractions, it
            started with "begin gradients".
            Aborts with 2316 if the "begin gradients" sub-block did not
            specify if the gradients were percentages or fractions, it
            specified something else.
    '''
    debug1 = settings_dict["debug1"]
    file_name = settings_dict["file_name"]
    units = settings_dict["units"]
    max_trlen = settings_dict["max_trlen"]
    # Create a dictionary to hold the data for this route.

    valid_settings = {
                      "origin": ("float any dist1 a route origin chainage",),
                      "portal": ("float any dist1 an up portal chainage",),
                      "begin":   (("tunnels", "gradients", "elevations",
                                   "schedule", "speedlimits", "lanes",
                                   "radii", "sectors", "coasting",
                                   "regenfractions",),
                                  "QAstr"),
                      "#skip": "discard"  # This catches all other lines
                     }
    #
    # Define the optional entries allowed in each keyword.  The origin
    # can be given an optional elevation to start from.
    optionals = {"origin": {"elevation": "float 0+ null an elevation"},
                 "begin":  {"switcheroo": ("true", "false")
                           },
                }
    #
    # Make a list of what entries we must have.
    requireds = ("origin", "portal")
    #
    # Make a list of what entries can be duplicated (none).
    duplicates = ("begin",)

    settings = (valid_settings, requireds, optionals, duplicates)
    block_name = "route"


    if debug1:
        print("Processing route lines")

    result = ProcessBlock(line_triples, tr_index, settings_dict,
                          block_name, {}, settings, log)
    if result is None:
        return(None)
    else:
        (route_name, new_route_dict) = result
        # Store the route name and the index of the start of the block
        # so we can find it if it is needed for later error messages,
        # like traffic.  This saves us passing both the route dictionary
        # and the route name as arguments.

    if route_name in tunnels_dict:
        err = ('> In the file named "' + file_name + '"\n'
               '> there was a route named "' + route_name + '", which\n'
               '> has the same name as a tunnel.  Please edit\n'
               '> the file to change the name of the route.\n'
               '> Alternatively, change the name of the tunnel.'
              )
        gen.WriteError(2301, err, log)
        gen.ErrorOnLine2(tr_index, line_triples, log, False)
        return(None)

    # We now have the mandatory route origin chainage and mandatory portal chainage.
    # chainage. Check that the portal chainage is above the origin chainage.
    # First figure out the chainages and the details of the lines they appear on
    origin_stuff = new_route_dict["origin"]
    origin_index = origin_stuff[2]

    start = origin_stuff[0]
    line1 = line_triples[origin_index]

    portal_stuff = new_route_dict["portal"]
    portal_index = portal_stuff[2]

    portal = portal_stuff[0]
    line2 = line_triples[portal_index]

    # Build suitable lines of error message.
    err_lines = ('> The portal chainage must be higher than the\n'
                 '> route origin chainage, and in route "'
                  + route_name + '"\n'
                 '> it was not.'
                )
    result = CheckGreater(start, line1, portal, line2, ">", err_lines,
                          settings_dict, log)
    if result is None:
        return(None)

    # If we get to here, everything is OK.  Now go through and process
    # the tunnels block, ignoring the lines with "origin" and "portal"
    # at the start.  The same loop processes the gradients or elevations
    # blocks (one or the other, not both).
    done_already = ("origin", "portal")

    # Create a boolean that we set True when we find "begin tunnels"
    # and set false when we find "end tunnels".  All the words on
    # lines between those are the names of tunnels.
    tunnels_block = False
    # Create a list to hold the tunnel names.  If the name is preceded
    # by a minus sign, it is removed before adding it (the names with
    # minus signs signify a tunnel reversed in the route: they and their
    # minus signs are kept in a similar list called "signed_names").
    tunnel_names = []
    # Create a list to hold the route chainages each tunnel ends at.
    # We start with the portal chainage so that the list holds all the
    # chainages we need to determine the up and down chainages of every
    # tunnel in the list.
    tunnel_chs = [portal]




    # Create lists to hold the vertical profile (heights, gradients and
    # the chainages that they change at).  The profile can be set by a
    # "begin chainages" block or a "begin elevations" block (but not both).
    # If neither block is present, the route is flat.
    # If an elevation was not set as an optional entry in the "origin"
    # command then the elevation at the origin is zero.
    # Check the optionals dictionary for an elevation at the origin.
    if "elevation" in origin_stuff[1]:
        start_elev = float(origin_stuff[1]["elevation"])
    else:
        start_elev = 0.0
    # Make a tag to figure out whether the gradients were set by
    # gradients, elevations or not at all.
    profile = "none"
    # Store tr_index in case we need it to complain about a missing block.
    start_index = tr_index
    # Make a dictionary to store the train movement details.
    schedules_dict = {}

    # Process the "begin tunnels" block first, as we need the tunnel
    # end chainages for the other blocks.
    while tr_index < len(line_triples):
        tr_index += 1
        (line_number, line_data, line_text) = line_triples[tr_index]
        if debug1:
            print("Line1",str(line_number) + ":", line_data)

        # Get any optional arguments out of the line text.  This
        # is needed in case someone uses a line of entry like
        #   begin gradients switcheroo :=true  percentages
        # We need to turn it into
        #   begin gradients percentages
        # before processing it.
        line_data, optionals = gen.GetOptionals(line_number, line_data,
                                              line_text, file_name,
                                              debug1, log)

        # Many of the blocks allow the user to switch the order of the
        # values and the chainages.  We build most of the entries we
        # will pass to ProcessNumberList.  We set the values of
        # name1 or name2 depending on the value of 'switcheroo'.
        # The default is value then chainage.  If switcheroo := true
        # it becomes chainage then value.  The option does not apply
        # to the "elevations" block, which is always chainage followed
        # by elevation.
        if ("switcheroo" in optionals and
            optionals["switcheroo"]) == "true":
            rule1 = ">"   # chainage
            rule2 = ""
            name1 = "chainages"
            switcheroo = True
        else:
            # This is the default.
            rule1 = ""
            rule2 = ">"   # chainage
            name2 = "chainages"
            switcheroo = False

        words = line_data.split()
        words_low = line_data.lower().split()

        if words_low[:2] == ["end", "route"]:
            # We've finished, break out of the loop.
            break
        elif words_low[:2] == ["begin", "tunnels"]:
            tunnels_block = True
            # Get the index of the "begin tunnels" block in the route
            # definition in case we need it for an error message later.
            tunblock_index = tr_index
        elif words_low[:2] == ["end", "tunnels"]:
            tunnels_block = False
            # Store the tunnel chainages so we can use it in a test for
            # lanes ending before the tunnels end.
            new_route_dict.__setitem__("tunnel_names", tunnel_names)
            new_route_dict.__setitem__("tunnel_chs", tunnel_chs)
        elif tunnels_block is True:
            # We are at a line between "begin tunnels" and "end tunnels".
            # All of the words on the line should be the names of tunnels
            # or the names of tunnels preceded by a minus sign.
            for candidate in words_low:
                if candidate[0] == "-":
                    # The tunnel name started with "-" so the tunnel is
                    # reversed in the route.  We don't need to worry about
                    # this here, we just lop off the first character.
                    tunnel_name = candidate[1:]
                else:
                    tunnel_name = candidate
                if tunnel_name not in tunnels_dict:
                    err = ('> In the file named "' + file_name + '"\n'
                           '> the route named "' + route_name + '" refers to a\n'
                           '> tunnel that does not exist (the name\n'
                           '> given was "' + tunnel_name + '").\n'
                           '> Please edit the file to correct it.  For\n'
                           "> what it's worth, here are the names of\n"
                           "> the tunnel(s) you've created:\n"
                          )
                    err = err + gen.FormatOnLines(tuple(tunnels_dict.keys()))
                    gen.WriteError(2302, err, log)
                    gen.ErrorOnLine2(tr_index, line_triples, log, False)
                    return(None)
                elif tunnel_name in tunnel_names:
                    # The tunnel is a duplicate, which is also not allowed.
                    err = ('> In the file named "' + file_name + '"\n'
                           '> tunnel "' + tunnel_name + '" appears more than\n'
                           '> once in the route named "' + route_name + '".\n'
                           '> Please edit the file to remove the duplicate.'
                          )
                    gen.WriteError(2303, err, log)
                    gen.ErrorOnLine2(tr_index, line_triples, log, False)
                    return(None)
                else:
                    tunnel_data = tunnels_dict[tunnel_name]
                    # Figure out what route chainage this tunnel ends at
                    # and add it to the list.
                    fwd_end = tunnel_data["fwd"]
                    back_end = tunnel_data["back"]
                    length = abs(fwd_end[0] - back_end[0])

                    tunnel_chs.append(tunnel_chs[-1] + length)
                    # Now store the tunnel name, keeping the negative
                    # sign if present.
                    tunnel_names.append(candidate)

            # Catch an unlikely case that could cause confusion: a tunnel
            # named "origin" or "portal" (words that we use as route keywords).
            # We don't need to check for "begin" or "end" because the syntax
            # checker would have already caught those.
            if words_low[0] in done_already:
                err = ('> In the file named "' + file_name + '"\n'
                       '> the route named "' + route_name + '" refers to a\n'
                       '> tunnel named "' + words[0] + '".  That name is not\n'
                       '> allowed because it leads to a confusing\n'
                       '> state of affairs where "' + words[0] + '" could be\n'
                       '> treated as both a route keyword and as a\n'
                       '> tunnel name.\n'
                       '> Please edit the file to move that line before\n'
                       '> "begin tunnels" or after "end tunnels" and\n'
                       '> rename the tunnel you created called "' + words[0] + '"'
                      )
                gen.WriteError(2304, err, log)
                gen.ErrorOnLine2(tr_index, line_triples, log, False)
                return(None)

    # Now do all the other blocks.
    tr_index = start_index
    while tr_index < len(line_triples):
        tr_index += 1
        (line_number, line_data, line_text) = line_triples[tr_index]
        if debug1:
            print("Line1",str(line_number) + ":", line_data)

        # Get any optional arguments out of the line text.  This
        # is needed in case someone uses a line of entry like
        #   begin gradients switcheroo :=true  percentages
        # We need to turn it into
        #   begin gradients percentages
        # before processing it.
        line_data, optionals = gen.GetOptionals(line_number, line_data,
                                              line_text, file_name,
                                              debug1, log)

        # Many of the blocks allow the user to switch the order of the
        # values and the chainages.  We build most of the entries we
        # will pass to ProcessNumberList.  We set the values of
        # name1 or name2 depending on the value of 'switcheroo'.
        # The default is value then chainage.  If switcheroo := true
        # it becomes chainage then value.  The option does not apply
        # to the "elevations" block, which is always chainage followed
        # by elevation.
        if ("switcheroo" in optionals and
            optionals["switcheroo"]) == "true":
            rule1 = ">"   # chainage
            rule2 = ""
            name1 = "chainages"
            switcheroo = True
        else:
            # This is the default.
            rule1 = ""
            rule2 = ">"   # chainage
            name2 = "chainages"
            switcheroo = False

        words = line_data.split()
        words_low = line_data.lower().split()

        if words_low[:2] == ["end", "route"]:
            # We've finished, break out of the loop.
            break
        elif words_low[:2] == ["begin", "tunnels"]:
            tunnels_block = True
            # Get the index of the "begin tunnels" block in the route
            # definition in case we need it for an error message later.
            tunblock_index = tr_index
        elif words_low[:2] == ["end", "tunnels"]:
            tunnels_block = False
            # Store the tunnel chainages so we can use it in a test for
            # lanes ending before the tunnels end.
            new_route_dict.__setitem__("tunnel_names", tunnel_names)
            new_route_dict.__setitem__("tunnel_chs", tunnel_chs)
        elif tunnels_block is True:
            # We are at a line between "begin tunnels" and "end tunnels".
            # We've already read these, so we just pass.
            pass
        elif words_low[:2] in ( ["begin", "gradients"],
                                ["begin", "elevations"]):
            # Check if we have already read the profile data and complain if
            # we have.  This error message gives the lines that the two
            # blocks started at.
            if ( (profile == "elevations" and words_low[1] == "gradients") or
                 (profile == "gradients" and words_low[1] == "elevations") ):
                err = ('> In the file named "' + file_name + '"\n'
                       '> the route named "' + route_name + '" had a set\n'
                       '> of elevations and a set of gradients.  This is\n'
                       '> not allowed, you must define the profile by\n'
                       '> using one or the other, not both.\n'
                       '> Please edit the file to delete the gradients\n'
                       '> block or the elevations block.'
                      )
                gen.WriteError(2305, err, log)

                gen.ErrorOnTwoLines(old_line_number, old_line_text,
                                        line_number, line_text, log)
                return(None)

            if words_low[1] == "gradients":
                # Check for something specifying what the gradients are
                # (fractions -1 to +1 or percentages -100 to +100).
                if len(words) < 3:
                    err = ('> In the file named "' + file_name + '"\n'
                           '> the route named "' + route_name + '" had a\n'
                           '> "gradients" sub-block, but the line that\n'
                           '> started the block did not specify whether\n'
                           '> the gradients were given as fractions or\n'
                           '> percentages, it was just "begin gradients"\n'
                           '> If the numbers that specify your gradients\n'
                           '> are percentages (-100 to +100), then change\n'
                           '> the line to "begin gradients percentages".\n'
                           '> If the numbers that specify your gradients\n'
                           '> are fractions (-1 to +1), then change the\n'
                           '> line to "begin gradients fractions".\n'
                           "> If you don't know whether your gradients are\n"
                           '> percentages or fractions, you should probably\n'
                           '> stop using the program until you figure it\n'
                           '> out, as you have bigger problems.'
                          )
                    gen.WriteError(2315, err, log)
                    gen.ErrorOnLine(line_number, line_text, log)
                    return(None)
                elif words_low[2] not in ("percentages", "fractions"):
                    err = ('> In the file named "' + file_name + '"\n'
                           '> the route named "' + route_name + '" had a\n'
                           '> "gradients" sub-block, but the line that\n'
                           '> started the block did not specify whether\n'
                           '> the gradients were given as fractions or\n'
                           '> percentages, it was "begin gradients '
                             + words[2] + '".\n'
                           '> If the numbers that specify your gradients\n'
                           '> are percentages (-100 to +100), then change\n'
                           '> the line to "begin gradients percentages".\n'
                           '> If the numbers that specify your gradients\n'
                           '> are fractions (-1 to +1), then change the\n'
                           '> line to "begin gradients fractions".\n'
                           "> If you don't know whether your gradients are\n"
                           '> percentages or fractions, you should probably\n'
                           '> stop using the program until you figure that\n'
                           '> out.'
                          )
                    gen.WriteError(2316, err, log)
                    gen.ErrorOnLine(line_number, line_text, log)
                    return(None)
                elif words_low[2] == "percentages":
                    # We need to multiply the percentages by 0.01, and
                    # we set text to use in error message 2306.
                    grad_mult = 0.01
                    grad_err = ('> +100 (vertical up) to -100 (vertical down).\n'
                               '> Please edit the file and get the absolute\n'
                               '> value of the gradient under 100.  Note\n'
                               '> that gradients in this block are expressed\n'
                               '> as percentages (-100 to +100), not as\n'
                               "> fractions (-1 to +1) because you started\n"
                               '> the block with "begin gradients percentages"\n'
                               '> instead of "begin gradients fractions".')

                else:
                    grad_mult = 1.0
                    grad_err = ('> +1 (vertical up) to -1 (vertical down).\n'
                               '> Please edit the file and get the absolute\n'
                               '> value of the gradient under 1.  Note that\n'
                               '> gradients in this block are expressed as\n'
                               '> fractions (-1 to +1) not as percentages\n'
                               "> (-100 to +100) because you started the \n"
                               '> block with "begin gradients fractions"\n'
                               '> instead of "begin gradients percentages".')


            # Read gradients and distances (gradient followed by the
            # distance it stops at) or elevations (elevations followed
            # by the distances they occur at).
            #
            # If gradients are given there must be at least one gradient.
            # There can be an even or odd number of entries.
            #
            # The first gradient starts at the route origin.  If only one
            # gradient is given, this gradient is assumed to extend from
            # the route origin to infinity.
            #
            # If elevations are given, there must be at least one elevation
            # and one distance.  There must be an even number of entries.
            #
            # Save the details of where this profile block started in case
            # we need it for error message 2305.
            old_line_number, old_line_text = line_number, line_text

            if words_low[1] == "gradients":
                # Store the index of the line with "begin gradients" on it
                # and move to the next line.
                prof_index = tr_index
                tr_index += 1
                # We are expecting to read a list of gradients and chainages
                # as follows:
                #
                #    gradient1   [chainage it applies down to]  gradient2  chainage,
                #    gradient3  chainage3  gradient4
                # or
                #    gradient1, [chainage it applies down to], gradient2, chainage,
                #    gradient3, chainage3, gradient4, chainage4
                #
                # That way of setting things works for me, but some prefer to
                # have a chainage followed by gradient instead:
                #
                #    chainage1   gradient1
                #    chainage2   gradient2
                #    chainage3   gradient3
                #    chainage4   gradient4
                #
                # The first way is the default.  The second is accommodated
                # by giving "begin gradients" line an optional argument
                # that can be set to "switcheroo := true".  Most of the
                # entries were processed at the top of this loop, but we
                # set two values here.
                #
                if switcheroo is True:
                    name2 = "gradients"
                else:
                    name1 = "gradients"

                # Call a subroutine to read the numbers in the block and split
                # them into two lists.  This returns when it encounters an
                # "end <block_name>" line.  We can let the gradients be anything
                # but we want each chainage to be higher than its predecessor.
                result = ProcessNumberList(line_triples, tr_index, settings_dict,
                                           words_low[1], name1, name2,
                                           rule1, rule2, log)
                if result is None:
                    return(None)

                # We have a list of gradients and distances.  Check that all
                # the gradients are between -1 and +1 (fractions).  -1 means
                # vertical down, +1 means vertical up, 0 means flat.
                if switcheroo:
                    (tr_index, chainages, grads_read,
                     ch_lines, grad_lines) = result
                else:
                    (tr_index, grads_read, chainages,
                     grad_lines, ch_lines) = result
                for grad_index, gradient in enumerate(grads_read):
                    mod_grad = gradient * grad_mult
                    if abs(mod_grad) > 1.0:
                        err = ('> Came across faulty input in "'
                                 + file_name + '".\n'
                               '> In route "' + route_name
                                 + '" there was an impossible\n'
                               '> gradient (' + str(gradient)
                                  + ').  Gradients are limited to \n'
                                  + grad_err
                              )
                        gen.WriteError(2306, err, log)
                        # Get the details of the line it is on.

                        (line_number, line_data, line_text) = \
                                    line_triples[grad_lines[grad_index]]
                        gen.ErrorOnLine(line_number, line_text, log, False)
                        return(None)
                    else:
                        # Change the gradient to fractions.
                        grads_read[grad_index] = mod_grad


                # Figure out the elevations from the heights and chainages.
                elevgrad_chs = [start] + chainages
                gradients = [grads_read[0]] + grads_read
                elevations = [start_elev]
                if len(gradients) != len(elevgrad_chs):
                    # The lists of gradients and chainages ended in a
                    # gradient, meaning that the user wanted that gradient
                    # to extend to infinity.  We extend it just over the
                    # length of the longest train type, so that when the
                    # tail of the train reaches the last chainage in the
                    # input file, the nose isn't beyond the definition
                    # of the elevation.
                    elevgrad_chs.append(elevgrad_chs[-1] + max_trlen)
                # Now calculate the elevations at each of the chainages.
                for index in range(1, len(gradients)):
                    delta_ch = elevgrad_chs[index] - elevgrad_chs[index - 1]
                    new_elevation = elevations[-1] + gradients[index] * delta_ch
                    elevations.append(new_elevation)
                # Note the fact that we read a gradients block.  If an
                # elevations block turns up, it triggers error 2305.
                profile = "gradients"
            else:
                # We want a list of chainages and elevations.
                # Store the index of the line with "begin elevations" on
                # it and move to the next line.
                prof_index = tr_index
                tr_index += 1

                # Call a subroutine to read the numbers in the block and split
                # them into two lists.  This returns when it encounters an
                # "end <block_name>" line.  We can let the elevations be
                # anything, but we want each chainage to be higher than its
                # predecessor.   Note that if the "switcheroo := True" optional
                # argument was included in "begin elevations" command it is
                # ignored, I see no need for it.
                result = ProcessNumberList(line_triples, tr_index, settings_dict,
                                           words_low[1],
                                           "chainages", "elevations",
                                           ">", "", log)
                if result is None:
                    return(None)
                else:
                    (tr_index, chainages, elevations, ch_lines, elev_lines) = result
                # Figure out the gradients from the heights and chainages.
                elevgrad_chs = [start] + chainages
                elevations = [start_elev] + elevations
                gradients = []
                for index in range(1, len(elevations)):
                    delta_ch = elevgrad_chs[index] - elevgrad_chs[index - 1]
                    delta_h = elevations[index] - elevations[index - 1]
                    new_gradient =  delta_h / delta_ch
                    # Check if the change in vertical distance exceeds the
                    # change in horizontal distance and fault if it does.
                    # We use a math.isclose check here as well as a greater
                    # than check to reduce the chance of floating point
                    # rounding triggering this error.
                    if (abs(delta_h) > abs(delta_ch)) and   \
                        not(math.isclose(delta_ch, delta_h)):
                        err = ('> Came across faulty input in "' + file_name + '".\n'
                               '> In route "' + route_name + '" there was a\n'
                               '> "begin elevations" block.  The difference\n'
                               '> in height between two points exceeded their\n'
                               '> distance.  Details are:\n'
                               '>   * elevation ' + str(elevations[index - 1])
                                  + ' at chainage ' + str(elevgrad_chs[index - 1])
                                  + ' and\n'
                               '>   * elevation ' + str(elevations[index])
                                  + ' at chainage ' + str(elevgrad_chs[index])
                                  + '\n'
                               '> These combine to give an impossible value of\n'
                               '> gradient (' + str(round(new_gradient, 8)) + ').\n'
                               '> Please edit the file to get the absolute\n'
                               '> value of the gradient to be below 1.'
                              )
                        gen.WriteError(2307, err, log)
                        # Figure out which line(s) the four numbers are on and
                        # print them after the error message.
                        if index == 1:
                            # The first chainage is in the origin line.  Get
                            # the index in line triples of where it was set.
                            source = new_route_dict["origin"][2]
                            line1_num, discard, line1_text = line_triples[source]
                            # The first elevation was either zero or was
                            # set in the first line.  Set it to the same.
                            line2_num = line1_num
                            line2_text = line1_text
                        else:
                            source1 = ch_lines[index - 2]
                            line1_num, discard, line1_text = line_triples[source1]
                            source2 = elev_lines[index - 2]
                            line2_num, discard, line2_text = line_triples[source2]
                        source3 = ch_lines[index - 1]
                        line3_num, discard, line3_text = line_triples[source3]
                        source4 = elev_lines[index - 1]
                        line4_num, discard, line4_text = line_triples[source4]

                        # Call a routine that will figure out how many lines
                        # to print after the error message (it could be one,
                        # it could be four.
                        gen.ErrorOnManyLines(line1_num, line1_text,
                                             line2_num, line2_text,
                                             line3_num, line3_text,
                                             line4_num, line4_text,
                                              log, False)
                        return(None)
                    else:
                        gradients.append(new_gradient)
                        if index == 0:
                            # As this is the first gradient, we append the
                            # new gradient twice.
                            gradients.append(new_gradient)
                # Note the fact that we read an elevations block.  If a
                # gradients block turns up, it triggers error 2305.
                profile = "elevations"
        elif words_low[:2] == ["begin", "speedlimits"]:
            # We want a list of speeds and chainages they apply up to.
            # This works for both road and rail tunnels.
            spd_index = tr_index
            tr_index += 1
            if switcheroo is True:
                name2 = "speedlimits"
            else:
                name1 = "speedlimits"

            # Call a subroutine to read the numbers in the block and split
            # them into two lists.  This returns when it encounters an
            # "end <block_name>" line.
            result = ProcessNumberList(line_triples, tr_index, settings_dict,
                                       words_low[1], name1, name2,
                                       rule1, rule2, log)
            if result is None:
                return(None)

            if switcheroo:
                (tr_index, chainages, speeds,
                 ch_lines, sp_lines) = result
            else:
                (tr_index, speeds, chainages,
                 sp_lines, ch_lines) = result

            # Check that none of the speed limits are zero, which
            # would stop all traffic.
            for sp_index, speed in enumerate(speeds):
                if speed < 0.0 or math.isclose(speed, 0.0, abs_tol = 1e-9):
                    tr_index = sp_lines[sp_index]
                    line_num, line_data, line_text = line_triples[tr_index]
                    err = ('> Came across faulty input in "'
                             + file_name + '".\n'
                           '> In route "' + route_name + '" there was a\n'
                           '> block of speed limits, one of which was\n'
                           '> zero or negative.  Please edit the file\n'
                           '> to set all speed limits above zero.'
                          )
                    gen.WriteError(2314, err, log)
                    gen.ErrorOnLine(line_number, line_text, log, False)
                    return(None)
            # Check that the points at which the speed limits change
            # are not less than 10 m apart.
            result = CheckRouteChs(chainages, ch_lines, "speed limits",
                                   "speed", file_name, route_name,
                                   line_triples,units, log)
            if result is None:
                return(None)
            # Add the pair of lists of speed limits and chainages to
            # the route dictionary, along with tr_index (for an error
            # message.
            chainages.append(math.inf)
            speeds.append(speeds[-1])
            new_route_dict.__setitem__("speedlimits", (speeds, chainages,
                                                       spd_index))
        elif words_low[:2] == ["begin", "lanes"]:
            # We want a list of lane counts and the chainages they apply
            # up to.  This works for road tunnels only.  It is used when
            # modelling stationary traffic, as traffic is defined as
            # a given number of veh/lane-km and the count of lanes
            # can change along the length of a route.

            result = Process8CLists(line_triples, tr_index, settings_dict,
                                    "lanes", new_route_dict, route_name,
                                    switcheroo, units, log)
            if result is None:
                return(None)
            else:
                (tr_index, new_route_dict) = result
        elif words_low[:2] == ["begin", "radii"]:
            # We want a list of track radii and the chainages they apply
            # up to.  This is for train performance calculations in SES;
            # and the values are used to build SES input form 8C and
            # written to a new SES input file if there is an "SESdata"
            # block in the input file.
            result = Process8CLists(line_triples, tr_index, settings_dict,
                                    "radii", new_route_dict, route_name,
                                    switcheroo, units, log)
            if result is None:
                return(None)
            else:
                (tr_index, new_route_dict) = result
        elif words_low[:2] == ["begin", "sectors"]:
            # We want a list of energy sectors and the chainages they apply
            # up to.  This is for train performance calculations in SES;
            # and the values are used to build SES input form 8C and
            # written to a new SES input file if there is an "SESdata"
            # block in the input file.
            result = Process8CLists(line_triples, tr_index, settings_dict,
                                    "sectors", new_route_dict, route_name,
                                    switcheroo, units, log)
            if result is None:
                return(None)
            else:
                (tr_index, new_route_dict) = result
        elif words_low[:2] == ["begin", "coasting"]:
            # We want a list of coasting rules and the chainages they apply
            # up to.  This is for train performance calculations in SES;
            # and the values are used to build SES input form 8C and
            # written to a new SES input file if there is an "SESdata"
            # block in the input file.
            result = Process8CLists(line_triples, tr_index, settings_dict,
                                    "coasting", new_route_dict, route_name,
                                    switcheroo, units, log)
            if result is None:
                return(None)
            else:
                (tr_index, new_route_dict) = result
        elif words_low[:2] == ["begin", "regenfractions"]:
            # We want a list of regenerative braking fractions and the
            # chainages they apply up to.  This is for train performance
            # calculations in SVS (not SES; just SVS).  The values are
            # used to build SVS input form 8C and written to a new SVS
            # input file if there is an "SESdata" block in the input file.
            result = Process8CLists(line_triples, tr_index, settings_dict,
                                    "regenfractions", new_route_dict,
                                    route_name, switcheroo, units, log)
            if result is None:
                return(None)
            else:
                (tr_index, new_route_dict) = result
        elif words_low[:2] == ["begin", "schedule"]:
            # Get the dictionary key we will use for this schedule.
            sch_name = words_low[2]
            result = ProcessSchedule(line_triples, tr_index, settings_dict,
                                     trtypes_dict, route_name, sch_name,
                                     schedules_dict, log)
            if result is None:
                return(None)
            else:
                (tr_index, schedules_dict) = result
        elif words_low[0] in done_already:
            # We've already processed this line in ProcessBlock.
            pass
        else:
            # We have encountered a word that is not a keyword or the
            # name of a tunnel in the "begin tunnels" block. Complain.
            # We have a special for lines beginning with "tunnels" or
            # "tunnel" because I keep using it!
            err = ('> Came across faulty input in "' + file_name + '".\n'
                   '> In route "' + route_name + '" there was a line starting\n'
                   '> with an unrecognised keyword, "' + words[0] + '".')
            if words_low[0] in ("tunnel", "tunnels"):
                err = err + ('  Note that\n'
                   '> trying a "' + words[0]
                      + '" keyword was a good guess, but\n'
                   '> you need to use a "begin tunnels...end tunnels"\n'
                   '> sub-block in the route definition.')
            else:
                err = err + '\n'
            err = err + (
                   '> Please edit the file to correct or remove the\n'
                   '> entry.'
                    )
            gen.WriteError(2308, err, log)
            gen.ErrorOnLine(line_number, line_text, log, False)
            return(None)
        # else:
        #     # We can only get here if we create a dangling else
        #     # during programming work.
        #     print("Failed to catch a new branch in PROC ProcessRoute.")
        #     gen.OopsIDidItAgain(log, file_name)
        #         return(None)

    # If we get to here we successfully traversed the while loop.
    # Now check if there were any tunnels in the route (we can't tell
    # from ProcessBlock, alas, because it only checks for "begin").
    if tunnel_names == []:
        err = ('> Came across missing input in "' + file_name + '".\n'
               '> In route "' + route_name + '" there was no\n'
               '> "begin tunnels...end tunnels" block, which\n'
               '> much makes the route pretty much useless.\n'
               '> Please edit the file to remove the route, or\n'
               '> add a "begin tunnels...end tunnels" block\n'
               '> to the route definition.'
              )
        gen.WriteError(2309, err, log)
        (line_number, line_data, line_text) = line_triples[start_index]
        gen.ErrorOnLine(line_number, line_text, log, False, False, "Relevant")
        return(None)

    # Check if gradients were defined and if not, set them to zero. Set
    # the end of the route to just beyond the down end of the tunnels,
    # so that the trains vanish from the route after leaving the tunnel.
    if profile == "none":
        # Set the default gradients and heights.  We use "discard" as the
        # first entry in the gradients because the gradient up to the origin
        # chainage is undefined and must not be used.  If we use it, the
        # string causes a type mismatch that will alert us.
        # We set the down end of the route just beyond the down end of
        # the tunnel complex.
        elevgrad_chs = [start, tunnel_chs[-1] + max_trlen]
        gradients = [0.0, 0.0]
        elevations = [start_elev] * 2
        # We set the index to line_triples to math.nan so we know that
        # the profile has not been set in the input file.
        prof_index = math.nan

    # Get the minimum and maximum chainages, which we use as
    default_chs = [start, elevgrad_chs[-1]]
    if "speedlimits" not in new_route_dict:
        # Set an entry for the speed limit plot data that indicates
        # that user didn't set one (zero values from the origin to
        # beyond the down end of the tunnels).
        new_route_dict.__setitem__("speedlimits", "Not set")
        # Spoof the speed limits as zero for plotting.
        new_route_dict.__setitem__("speed_chs", default_chs)
        new_route_dict.__setitem__("speed_plots", [0.0, 0.0])
    else:
        speeds, chs, discard = new_route_dict["speedlimits"]
        # Discard any settings that occur before the route origin
        chs, speeds = ClipProfile(chs, speeds, start)
        # Set an entry for the speed limit plot data that indicates
        # that user didn't set one (zero values from the origin to
        # beyond the down end of the tunnels).
        new_route_dict.__setitem__("speed_chs", [start] + chs[:-1])
        new_route_dict.__setitem__("speed_plots", speeds)

    if "lanes" not in new_route_dict:
        # Set an entry for the lane count plot data that indicates
        # that user didn't set one (zero values from the origin to
        # beyond the down end of the tunnels).
        new_route_dict.__setitem__("lane_chs", default_chs)
        new_route_dict.__setitem__("lane_plots", [0, 0])
    else:
        lanes, chs, discard = new_route_dict["lanes"]
        # Discard any settings that occur before the route origin
        chs, lanes = ClipProfile(chs, lanes, start)
        # Get suitable lane counts for plotting.  The last entry in
        # 'chs' is math.inf, which isn't practical when plotting.  We
        # also prepend the origin of the route.
        new_route_dict.__setitem__("lane_chs", [start] + chs[:-1])
        new_route_dict.__setitem__("lane_plots", lanes)

    if "radii" not in new_route_dict:
        # Set an entry for the track radius plot data that indicates
        # that user didn't set one (zero values from the origin to
        # beyond the down end of the tunnels).
        new_route_dict.__setitem__("radii_chs", default_chs)
        new_route_dict.__setitem__("radii_plots", [0, 0])
        new_route_dict.__setitem__("radii", ([0, 0], default_chs, math.nan))
    else:
        radii, chs, discard = new_route_dict["radii"]
        # Discard any settings that occur before the route origin
        chs, radii = ClipProfile(chs, radii, start)
        # Get suitable track radii for plotting.  The last entry in
        # 'chs' is math.inf, which isn't practical when plotting.  We
        # also prepend the origin of the route.
        new_route_dict.__setitem__("radii_chs", [start] + chs)
        new_route_dict.__setitem__("radii_plots", radii)

    if "sectors" not in new_route_dict:
        # Set an entry for the track sector plot data that indicates
        # that user didn't set one (energy sector 1 from the origin to
        # beyond the down end of the tunnels).
        new_route_dict.__setitem__("sectors_chs", default_chs)
        new_route_dict.__setitem__("sectors_plots", [1, 1])
        new_route_dict.__setitem__("sectors", ([1, 1], default_chs, math.nan))
    else:
        sectors, chs, discard = new_route_dict["sectors"]
        # Discard any settings that occur before the route origin
        chs, sectors = ClipProfile(chs, sectors, start)
        # Get suitable track sectors for plotting.  The last entry in
        # 'chs' is math.inf, which isn't practical when plotting.  We
        # also prepend the origin of the route.
        new_route_dict.__setitem__("sectors_chs", [start] + chs)
        new_route_dict.__setitem__("sectors_plots", sectors)

    if "coasting" not in new_route_dict:
        # Set an entry for the coasting plot data that indicates
        # that user didn't set one (zero values from the origin to
        # beyond the down end of the tunnels).
        new_route_dict.__setitem__("coasting_chs", default_chs)
        new_route_dict.__setitem__("coasting_plots", [0, 0])
        new_route_dict.__setitem__("coasting", ([0, 0], default_chs, math.nan))
    else:
        coasting, chs, discard = new_route_dict["coasting"]
        # Discard any settings that occur before the route origin
        chs, coasting = ClipProfile(chs, coasting, start)
        # Get suitable coasting for plotting.  The last entry in
        # 'chs' is math.inf, which isn't practical when plotting.  We
        # also prepend the origin of the route.
        new_route_dict.__setitem__("coasting_chs", [start] + chs)
        new_route_dict.__setitem__("coasting_plots", coasting)

    if "regenfractions" not in new_route_dict:
        # Set an entry for the track regenerative braking fractions plot
        # data that indicates that user didn't set one (zero values from
        # the origin to beyond the down end of the tunnels).
        new_route_dict.__setitem__("regenfractions_chs", default_chs)
        new_route_dict.__setitem__("regenfractions_plots", [0, 0])
        new_route_dict.__setitem__("regenfractions", ([0, 0], default_chs,
                                   math.nan))
    else:
        regenfractions, chs, discard = new_route_dict["regenfractions"]
        # Discard any settings that occur before the route origin
        chs, regenfractions = ClipProfile(chs, regenfractions, start)
        # Get suitable regen braking fractions for plotting.  The last
        # entry in 'chs' is math.inf, which isn't practical when
        # plotting.  We also prepend the origin of the route.
        new_route_dict.__setitem__("regenfractions_chs", [start] + chs)
        new_route_dict.__setitem__("regenfractions_plots", regenfractions)

    # Add the lists of the gradients and elevations to the dictionary.
    new_route_dict.__setitem__("gradients2", gradients[1:])
    new_route_dict.__setitem__("elevations", elevations)
    new_route_dict.__setitem__("elevgrad_chs", elevgrad_chs)
    # Add an entry that we can call easily from PropsInTunnel.
    new_route_dict.__setitem__("gradients", (gradients, elevgrad_chs))
    # We save a note about how the profiles were set: this will be "gradients"
    # "elevations" or "none" (if none were set and the defaults were used).
    # We also save the index of the "begin gradients" line or the "begin
    # elevations" line (or NaN if no profile was set by the user).
    # This is used when recreating input files and in error messages.
    new_route_dict.__setitem__("profile", (profile, prof_index))
    # Save the dictionary of schedules, if needed.
    if schedules_dict != {}:
        new_route_dict.__setitem__("schedules", schedules_dict)
    if debug1:
        print("The names of all tunnels in the route are valid.")

    # Check each train schedule to see if the user set the location where
    # trains pop into existence (the trains' start points).  If they did
    # not, set the start point to the route origin.  If they did, check if
    # the start point is between the start and end of the route and complain
    # if it is not.
    # We can't do this check in the routine that reads the schedules because
    # there is no guarantee that we will read the route gradients before
    # reading the schedule, so we don't know where the route ends.  We could
    # check for start points before the origin if we wanted to but it's
    # easier to do it here.
    # We also check that if the trains are moving, they spawn outside
    # the tunnel system.  This is because the spawn times of trains
    # could be half-way between time intervals and this makes figuring
    # out the train end gridpoints at the previous timestep a chore.
    # Much easier if the trains spawn outside the tunnels or are
    # stationary when they spawn inside the tunnels.

    for sch_name in schedules_dict:
        sch_dict = schedules_dict[sch_name]
        start_ch = elevgrad_chs[0]
        stop_ch = elevgrad_chs[-1]
        if "downstart" not in sch_dict:
            # Set the trains to start at the up end origin of the route.
            spawn_ch = start
        else:
            spawn_ch, discard, err_index = sch_dict["downstart"]
            # Create the text we may need in error messages.
            if units == "us":
                # Turn the numbers we want to complain about from
                # metres to feet.
                start_txt = gen.RoundText(start_ch / 0.3048, 3) + " ft"
                stop_txt = gen.RoundText(stop_ch / 0.3048, 3) + " ft"
                spawn_txt = gen.RoundText(spawn_ch / 0.3048, 3) + " ft"
            else:
                start_txt = gen.RoundText(start_ch, 3) + " m"
                stop_txt = gen.RoundText(stop_ch, 3) + " m"
                spawn_txt = gen.RoundText(spawn_ch, 3) + " m"
            if spawn_ch < start_ch:
                err = ('> Came across faulty input in "' + file_name + '".\n'
                       '> In route "' + route_name + '", train schedule "'
                         + sch_name[5:] + '"\n'
                       '> starts trains at '
                         + spawn_txt + ', which is before the\n'
                       '> route origin.  Please change it so that the trains\n'
                       '> start at or between '
                         +  start_txt + ' and ' +  stop_txt + '.')
                gen.WriteError(2310, err, log)
                gen.ErrorOnLine2(err_index, line_triples, log, False)
                return(None)
            elif spawn_ch > stop_ch:
                err = ('> Came across faulty input in "' + file_name + '".\n'
                       '> In route "' + route_name + '", train schedule "'
                         + sch_name[5:] + '"\n'
                       '> starts trains at '
                         + spawn_txt + ', which is so far\n'
                       '> down the route that the alignment is undefined.\n'
                       '> Please change it so that the trains start at\n'
                       '> or between '
                         +  start_txt + ' and ' +  stop_txt + '.')
                gen.WriteError(2311, err, log)
                gen.ErrorOnLine2(err_index, line_triples, log, False)
                return(None)
            # Now get the train's initial speed.


        # Store the chainage that trains spawn at in the dictionary.
        sch_dict.__setitem__("spawn_ch", spawn_ch)
        schedules_dict.__setitem__(sch_name, sch_dict)

    # Create a list to hold the offsets and multipliers needed to
    # turn a route chainage into a distance along the tunnel.
    # We multiply the route chainage by its multiplier then add its
    # offset to get the distance along the tunnel.
    route2tun = []
    # Create a list to hold the tunnel distances in the order they
    # appear in the route.  This included because otherwise we'd need
    # to recreate it later in the moving traffic routine.
    tunnel_dists = []
    # Now check that the tunnels connect to each other at nodes.  We
    # don't allow routes to connect at joins, because that is too
    # difficult.  We might add that one day though, to be able to plot
    # conditions along escape routes.
    # Make a list of alternate tunnel names.
    if len(tunnel_names) == 1:
        # We preserve the state of the minus sign in the input file (if
        # any) for further processing of the route.
        signed_names = tunnel_names
        # Get the correlation between the route chainages and the
        # tunnel distances.
        up_ch, down_ch = tunnel_chs

        tun1_name = tunnel_names[0]
        if tun1_name[0] == '-':
            tun1_name = tun1_name[1:]
        tun1_data = tunnels_dict[tun1_name]
        tun1_back = tun1_data["back"]
        tun1_fwd = tun1_data["fwd"]
        if tunnel_names[0][0] == "-":
            # The tunnel is reversed in the route.
            multiplier, offset = MultOffset(tun1_fwd, tun1_back,
                                            up_ch, down_ch)
            tunnel_dists.append((tun1_fwd[0], tun1_back[0]))
        else:
            multiplier, offset = MultOffset(tun1_back, tun1_fwd,
                                            up_ch, down_ch)
            tunnel_dists.append((tun1_back[0], tun1_fwd[0]))
        route2tun.append((multiplier, offset))
    else:
        # We build a list of alternate names that signal the orientation
        # of the tunnels in the route.
        # Where a tunnel is reversed in the route, its name starts with
        # a minus sign.  Where a tunnel is not, it does not start with
        # a minus sign.  This holds regardless of whether there was a
        # minus sign before  the name in the input file.
        signed_names = []

        for index, tun1_name in enumerate(tunnel_names[:-1]):
            tun2_name = tunnel_names[index + 1]
            # Get the details of the tunnel and figure out the correct
            # orientation.  We ignore any minus signs prepended to the
            # tunnel names because we can figure out the orientation
            # from the way the nodes are laid out.  The minus sign is
            # only needed before the tunnel name in routes with one
            # tunnel.
            if tun1_name[0] == "-":
                tun1_name = tun1_name[1:]
            if tun2_name[0] == "-":
                tun2_name = tun2_name[1:]
            tun1_data = tunnels_dict[tun1_name]
            tun2_data = tunnels_dict[tun2_name]

            # Get the tunnel data from the dictionaries.  We pick up the
            # line numbers and line text too, for error messages.
            tun1_back = tun1_data["back"]
            tun1_fwd = tun1_data["fwd"]

            tun2_back = tun2_data["back"]
            tun2_fwd = tun2_data["fwd"]

            # Check the types of the tunnel ends and fault if one of them
            # is not a node.
            result = CheckForPortals(tun1_back, tun1_fwd, tunblock_index,
                                  line_triples, file_name, route_name,
                                  tun1_name, log)
            if result is None:
                return(None)
            else:
                (t1_backtype, t1_backvalue, t1_fwdtype, t1_fwdvalue) = result
            result = CheckForPortals(tun2_back, tun2_fwd, tunblock_index,
                                  line_triples, file_name, route_name,
                                  tun2_name, log)
            if result is None:
                return(None)
            else:
                (t2_backtype, t2_backvalue, t2_fwdtype, t2_fwdvalue) = result

            # If we get here, one or both ends of both tunnels have
            # a node.  Get a list of the names of the nodes and store
            # their tr_indices (in case we need them for error messages)
            nodes = []
            tr1_indices = []
            tr2_indices = []

            if t1_backtype == "node":
                nodes.append(t1_backvalue.lower())
                tr1_indices.append(tun1_back[-1])
            if t1_fwdtype == "node":
                nodes.append(t1_fwdvalue.lower())
                tr1_indices.append(tun1_fwd[-1])
            if t2_backtype == "node":
                nodes.append(t2_backvalue.lower())
                tr2_indices.append(tun2_back[-1])
            if t2_fwdtype == "node":
                nodes.append(t2_fwdvalue.lower())
                tr2_indices.append(tun2_fwd[-1])
            # We now have a list of two, three or four node names.  Count
            # how many pairs we have: none (bad), one (good) or two (bad).
            pairs = []
            for name in nodes:
                count = nodes.count(name)
                if count > 1 and name not in pairs:
                    pairs.append(name)
            # Fault if there are no common node names or if there are two
            # pairs of common nodes.
            if len(pairs) == 0:
                # There are no common nodes.  We want to raise an error
                # message complaining about three, four or five lines of
                # input, depending on how many tunnel ends are at nodes.
                # Get a list of all the tr_indices that point to node
                # definitions.
                tr_indices = tr1_indices + tr2_indices
                (line1_number, discard, line1_text) = line_triples[tunblock_index]
                (line2_number, discard, line2_text) = line_triples[tr_indices[0]]
                (line3_number, discard, line3_text) = line_triples[tr_indices[1]]
                err = ('> Came across faulty input in "' + file_name + '".\n'
                       '> In route "' + route_name + '", the list of tunnels\n'
                       '> included two tunnels that do not have a common\n'
                       '> node (tunnels "' + tun1_name + '" and "'
                         + tun2_name + '").\n'
                       '> Please either edit the list of tunnels to give \n'
                       '> tunnels with a common node or edit the definitions\n'
                       '> of the tunnel ends to give them a common node.'
                      )
                gen.WriteError(2312, err, log)
                if len(tr_indices) == 2:
                    gen.ErrorOnThreeLines(line1_number, line1_text,
                                          line2_number, line2_text,
                                          line3_number, line3_text,
                                          log, False, False, "Relevant")
                elif len(tr_indices) == 3:
                    (line4_number, discard, line4_text) = line_triples[tr_indices[2]]
                    gen.ErrorOnFourLines(line1_number, line1_text,
                                          line2_number, line2_text,
                                          line3_number, line3_text,
                                          line4_number, line4_text,
                                          log, False, False, "Relevant")
                elif len(tr_indices) == 4:
                    (line4_number, discard, line4_text) = line_triples[tr_indices[2]]
                    (line5_number, discard, line5_text) = line_triples[tr_indices[3]]
                    gen.ErrorOnFiveLines(line1_number, line1_text,
                                         line2_number, line2_text,
                                         line3_number, line3_text,
                                         line4_number, line4_text,
                                         line5_number, line5_text,
                                          log, False, False, "Relevant")
                return(None)
            elif len(pairs) == 2:
                # There are two pairs of common nodes.
                # We want to raise an error message complaining about five
                # lines of input.
                (line1_number, discard, line1_text) = line_triples[tunblock_index]
                (line2_number, discard, line2_text) = line_triples[tr1_indices[0]]
                (line3_number, discard, line3_text) = line_triples[tr1_indices[1]]
                (line4_number, discard, line4_text) = line_triples[tr2_indices[0]]
                (line5_number, discard, line5_text) = line_triples[tr2_indices[1]]
                err = ('> Came across faulty input in "' + file_name + '".\n'
                       '> In route "' + route_name + '", the list of tunnels\n'
                       '> included two tunnels that have two pairs of common\n'
                       '> nodes (tunnels "' + tun1_name + '" and "'
                         + tun2_name + '").\n'
                       '> This is not allowed because it is too difficult\n'
                       '> to figure out which way round they are in the\n'
                       '> route.  Please either edit the list of tunnels\n'
                       '> to give tunnels with one common node, or edit the\n'
                       '> definitions of the tunnel ends to give them one\n'
                       '> common node.\n'
                      )
                gen.WriteError(2313, err, log)
                gen.ErrorOnFiveLines(line1_number, line1_text,
                                     line2_number, line2_text,
                                     line3_number, line3_text,
                                     line4_number, line4_text,
                                     line5_number, line5_text,
                                     log, False, False, "Relevant")
                return(None)

            # If we get to here, all is well.  Note that we allow routes
            # to pass through the same node twice, as long as they don't
            # try to go through the same tunnels again.
            # Add the names of the tunnels to the list of alternative
            # tunnel names, prepending a minus sign if we need to.

            # If this is the first tunnel, figure out which way round
            # it is.  We also get the correlation between the route
            # chainages and the tunnel distances.
            up_ch, down_ch = tunnel_chs[index:index + 2]

            nodename = pairs[0]
            if index == 0:
                if t1_fwdtype == "node" and nodename == t1_fwdvalue.lower():
                    signed_names.append(tun1_name)
                    # Get the offset and multiplier needed to turn a
                    # route chainage that lies inside this tunnel into
                    # a distance along the tunnel.
                    multiplier, offset = MultOffset(tun1_back, tun1_fwd,
                                                    up_ch, down_ch)
                    tunnel_dists.append((tun1_back[0], tun1_fwd[0]))
                else:
                    # The first tunnel is in the route backwards.
                    signed_names.append("-" + tun1_name)
                    multiplier, offset = MultOffset(tun1_fwd, tun1_back,
                                                    up_ch, down_ch)
                    tunnel_dists.append((tun1_fwd[0], tun1_back[0]))
                # Store the data for the first tunnel
                route2tun.append((multiplier, offset))
            # Figure out which way round the next tunnel is.
            up_ch, down_ch = tunnel_chs[index + 1:index + 3]
            if t2_backtype == "node" and nodename == t2_backvalue.lower():
                signed_names.append(tun2_name)
                multiplier, offset = MultOffset(tun2_back, tun2_fwd,
                                                up_ch, down_ch)
                tunnel_dists.append((tun2_back[0], tun2_fwd[0]))
            else:
                signed_names.append("-" + tun2_name)
                multiplier, offset = MultOffset(tun2_fwd, tun2_back,
                                                up_ch, down_ch)
                tunnel_dists.append((tun2_fwd[0], tun2_back[0]))
            # Store the data for this tunnel.
            route2tun.append((multiplier, offset))
    new_route_dict.__setitem__("route2tun", route2tun)
    new_route_dict.__setitem__("signed_names", signed_names)
    new_route_dict.__setitem__("tunnel_dists", tunnel_dists)

    # Now replace the list returned by the key "tunnel_names" with a
    # list of tunnel names with all minus signs removed.  We have a
    # list of the tunnel names with minus signs included (they
    # indicate that the tunnel is reversed in the route).
    mod_names = []
    for name in new_route_dict["tunnel_names"]:
        if name[0] == '-':
            mod_names.append(name[1:])
        else:
            mod_names.append(name)
    new_route_dict.__setitem__("tunnel_names", mod_names)


    # Print the contents of the route dictionaries and subdictionaries.
    if False:  # print dicts xxxx
        print(route_name)
        for key in new_route_dict:
            if key in ("schedules", ):
                # The result of this is a sub-dictionary.
                for sch_name in new_route_dict[key]:
                    print("  ", sch_name)
                    sch_dict = new_route_dict[key][sch_name]
                    for key2 in sch_dict:
                        print("     ", key2, sch_dict[key2])
            else:
                print("  ", key, new_route_dict[key])
    # Write the files and their nicknames to the log file for the
    # record.
    gen.LogBlock(new_route_dict, "route " + route_name, debug1, log)
    # gen.LogBlock(block_dict, block_name + " " + entity_name, debug1, log)
    return(route_name, new_route_dict)


def ClipProfile(chs, values, start):
    mod_chs = []
    mod_values = []
    for ch, value in zip(chs, values):
        if ch > start:
            mod_chs.append(ch)
            mod_values.append(value)
    return(mod_chs, mod_values)


def Process8CLists(line_triples, tr_index, settings_dict, key,
                      new_route_dict, route_name, switcheroo, units, log):
    '''Process a sub-block in a route definition that reads integer
    values.  This was originally written to read counts of lanes as
    an 'elif' block inside PROC ProcessRoute.  It was split out into
    a procedure so that it can be used to read counts of lanes, SES
    energy sector numbers and SES coasting switches, as they are
    similar.
    '''
    file_name = settings_dict["file_name"]
    tunnel_chs = new_route_dict["tunnel_chs"]

    # Set the arguments we send to ProcessNumberList.
    if switcheroo is True:
        rule1 = ">"   # chainage
        rule2 = ""
        name1 = "chainages"
        name2 = key
    else:
        rule1 = ""
        rule2 = ">"   # chainage
        name1 = key
        name2 = "chainages"

    # Customise the text of the error messages to the block we are
    # reading.
    if key == "lanes":
        # Set the count of lanes for stationary traffic calculations.
        descrip1 = 'count of lanes'
        descrip2 = 'lane \n> counts,'
        minval = 1
        maxval = math.inf
        integers = True
    elif key == "radii":
        # This applies to SES input files only.  It is the track radii,
        # to set in form 8C, with zero being straight track.
        descrip1 = 'track radii'
        descrip2 = 'track \n> radii,'
        minval = 0
        maxval = math.inf
        integers = False
    elif key == "sectors":
        # This applies to SES input files only.  It is the energy sectors
        # to assign pieces of track to in form 8C.
        descrip1 = 'energy sectors'
        descrip2 = 'energy \n> sectors,'
        minval = 0
        maxval = math.inf
        integers = True
    elif key == "coasting":
        descrip1 = 'coasting switches'
        descrip2 = 'coasting \n> switches,'
        minval = 0
        maxval = 1
        integers = True
    elif key == "regenfractions":
        # This applies to SVS input files only.  It is route-based
        # regenerative braking fraction.
        descrip1 = 'regen fractions'
        descrip2 = 'regen \n> fractions,'
        minval = 0
        maxval = 1
        integers = False
    # elif key == "heatgains":
    else:
        # This is heat gains per unit length.  These are turned into
        # fixed heat gains in form 3D in the line segments that a
        # route passes through.  These types of entry originate in
        # the "SESdata" blocks instead of the "route" blocks, so that
        # different "SESdata" blocks can set different heat gains in
        # their respective SES input files.
        descrip1 = 'heat gains'
        descrip2 = 'heat \n> gains,'
        minval = -math.inf
        maxval = math.inf
        integers = False

    # Store the index of the line with "begin " <keyword> entry
    # for the error message for duplicate blocks.
    tr_start = tr_index

    # Read the numbers in the block.
    result = ProcessNumberList(line_triples, tr_index + 1,
                               settings_dict, key,
                               name1, name2, rule1, rule2, log)
    if result is None:
        return(None)
    elif switcheroo:
        (tr_index, chainages, values, ln_lines, ch_lines) = result
    else:
        (tr_index, values, chainages, ln_lines, ch_lines) = result
    # Check the ranges and insist on integers in some cases.
    for index, value in enumerate(values):
        if value < minval:
            tr_index = ln_lines[index]
            line_num, line_data, line_text = line_triples[tr_index]
            if key == "lanes":
                # The count of lanes has a minimum of 1 and be integers.
                text1 = 'one ('+ str(value) + ').\n'
                text2 = 'integers of one or more.'
            elif key == "sectors":
                # The energy sectors should be 0 or above and be integers.
                text1 = 'zero ('+ str(value) + ').\n'
                text2 = 'integer values zero or above.'
            elif key == "coasting":
                # The coasting parameter should be 0 or 1.
                text1 = 'zero ('+ str(value) + ').\n'
                text2 = 'integer values zero or one.'
            elif key == "regenfractions":
                # The regen fraction can't be less than zero.
                text1 = 'zero ('+ str(value) + ').\n'
                text2 = 'real numbers between zero and one.'
            else:
                # This is heat gains.  We can't get here because the
                # minimum is -math.inf, but that could change if
                # someone alters the limits.
                text1 = '-math.inf ('+ str(value) + ').\n'
                text2 = 'real numbers between -math.inf and\n> +math.inf.'
            err = ('> Came across faulty input in "'
                     + file_name + '".\n'
                   '> In route "' + route_name
                     + '" there was a block of ' + descrip2
                     + ' one of which was less than ' + text1 +
                   '> Please edit the file to set the ' + descrip1 + '\n'
                   '> to be ' + text2
                  )
            gen.WriteError(3121, err, log)
            gen.ErrorOnLine(line_num, line_text, log, False)
            return(None)
        elif value > maxval:
            tr_index = ln_lines[index]
            line_num, line_data, line_text = line_triples[tr_index]
            if key in ("lanes", "sectors"):
                # The count of lanes and the energy sector entry have
                # no maximum.  We spoof the message, as we can never
                # actually get here (I think).
                text1 = 'math.inf ('+ str(value) + ').\n'
                text2 = 'integers of one or more.'
            elif key == "coasting":
                # The coasting parameter should be 0 or 1
                text1 = 'one ('+ str(value) + ').\n'
                text2 = 'integer values zero or one.'
            elif key == "regenfractions":
                # The coasting parameter should be 0 or 1 and the
                # regen fraction can't be less than zero.
                text1 = 'one ('+ str(value) + ').\n'
                text2 = 'real numbers between zero and one.'
            else:
                # This is heat gains.  We can't get here because the
                # maximum is +math.inf, but that could change if
                # someone changes the limits.
                text1 = '+math.inf ('+ str(value) + ').\n'
                text2 = 'real numbers between -math.inf and\n> +math.inf.'
            err = ('> Came across faulty input in "'
                     + file_name + '".\n'
                   '> In route "' + route_name
                     + '" there was a block of ' + descrip2
                     + ' one of which was more than ' + text1 +
                   '> Please edit the file to set the ' + descrip1 + '\n'
                   '> to be ' + text2
                  )
            gen.WriteError(3122, err, log)
            gen.ErrorOnLine(line_num, line_text, log, False)
            return(None)
        elif integers and not math.isclose(value, int(value)):
            tr_index = ln_lines[index]
            line_num, line_data, line_text = line_triples[tr_index]
            err = ('> Came across faulty input in "'
                     + file_name + '".\n'
                   '> In route "' + route_name
                     + '" there was a block of ' + descrip2
                     + ' one of which was not an integer ('
                     + str(value) + ').\n'
                   '> Please edit the file to set the ' + descrip1 + '\n'
                   '> to integers.'
                  )
            gen.WriteError(3123, err, log)
            gen.ErrorOnLine(line_num, line_text, log, False)
            return(None)
    # We can't have the count of lanes run out before the down portal
    # so we test for that.  No need for this when doing SES energy
    # sectors or coasting switches, which can be extended to the route
    # end like gradient is extended.
    if key == "lanes":
        # Now check if the last chainage is at or beyond the down
        # end tunnel portal and fault if it is not.
        last_ch = chainages[-1]
        if last_ch < tunnel_chs[-1]:
            tr_index = ln_lines[-1]
            line_num, line_data, line_text = line_triples[tr_index]
            err = ('> Came across faulty input in "'
                     + file_name + '".\n'
                   '> In route "' + route_name
                     + '" there was a block of lane \n'
                   '> counts.  The last count ended at chainage\n'
                   '> ' + str(last_ch)
                     + ', which is before the down daylight\n'
                   '> portal of the tunnel complex.\n'
                   '> This means traffic disappears in the middle\n'
                   '> of the tunnel, which is bad.  Please edit\n'
                   '> the file to set lanes all the way to the \n'
                   "> tunnel end (it's at " + str(tunnel_chs[-1]) + ').'
                  )
            gen.WriteError(3124, err, log)
            gen.ErrorOnLine(line_num, line_text, log, False)
            return(None)
        # Check that the changes of lanes are at 10 m apart or more.
        # We want this so that when the lanes in two routes are shared,
        # it makes it easier to catch mismatches between them.
        result = CheckRouteChs(chainages, ch_lines, "lane counts",
                               "lane", file_name, route_name,
                               line_triples, units, log)
        if result is None:
            return(None)
    if key in ("lanes", "coasting", "sectors"):
        # We store the values as integers.
        values = [int(value) for value in values]
    # Extend the lane counts and speed limits to infinity so we can
    # use them in traffic/train calculations.  No need for this with
    # the stuff we send to SES/SVS input files.
    if key in ("lanes", "speedlimits"):
        chainages.append(math.inf)
        values.append(values[-1])
    # Store the values, the chainages they apply up to and the index
    # of the start of the block (we store the index in case we need
    # it for error message 3121 above).
    new_route_dict.__setitem__(key, (values, chainages, tr_start))
    return(tr_index, new_route_dict)


def CheckRouteChs(chainages, ch_lines, text1, text2, file_name,
                  route_name, line_triples, units, log):
    '''Check that something that varies along a route (like traffic
    lanes, gradients, speed limits) are not set at chainages that
    are closer than 10 m.
    We want this so that when the lanes in two routes are shared it
    makes it easier to catch mismatches between them.

        Parameters:
            chainages       [float]         A list of chainages to check.
            ch_lines        [int]           A list of tr_indices, one
                                            per chainage.  These are used
                                            to get the lines that each
                                            chainage appears on in the
                                            input file.
            text1           str             A description used in the
                                            error message, e.g. "lane
                                            counts".
            text2           str             A description used in the
                                            error message, e.g. "lane".
            file_name       str             The file name without the
                                            file path.
            route_name      str             Name of the route.
            line_triples  [(int, str, str)] List of lines in the file.
            units           str             "us" or "si".  Used to add
                                            text complaining about the
                                            chainages being closer than
                                            10 m or 32.81 feet.
            log             handle          The handle of the logfile.

        Returns:
            A string that is discarded.

        Errors:
            Aborts with 2901 if two chainages are closer than 10 m.
    '''
    for (ch1_index, ch1) in enumerate(chainages[:-1]):
        ch2_index = ch1_index + 1
        ch2 = chainages[ch2_index]
        if ch2 - ch1 < 10.0:
            tr1_index = ch_lines[ch1_index]
            tr2_index = ch_lines[ch2_index]
            if units == "us":
                text3 = 'closer than 10 m (32.81 ft):\n'
            else:
                text3 = 'closer than 10 m:\n'
            err = ('> Came across faulty input in "'
                     + file_name + '".\n'
                   '> Route "' + route_name
                     + '" had a block setting ' + text1 + '.\n'
                   '> Two changes of ' + text2 + ' were ' + text3 +
                   '>  chainage ' + gen.RoundText(ch1, 2)
                     + ' and chainage ' + gen.RoundText(ch2, 2) + '\n'
                   '> This is not allowed because it makes it hard\n'
                   '> to catch differences between routes in road\n'
                   '> tunnels that are in the same "trafficsteady"\n'
                   '> block.\n'
                   '> Please edit the file to ensure that these\n'
                   '> chainages (and all others in this block) are\n'
                   '> at least ten metres apart.'
                  )
            gen.WriteError(2901, err, log)
            OneOrTwoErrLines(tr1_index, tr2_index, line_triples, log)
            return(None)
    return("Chainages are OK")


def MultOffset(tun_up_details, tun_down_details, up_ch, down_ch):
    '''Take the details of the up end of a tunnel, the details of
    the down end of the tunnel (they may be back end then forward end
    or forward end then back end).  Take the up and down chainages of
    the tunnel ends in the route.
    Figure out what multiplier and offset are needed to transform a
    chainage in the route into a distance along the tunnel.

    When converting route chainages to tunnel distances, the chainage
    is multiplied first (by either +1 or -1) then the offset is added
    to the result.

    When converting tunnel distances to route chainages, the distance
    has the offset subtracted from it and the result is multiplied (or
    divided: it doesn't matter) by either +1 or -1.

        Parameters:
            tun_up_details  []              List of entries from ProcessBlock
                                            for a 'back' or 'fwd' line
                                            of entry.
            tun_up_details  []              Also a list of entries from
                                            ProcessBlock for a 'back' or
                                            'fwd' line of entry.
            up_ch           float           Chainage in the route of the
                                            up end of the tunnel.
            down_ch         float           Chainage in the route of the
                                            down end of the tunnel.

        Returns:
            multiplier      float           +1 if the up end of the
                                            tunnel is at a lower distance
                                            than the down end, -1 if
                                            it is not.
            offset          float           An offset to add after
                                            applying the multiplier.
    '''
    up2_ch = tun_up_details[0]
    down2_ch = tun_down_details[0]

    # We know that route chainages always increase as we go down
    # the route.  But which way round is the tunnel in the route?
    if up2_ch > down2_ch:
        multiplier = -1.
        offset = down2_ch + down_ch
    else:
        multiplier = 1.
        offset = up2_ch - up_ch

    return(multiplier, offset)


def CheckForPortals(tun_back, tun_fwd, tunblock_index, line_triples,
                    file_name, route_name, tun_name, log):
    '''Take the specifications of what's at the back end of a tunnel
    (portal, node, join etc.) and what's at the forward end of the
    tunnel.  If neither is a node, fault.  Otherwise return a tuple
    of the details (end type and value/name) at each end.
    This is used to ensure that routes that run through more than one
    tunnel can be linked up.

        Parameters:
            tun_back        []              The details of what is in
                                            the line with the "back"
                                            keyword (from ProcessBlock).
            tun_fwd         []              The details of what is in
                                            the line with the "fwd"
                                            keyword (from ProcessBlock).
            tunblock_index  int             Pointer to where the line
                                            holding "begin tunnels" is
                                            in the route definition.
                                            Used in error messages.
            line_triples  [(int, str, str)] List of lines in the file.
            file_name       str             The file name without the
                                            file path.
            tun_name        str             The name of the tunnel that
                                            is being looked at.
            log             handle          The handle of the logfile.

        Returns:
            tun_backtype    str             Word describing what's at
                                            the back end of a tunnel
                                            ("node", "q_inflow",
                                            "portal" etc.
            tun_backvalue   anything        Could be a node name, a
                                            flow value, pressure, etc.
            tun_fwdtype     str             Word describing what's at
                                            the forward end of a tunnel
                                            ("node", "q_inflow",
                                            "portal" etc.
            tun_fwdvalue    anything        Could be a node name, a
                                            flow value, pressure, etc.

        Errors:
            Aborts with 2661 if neither end of the tunnel was a node.
    '''
    tun_backtype, tun_backvalue = tun_back[1:3]
    tun_fwdtype, tun_fwdvalue = tun_fwd[1:3]
    back_index = tun_back[-1]
    fwd_index = tun_fwd[-1]

    # Get the data we may need to complain about up to three lines
    # of data at a time.  These are the line starting the "begin tunnels"
    # block in the route definition and the four lines defining the back
    # and forward ends of the two tunnels.

    # Check for tunnels with no nodes at both ends and raise a fault.
    if tun_backtype != "node" and tun_fwdtype != "node":
        (line1_num, discard, line1_text) = line_triples[tunblock_index]
        (line2_num, discard, line2_text) = line_triples[back_index]
        (line3_num, discard, line3_text) = line_triples[fwd_index]
        err = ('> Came across faulty input in "' + file_name + '".\n'
               '> In route "' + route_name + '" the tunnel "'
                 + tun_name + '" was\n'
               '> given as one of the tunnels in a list of\n'
               '> tunnels.  But that tunnel has no nodes connecting\n'
               '> it to other tunnels, so it cannot appear in a\n'
               '> route with other tunnels.  Please either edit \n'
               '> the "begin tunnels" block in the route to remove\n'
               '> the tunnel, or edit the definition of the tunnel\n'
               '> ends to add a node or two.'
              )
        gen.WriteError(2661, err, log)
        gen.ErrorOnThreeLines(line1_num, line1_text,
                              line2_num, line2_text,
                              line3_num, line3_text, log, False)
        return(None)
    return(tun_backtype, tun_backvalue, tun_fwdtype, tun_fwdvalue)


def ProcessSchedule(line_triples, tr_index, settings_dict, trtypes_dict,
                    route_name, sch_name, schedules_dict, log):
    '''Process a "schedule" sub-block (inside a "route" block) that
    controls the launching of trains.
    The entries select one train type: if you need different types of
    train you will need more than one "begin schedule" block).
    It sets where along the route they appear and the train schedule.
    The schedule could be one number (the runtime that one train
    starts at) or a list of numbers built by lists, range() functions,
    startstepcount() functions and startstopcount() functions (all
    processed by PROC CheckListAndRange).
    Train speeds can be set as a fixed speed (default is km/h or mph
    depending on the base units, but can be changed to m/s or fps by an
    optional argument).  In future versions, variable train speeds will
    be added.

    It creates a dictionary that defines what type of train to launch,
    when to launch it/them and what to use to set the train speed
    profile.  Then is stores that train schedule in 'schedules_dict'
    using the name of the block as the key.  It also returns the
    tr_index of the "end schedule" line.

       Parameters:
            line_triples  [(int, str, str)] List of lines in the file.
            tr_index        int             Pointer to where the block
                                            starts in line_triples.
            settings_dict   {}              Dictionary of the run settings.
            trtypes_dict    {}              Dictionary of the train
            route_name      str             Name of the new route, used as
                                            a dictionary key.
            sch_name        str             Name of the schedule, from
                                            the "begin schedule <name>"
                                            line.
            schedules_dict  {}              Dictionary of schedules that
                                            have already been defined.
            log             handle          The handle of the logfile.

        Returns:
            tr_index        int             Pointer to where the block
                                            ends.  The calling routine
                                            needs to know where to pick
                                            up processing of the rest
                                            of the route definition.
            schedules_dict  {}              An updated dictionary of
                                            schedules.

        Errors:
            Aborts with 2621 if the name of the schedule has already
            been used for another schedule in this route.
            Aborts with 2622 if the name of the train type to be used
            has not been defined and there are no train types in the
            file.
            Aborts with 2623 if the name of the train type to be used
            has not been defined and there are train types in the file.
            Aborts with 2624 if there are duplicate train spawn times.
            Warns with 2625 if the initial train speed is negative and
            there is no "downstart" keyword (it warns that the user
            probably forgot to put one in).
    '''
    debug1 = settings_dict["debug1"]
    file_name = settings_dict["file_name"]
    units = settings_dict["units"]
    time_accuracy = settings_dict["time_accuracy"]

    sch_key = "#sch_" + sch_name.lower()
    # First check if the name of the schedule clashes with the name of
    # another schedule.
    if sch_key in schedules_dict:
        line2_num, line_data2, line2_text = line_triples[tr_index]
        old_sch = schedules_dict[sch_key]
        index1 = old_sch["traintype"][-1]
        # Count back to the "begin schedule" line.
        while True:
            tr_index -= 1
            line1_num, line_data1, line1_text = line_triples[tr_index]
            if line_data1.lower().split()[:2] == ["begin", "schedule"]:
                break

        err = ('> Came across faulty input in "' + file_name + '".\n'
               '> In route "' + route_name + '", train schedule "'
                 + sch_name + '"\n'
               '> was defined twice.  Please rename one of them or\n'
               '> delete one of them.')
        gen.WriteError(2621, err, log)
        gen.ErrorOnTwoLines(line2_num, line2_text,
                            line1_num, line1_text,
                            log)
        return(None)

  # begin schedule  once
  #   traintype  G+P-Fig-5
  #   spawnpoint -347
  #   schedule  [0] # One train
  #   # Train speed 34.7 m/s (124.9 km/h).
  #   speed1   34.7  units := m/s   # Fixed speed
  # end schedule
    valid_settings = {"traintype":  ("#name",),
                      "downstart":  ("float any dist1 a chainage",),
                      "times":      ("QAstr",),
                      "fixed-speed": ("float any  null  a train speed",),
                      # "time-speed": ("#name", "#name", "#name",),
                      # "dist-speed": ("#name", "#name", "#name",),
                     }
    #
    # Define the optional entries allowed in each keyword.  This differs
    # depending on the units selected in the settings block.  Note that
    # in the valid settings above the conversion factor for speed has
    # been set to null so that we can do it below if a specific unit
    # is set as an optional argument.
    if units == "us":
        usables = ("mph", "fpm", "fps")  # Default is mph
    else:
        usables = ("km/h", "kmh", "kph", "m/s") # Default is km/h
    optionals = {"fixed-speed": {"units": usables},
                 "time-speed": {"units": usables},
                 "dist-speed": {"units": usables},
                }
    #
    # Make a list of what entries we must have.  We don't need "start",
    # because if we don't set it, we use the route origin as the place
    # where trains start from.

    requireds = ("traintype", "times",
                    ("fixed-speed", "time-speed", "dist-speed"), # Need one of these.
                )
    #
    # Make a list of what entries can be duplicated (none).
    duplicates = ()

    settings = (valid_settings, requireds, optionals, duplicates)
    block_name = "schedule"

    if debug1:
        print("Processing a train schedule")

    result = ProcessBlock(line_triples, tr_index, settings_dict,
                          block_name, {}, settings, log)
    if result is None:
        return(None)
    else:
        (discard, schedule_dict) = result

    # Check that the train type exists.
    trtype = schedule_dict["traintype"][0]
    if trtype not in trtypes_dict:
        # Check if any train types have been defined.
        tr_names = list(trtypes_dict.keys())
        err_index = schedule_dict["traintype"][2]
        if tr_names == []:
            err = ('> Came across faulty input in "' + file_name + '".\n'
                   '> In route "' + route_name + '", train schedule "'
                     + sch_name + '"\n'
                   '> referred to train type "' + trtype + '", but no such\n'
                   '> train type exists.  Please define it using a\n'
                   '> "begin traintype ' + trtype + '" block in the file.')
            gen.WriteError(2622, err, log)
            gen.ErrorOnLine2(err_index, line_triples, log, False)
            return(None)
        else:
            tr_names.sort()
            err = ('> Came across faulty input in "' + file_name + '".\n'
                   '> In route "' + route_name + '", train schedule "'
                     + sch_name + '"\n'
                   '> referred to train type "' + trtype + '", but no such\n'
                   '> train type exists.  Please define it or change\n'
                   '> the train type to one of the following:\n'
                     + gen.FormatOnLines(tr_names))
            gen.WriteError(2623, err, log)
            gen.ErrorOnLine2(err_index, line_triples, log, False)
            return(None)

    # Figure out what train start times we have and fault if the entries
    # are not well defined.
    maybe_list, options_dict, start_index = schedule_dict["times"]
    line_number, discard, line_text = line_triples[start_index]
    result = CheckListAndRange(maybe_list, "spoof_it",
                               settings_dict, line_triples,
                               line_number, line_text, False, log)
    if result is None:
        return(None)
    else:
        # If the user put in a number instead of a list, turn it into
        # a list.
        if type(result) is float:
            result = [result]

    # These are train launch times that may have been generated by numpy
    # functions and be a bit off.  Round them to the time accuracy we
    # are using (currently 10 nanoseconds).
    traintimes_list = [round(time, time_accuracy) for time in result]
    tr_count = len(traintimes_list)

    # Check for duplicates and complain if there are any.
    traintimes_set = set(traintimes_list)
    if len(traintimes_set) != tr_count:
        duplicates = traintimes_list.copy()
        for time in traintimes_set:
            duplicates.remove(time)
        err = ('> Came across faulty input in "' + file_name + '".\n'
               '> In route "' + route_name + '", train schedule "'
                 + sch_name + '"\n'
               '> has trains spawning at the same time, which is not\n'
               '> allowed.  The following is a list of the duplicated\n'
               '> times:\n'
                 + gen.FormatOnLines(duplicates) + '\n'
               '> Please edit the entries in the "times" keyword to\n'
               '> to remove the duplicate times.')
        if '(' in line_text:
            # The user used a range function, startstepcount function
            # or startstopcount function.  Give some extra advice in
            # the error message and show the full list of times that
            # the functions produced.
            err = err + '\n'\
               '> You used the range, startstepcount or startstopcount\n' \
               '> function to build the list of times so it may not be\n' \
               '> obvious where the duplication sprang from.  Here is\n' \
               '> the full list of times:\n' \
                 + gen.FormatOnLines(traintimes_list)

        gen.WriteError(2624, err, log)
        gen.ErrorOnLine(line_number, line_text, log, False)
        return(None)
    else:
        traintimes_list.sort()
        # Now set an entry in the dictionary holding the times that the
        # trains all start off at.
        schedule_dict.__setitem__("traintimes", traintimes_list)

    if "fixed-speed" in schedule_dict:
        speed, options_dict = schedule_dict["fixed-speed"][:2]
        # Figure out if we need to divide by something to get the train
        # speed to m/s.
        factor = GetSpeedUnits(units, options_dict, debug1, log)
        if factor is None:
            return(None)

        # This is a constant speed train so we just set one number, speed
        # in m/s.  More complex variations (below) need a pair of lists
        # of times and associated train speeds.
        schedule_dict.__setitem__("trainspeeds", speed / factor)
        # Get the initial speed, to see if we need to raise warning 2624
        # below.
        start_speed = speed
    elif "time-speed" in schedule_dict:
        print("The speed-time function for trains is not implemented yet.")
        gen.OopsIDidItAgain(log, file_name)
        return(None)
        # spd_times = np.array((0.0, math.inf))
        # spd_vels = np.array((speed, speed))
        # schedule_dict.__setitem__("trainspeeds", (spd_times, spd_vels))
    elif "dist-speed" in schedule_dict:
        print("The speed-distance function for trains is not implemented yet.")
        gen.OopsIDidItAgain(log, file_name)
        return(None)

    # Do a last check.  If the trains have initial -ve speed and the user
    # did not set a "downstart" keyword, then the trains will leave the
    # simulation the timestep after they appear.  It's likely that the
    # user forgot to set a "downstart" keyword to make the trains spawn
    # on the down side of the tunnel.
    # If the user did set a downstart keyword and it is on the up side
    # of the tunnel complex, that's their lookout.
    if start_speed < 0.0 and "downstart" not in schedule_dict:
        err = ('> Came across unusual input in "' + file_name + '".\n'
               '> In route "' + route_name + '", train schedule "'
                 + sch_name + '"\n'
               '> there are ' + str(tr_count)
                 + ' trains that start at the route origin\n'
               '> with a negative train speed, meaning that they exit as\n'
               '> soon as they spawn.  You probably forgot to include a\n'
               '> "downstart" keyword to make the trains spawn on the\n'
               '> down side of the tunnel complex.  The run will proceed\n'
               '> but you may want to look into this.')
        gen.WriteError(2625, err, log)

    # We need to set tr_index to the "end schedule" line.
    while True:
        tr_index += 1
        if line_triples[tr_index][1].lower().split() == ["end", "schedule"]:
            # tr_index now points to the line for the "end schedule",
            # which we want to return along with the dictionary.
            break
    schedules_dict.__setitem__(sch_key, schedule_dict)
    return(tr_index, schedules_dict)


def GetSpeedUnits(units, options_dict, debug1, log):
    '''Take the units, the options dictionary of a train speed definition
    and the handle of the log file.  Return the conversion factor that
    we divide the value of train speeds by to get the speeds in m/s.

    If there is an unhandled key in the options dictionary, raise a
    "fouled up the code" error and return None.

       Parameters:
            units           str             "si" or "us", the units set
                                            in the "settings" block.
            options_dict    {}              A dictionary of optional
                                            entries on the line that
                                            set the train speed.
            debug1          bool            The debug Boolean set by
                                            the user.
            log             handle          The handle of the logfile.

        Returns:
            factor         float            A conversion factor to turn
                                            values of train speed in the
                                            input file into m/s for the
                                            internal calculations.
    '''
    if "units" in options_dict:
        spd_units = options_dict["units"]
    elif units == "si":
        # The user set SI units in the settings block (or did not set
        # anything).
        spd_units = "km/h"
    else:
        # The user set US units in the settings block.
        spd_units = "mph"

    # Users can set an optional entry to define a train speed in
    # different units.  We check for those here.
    if spd_units in ("km/h", "kmh", "kph"):
        factor = 3.6
    elif spd_units == "m/s":
        factor = 1.0
    elif spd_units == "mph":
        factor = USc.ConvertToSI("speed3", 1.0, debug1, log)[0]
    elif spd_units == "fpm":
        factor = USc.ConvertToSI("speed1", 1.0, debug1, log)[0]
    elif spd_units == "fps":
        factor = USc.ConvertToSI("speed4", 1.0, debug1, log)[0]
    else:
        # It's not a good idea to have values in feet per second as
        # the 'else' clause.
        # Instead, raise an error that calls me out as lazy.  We may
        # not ever get here, but you never know when someone will
        # ask for a new speed unit to be added.
        print("Need to add a new clause to handle a new unit for\n"
              'train speeds in PROC SpeedUnits: "' + units + '".')
        gen.OopsIDidItAgain(log, file_name)
        return(None)
    return(factor)


def CheckGreater(value1, line1, value2, line2, relation, err_lines,
                 settings_dict, log):
    '''Take two numbers and check that the second one is larger than the
    first (or equal to or larger than it).  This is mostly used in situations
    where one number is set on one line and the other is set on a different
    line.  A case in point: ensuring that the up portal chainage on a route
    is down from the route origin chainage.  These are set in different
    entries but need to be compared.  Same goes for the origin chainage and
    the first chainage in the gradients, elevations, speed limits, etc.

        Parameters:
            value1          float           First value to be compared
            line1         [int, str, str]   Details of the line holding the
                                            first line.  Used in the error
                                            message.
            value2          float           First value to be compared
            line2         [int, str, str]   Details of the line holding the
                                            second line.  Used in the error
                                            message.
            relation        str             If ">", value1 must be higher than
                                            value2.
                                            If ">", value1 must be higher than
                                            or equal to value2.
            err_lines       str             One or more lines of descriptive
                                            text, to be used in error messages.
            settings_dict   {}              Dictionary of the run settings.
            log             handle          The handle of the logfile.

        Returns:
            True if the test is passed, None if it is not.

        Errors:
            Aborts with 2361 if the second number was not greater than the
            first number when that was the condition imposed.
            Aborts with 2362 if the second number was not greater than or
            equal to the first number when that was the condition imposed.
    '''
    file_name = settings_dict["file_name"]

    if relation == ">" and value2 <= value1:
        err = ('> Came across faulty input in "' + file_name + '".\n'
                 + err_lines + '\n'
             '> Please edit the file to correct the mistake.'
            )
        gen.WriteError(2361, err, log)
        (line1_num, line_data1, line1_text) = line1
        (line2_num, line_data2, line2_text) = line2
        gen.ErrorOnTwoLines(line2_num, line2_text,
                            line1_num, line1_text,
                            log)
        return(None)
    elif relation == ">=" and value2 <= value1:
        err = ('> Came across faulty input in "' + file_name + '".\n'
                 + err_lines + '\n'
             '> Please edit the file to correct the mistake.'
            )
        gen.WriteError(2362, err, log)
        (line1_num, line_data1, line1_text) = line1
        (line2_num, line_data2, line2_text) = line2
        gen.ErrorOnTwoLines(line1_num, line1_text,
                            line2_num, line2_text,
                            log)
        return(None)
    return(True)


def ProcessNumberList(line_triples, tr_index, settings_dict,
                      block_name, first_name, second_name,
                      first_rule, second_rule, log):
    '''Read a list of lines with numbers (and/or the names of constants) on
    it, ending at a line with "end <block_name>" on it.  Put all the odd
    numbered items into one list and all the even numbered ones into
    another.

        Parameters:
            line_triples [(int, str, str)]  List of lines in the file.
            tr_index        int             Pointer to where the block starts
                                            in line_triples.
            settings_dict   {}              Dictionary of the run settings.
            block_name      str             The name of the block converted to
                                            lower case.  When we encounter
                                            a line with ["end", block_name]
                                            on it the block ends.
            first_name      str             Name of the first type of value,
                                            (plural, e.g. chainages).  Used
                                            in error messages.
            second_name     str             Name of the second type of value.
                                            Used in error messages.
            first_rule      str             If "", any values are allowed.
                                            If ">", the 3rd entry (Python index 2)
                                            must be higher than the 1st entry
                                            (Python index 0) and so on.
                                            If ">=", the 3rd entry (index 2)
                                            must be equal to or higher than the
                                            the 1st entry (index 0) and so on.

            second_rule     str             If "", any values are allowed.
                                            If ">", the 4th entry (index 3)
                                            must be higher than the 2nd entry
                                            (index 1) and so on.
                                            If ">", the 4th entry (index 3)
                                            must be equal to or higher than the
                                            the 2nd entry (index 1) and so on.
            log             handle          The handle of the logfile.

        Returns:
            tr_index        int             Pointer to where the block ended
                                            in line_triples.
            odd_list        [float]         All the entries with even Python
                                            indices (1st = 0, 3rd = 2, 5th = 4)
            even_list       [float]         All the entries with odd Python
                                            indices (2nd = 1, 4th = 3, 6th = 5)
            odd_where       [int]           tr_indices of the lines that hold
                                            the odd-numbered values.
            even_where      [int]           tr_indices of the lines that hold
                                            the even-numbered values.

        Errors:
            Aborts with 2341 if one of the entries was not a number or the
            name of a constant.
            Aborts with 2342 if the first entry in one pair was not higher
            than the first entry in the previous pair when this was
            disallowed.
            Aborts with 2343 if the first entry in one pair was not equal
            to or higher than the first entry in the previous pair when
            this was disallowed.
            Aborts with 2344 if the first entry in one pair was not lower
            than the first entry in the previous pair when this was
            disallowed.
            Aborts with 2345 if the first entry in one pair was not equal
            to or lower than the first entry in the previous pair when
            this was disallowed.
            Aborts with 2346 if the second entry in one pair was not higher
            than the second entry in the previous pair when this was
            disallowed.
            Aborts with 2347 if the second entry in one pair was not equal
            to or higher than the second entry in the previous pair when
            this was disallowed.
            Aborts with 2348 if the second entry in one pair was not lower
            than the second entry in the previous pair when this was
            disallowed.
            Aborts with 2349 if the second entry in one pair was not equal
            to or lower than the second entry in the previous pair when
            this was disallowed.
    '''
    debug1 = settings_dict["debug1"]
    file_name = settings_dict["file_name"]
    # Create a list to hold all the data.  We split it in two later.
    numbers = []
    # Create a list to hold which line number (tr_index) each value is
    # on.  We use this in the error messages to decide whether to show
    # two lines of input or one in the error message.
    where_index = []
    # Store the index of the line at the start of the list, which we
    # can use for debugging when coding.
    orig_index =  tr_index

    while tr_index < len(line_triples):
        (line_number, line_data, line_text) = line_triples[tr_index]

        words = line_data.split()
        words_low = line_data.lower().split()

        if words_low[:2] == ["end", block_name]:
            # We've finished.  Break out of the loop.
            break
        for index, word in enumerate(words_low):
            result = CheckForConstant(word, False, settings_dict)
            try:
                # It is a number or a valid constant
                numbers.append(float(result))
            except ValueError:
                quot, rem = divmod(index, 2)
                if rem == 0:
                    descrip = first_name
                else:
                    descrip = second_name
                err = ('> Came across faulty input in "' + file_name + '".\n'
                       '> A list of data pairs was given in which one\n'
                       '> of the ' + descrip + ' was not a number or the\n'
                       '> name of a constant, it was "' + result + '".\n'
                       '> Please edit the file to correct the mistake.'
                      )
                gen.WriteError(2341, err, log)
                gen.ErrorOnLine(line_number, line_text, log, False)
                # Set this True to do a bit of debugging.
                return(None)
            else:
                where_index.append(tr_index)


        tr_index += 1

    # Set this to True to print the lines that have been read (useful for
    # debugging).
    if False:
        print(line_triples[orig_index])
        print(block_name, orig_index, tr_index)
        for index in range(orig_index, tr_index):
            print(line_triples[index])
    # Split the list into two lists of numbers, the even ones (when numbered
    # from one) and the odd ones (also when numbered from one).  Because
    # Python starts its indexing at 0 we have to do something counterintuitive.
    odd_list = [number for index, number in enumerate(numbers) if index%2 == 0]
    odd_where = [number for index, number in enumerate(where_index) if index%2 == 0]
    even_list = [number for index, number in enumerate(numbers) if index%2 == 1]
    even_where = [number for index, number in enumerate(where_index) if index%2 == 1]

    if first_rule != "":
        # There is a rule to be applied to the odd numbers in the list.
        for index, value2 in enumerate(odd_list[1:]):
            value1 = odd_list[index]
            if first_rule == ">" and value2 <= value1:
                err = ('> Came across faulty input in "' + file_name + '".\n'
                       '> A list of data pairs was given in which the\n'
                       '> first entry in each pair must be higher than\n'
                       '> the first entry in the previous pair.\n'
                       "> That didn't happen here: there are two entries,\n"
                       '> "' + str(value1) + '" followed by "'
                             + str(value2) + '".\n'
                       '> Please edit the file to correct the mistake.'
                      )
                gen.WriteError(2342, err, log)
                # Call a routine that figures out whether to print one
                # line of input or two.
                OneOrTwoErrLines(odd_where[index], odd_where[index + 1],
                                 line_triples, log)
                return(None)
            elif first_rule == ">=" and value2 < value1:
                err = ('> Came across faulty input in "' + file_name + '".\n'
                       '> A list of data pairs was given in which the\n'
                       '> first entry in each pair must be equal to or\n'
                       '> higher than the first entry in the previous pair.\n'
                       "> That didn't happen here: there are two entries,\n"
                       '> "' + str(value1) + '" followed by "'
                             + str(value2) + '".\n'
                       '> Please edit the file to correct the mistake.'
                      )
                gen.WriteError(2343, err, log)
                OneOrTwoErrLines(odd_where[index], odd_where[index + 1],
                                 line_triples, log)
                return(None)
            elif first_rule == "<" and value2 >= value1:
                err = ('> Came across faulty input in "' + file_name + '".\n'
                       '> A list of data pairs was given in which the\n'
                       '> first entry in each pair must be lower than\n'
                       '> the first entry in the previous pair.\n'
                       "> That didn't happen here: there are two entries,\n"
                       '> "' + str(value1) + '" followed by "'
                             + str(value2) + '".\n'
                       '> Please edit the file to correct the mistake.'
                      )
                gen.WriteError(2344, err, log)
                OneOrTwoErrLines(odd_where[index], odd_where[index + 1],
                                 line_triples, log)
                return(None)
            elif first_rule == "<=" and value2 > value1:
                err = ('> Came across faulty input in "' + file_name + '".\n'
                       '> A list of data pairs was given in which the\n'
                       '> first entry in each pair must be lower than\n'
                       '> or equal to the first entry in the previous pair.\n'
                       "> That didn't happen here: there are two entries,\n"
                       '> "' + str(value1) + '" followed by "'
                             + str(value2) + '".\n'
                       '> Please edit the file to correct the mistake.'
                      )
                gen.WriteError(2345, err, log)
                OneOrTwoErrLines(odd_where[index], odd_where[index + 1],
                                 line_triples, log)
                return(None)
    if second_rule != "":
        # There is a rule to be applied to the even numbers in the list.
        for index, value2 in enumerate(even_list[1:]):
            value1 = even_list[index]

            if second_rule == ">" and value2 <= value1:
                err = ('> Came across faulty input in "' + file_name + '".\n'
                       '> A list of data pairs was given in which the\n'
                       '> second entry in each pair must be higher than\n'
                       '> the second entry in the previous pair.\n'
                       "> That didn't happen here: there are two entries,\n"
                       '> "' + str(value1) + '" followed by "'
                             + str(value2) + '".\n'
                       '> Please edit the file to correct the mistake.'
                      )
                gen.WriteError(2346, err, log)
                OneOrTwoErrLines(odd_where[index], odd_where[index + 1],
                                 line_triples, log)
                return(None)
            elif second_rule == ">=" and value2 < value1:
                err = ('> Came across faulty input in "' + file_name + '".\n'
                       '> A list of data pairs was given in which the\n'
                       '> second entry in each pair must be equal to or\n'
                       '> higher than the second entry in the previous pair.\n'
                       "> That didn't happen here: there are two entries,\n"
                       '> "' + str(value1) + '" followed by "'
                             + str(value2) + '".\n'
                       '> Please edit the file to correct the mistake.'
                      )
                gen.WriteError(2347, err, log)
                OneOrTwoErrLines(odd_where[index], odd_where[index + 1],
                                 line_triples, log)
                return(None)
            elif second_rule == "<" and value2 >= value1:
                err = ('> Came across faulty input in "' + file_name + '".\n'
                       '> A list of data pairs was given in which the\n'
                       '> second entry in each pair must be lower than\n'
                       '> the second entry in the previous pair.\n'
                       "> That didn't happen here: there are two entries,\n"
                       '> "' + str(value1) + '" followed by "'
                             + str(value2) + '".\n'
                       '> Please edit the file to correct the mistake.'
                      )
                gen.WriteError(2348, err, log)
                OneOrTwoErrLines(even_where[index], even_where[index + 1],
                                 line_triples, log)
                return(None)
            elif second_rule == "<=" and value2 > value1:
                err = ('> Came across faulty input in "' + file_name + '".\n'
                       '> A list of data pairs was given in which the\n'
                       '> second entry in each pair must be lower than\n'
                       '> or equal to the second entry in the previous\n'
                       "> pair.  That didn't happen here: there are two\n"
                       '> entries, "' + str(value1) + '" followed by "'
                             + str(value2) + '".\n'
                       '> Please edit the file to correct the mistake.'
                      )
                gen.WriteError(2349, err, log)
                OneOrTwoErrLines(even_where[index], even_where[index + 1],
                                 line_triples, log)
                return(None)
    # Check if the block is entirely empty of numbers and complain
    # if it is.
    if len(numbers) == 0:
        err = ('> Came across faulty input in "' + file_name + '".\n'
               '> A list of data pairs was given in which there\n'
               '> were no numbers at all, just the "begin '
                 + block_name + '"\n'
               '> and the "end ' + block_name + '"on adjacent lines.\n'
               '> Please edit the file to add at least two entries.'
              )
        gen.WriteError(2350, err, log)
        OneOrTwoErrLines(orig_index - 1, tr_index, line_triples, log)
        return(None)
    # Check that each list has at least one entry.
    if len(numbers) == 1:
        err = ('> Came across faulty input in "' + file_name + '".\n'
               '> A list of data pairs was given in a "'
                 + block_name + '" block\n'
               '> in which there was just one number.\n'
               '> Please edit the file to make the list of data pairs\n'
               '> have at least two entries.\n'
              )
        gen.WriteError(2351, err, log)
        line_num, discard, line_text = line_triples[odd_where[0]]
        gen.ErrorOnLine(line_num, line_text, log)
        return(None)

    return(tr_index, odd_list, even_list, odd_where, even_where)


def OneOrTwoErrLines(index1, index2, line_triples, log):
    '''Take the indices of lines that hold two values in a list whose
    entries are out of whack.  The program will already have written
    an error message with its number.  This routine writes one or two
    lines of input on which the numbers are.
    This is included here because otherwise the same code would occur
    eight times in ProcessNumberList() above.

        Parameters:
            index1          int             Pointer to where the 1st line
                                            of error is in line_triples.
            index2          int             Pointer to where the 2nd line
                                            of error is in line_triples.
            line_triples [(int, str, str)]  List of lines in the file.
            log             handle          The handle of the logfile.

        Returns:
            Nothing!        None            It is never checked.

    '''
    if index1 == index2:
        (line_number, line_data, line_text) = line_triples[index1]
        gen.ErrorOnLine(line_number, line_text, log, False)
    else:
        (line1_num, line_data1, line1_text) = line_triples[index1]
        (line2_num, line_data2, line2_text) = line_triples[index2]
        gen.ErrorOnTwoLines(line1_num, line1_text,
                            line2_num, line2_text,
                            log)
    return()


def ProcessNumberList2(line_num, mod_data, line_text, settings_dict,
                       block_name, first_name, second_name,
                       first_rule, second_rule, log):
    ''' Build a spoof set of line triples from part of the data on one line
    and feed it to ProcessNumberList to be checked.  This is easier than
    doing the same set of checks in a separate routine for a part of a
    line of data.

        Parameters:
            line_num        int             Line number of the line we want
                                            to process.
            mod_data        str             String of data taken from a line
                                            of input.
            line_text       str             All the contents of the line that
                                            the data came from.
            settings_dict   {}              Dictionary of the run settings.
            block_name      str             The name of the block converted to
                                            lower case.  When we encounter
                                            a line with ["end", block_name]
                                            on it the block ends.
            first_name      str             Name of the first type of value,
                                            (plural, e.g. chainages).  Used
                                            in error messages.
            second_name     str             Name of the second type of value.
                                            Used in error messages.
            first_rule      str             If "", any values are allowed.
                                            If ">", the 3rd entry (Python index 2)
                                            must be higher than the 1st entry
                                            (Python index 0) and so on.
                                            If ">=", the 3rd entry (index 2)
                                            must be equal to or higher than the
                                            the 1st entry (index 0) and so on.
            second_rule     str             If "", any values are allowed.
                                            If ">", the 4th entry (index 3)
                                            must be higher than the 2nd entry
                                            (index 1) and so on.
                                            If ">", the 4th entry (index 3)
                                            must be equal to or higher than the
                                            the 2nd entry (index 1) and so on.
            log             handle          The handle of the logfile.

        Returns:
            result          (,,,,)          A tuple of lists returned from
                                            ProcessNumberList.
    '''
    finish = "end " + block_name
    spoof_triples = [(line_num, mod_data, line_text),
                     (line_num + 1, finish, finish)]
    result = ProcessNumberList(spoof_triples, 0, settings_dict,
                               block_name, first_name, second_name,
                               first_rule, second_rule, log)
    return(result)


def ProcessTestBlock(line_triples, tr_index, settings_dict, log):
    '''Process a "begin testblock...end testblock" block and add its
    values into a dictionary.  The test block has a set of entries for testing
    the error messages only expected to be seen during development work when
    (say) a specification liked "float" is mis-spelled.

        Parameters:
            line_triples [(int, str, str)]  List of lines in the file.  First
                                            entry is the line number in the file
                                            (starting at one, not zero).
                                            Second is the valid data on the line.
                                            Third is the entire line (including
                                            comments) also used in error messages.
            tr_index        int             Where to start reading the sectypes block.
            settings_dict   {}              The entries in the settings block.
            log             handle          The handle of the logfile.

        Returns:
            testblock_dict   {}             The testblock, as a dictionary (updated).
    '''
    debug1 = settings_dict["debug1"]
    # We make a dictionary of valid testblock entries.  Some of these
    # have definitions that are deliberately malformed so that they trigger
    # errors that should only happen during development: (1201 - 1203).
    #
    #  1201 - too few entries in the number spec
    #  1202 - we mis-spelled "int" or "float" in a number spec
    #  1203 - the rule for checking numbers was mis-spelled
    #
    # Two instances where each is raised are present: the first
    # where a number spec is wrong for a high level number (one
    # we expect to have in the line) and at a low level (one we
    # have in the line after a keyword (i.e. when a gauge air
    # pressure is needed after declaring a tunnel end to be a
    # portal).
    # type   range  key description
    valid_settings = {
        "1201a": ("float     +    null", 'QAstr' ), # High level 1201
        "1201b": (("any_word float     +    null",),'QAstr'), # Low level 1201
        "int":   ("int      any   null   any number (testblock)", 'QAstr'),
        "float": ("float    any   null   any number (testblock)", 'QAstr'),
        "1202a": ("flaot     +    null   a number (testblock)", 'QAstr'),  # High level 1202
        "1202b": (("any_word tin + null  an integer (testblock)",), 'QAstr'),  # Low level 1202
        "-":     ("float     -    null   a number (testblock)", 'QAstr'),
        "-0":    ("float     -0   null   a number (testblock)", 'QAstr'),
        "0+":    ("float     0+   null   a number (testblock)", 'QAstr'),
        "+":     ("float     +    null   a number (testblock)", 'QAstr'),
        "1203a": ("float    ayn   null   any number (testblock)", 'QAstr'), # High level 1203
        "1203b": (("any_word float ayn null any number (testblock)",), 'QAstr'), # Low level 1203
        "req1a":  ("#name",),  # req1 and req2 are mutually exclusive options
        "req1b":  ("#name",),
        # These two are for completeness.
        "name": ("#name",),
        "any":   ("float    any   null   any number",),
                     }
    # We make a list of entries that we must have.
    requireds = [("req1a", "req1b")]
    # We make a dictionary of the optional keywords that can be entered
    # for each entry in valid_settings.  We give the "name" entry one
    # valid optional keyword so we can use it to check the raising of
    # 2108 with one optional keyword and the "any: entry two to check the
    # the raising of 2108 with more than one keyword.
    optionals = {"name": {"option_1": "float  any  null   a test option",},
                 "any":  {"option_1": "float  any  null   a test option",
                          "option_2": "float  any  null   a test option",
                          "tuple1":  ("reverse",),
                          "tuplemany":  ("one", "two", "three", "four", "five",
                                         "six", "seven", "eight", "nine"),}
                }
    duplicates = ("-0", "0+", "any")
    settings = (valid_settings, requireds, optionals, duplicates)
    block_name = "testblock"
    # Now call ProcessBlock.  We don't care what it returns.
    # We spoof the settings block with an entry setting SI units because
    # ProcessBlock needs it.
    settings_dict.__setitem__("units", "si")
    result = ProcessBlock(line_triples, tr_index, settings_dict,
                          block_name, {}, settings, log)
    if result is None:
        return(None)
    else:
        (discard, testblock_dict) = result
        if debug1:
            print("Test block entries:")
            print(testblock_dict)
    return(None)


def ProcessSectypes(line_triples, tr_index, settings_dict, sectypes_dict, log):
    '''Process a "begin sectypes...end sectypes" block and add its
    values into a dictionary of settings.  Each line has four mandatory
    entries (number/name, area, perimeter, roughness height/friction factor)
    and can have any number of optional entries.

        Parameters:
            line_triples [(int, str, str)]  List of lines in the file.  First
                                            entry is the line number in the file
                                            (starting at one, not zero).
                                            Second is the valid data on the line.
                                            Third is the entire line (including
                                            comments) also used in error messages.
            tr_index        int             Where to start reading the sectypes block.
            settings_dict   {}              The entries in the settings block.
            sectypes_dict   {}              The sectypes, as a dictionary (input).
            log             handle          The handle of the logfile.

        Returns:
            sectypes_dict   {}              The sectypes, as a dictionary (updated).

        Errors:
            Aborts with 2421 if the name of a sectype was "same".
            Aborts with 2422 if SI units were being used and the roughness
            heights were over the maximum allowed (this error is to catch
            cases where users think that their roughness heights are in
            millimetres instead of metres).
    '''
    file_name = settings_dict["file_name"]
    debug1 = settings_dict["debug1"]
    units = settings_dict["units"]
    max_roughness = settings_dict["max_roughness"]
    # We make a dictionary of valid settings.  The keyword "#name" is
    # a special meaning that any word is allowed, and we use it for
    # the name or number of the sectype.  We require three floating-
    # point numbers after it, which are the area, perimeter and
    # roughness height or friction factor.
    #
    # For floating point numbers and integers, each definition has
    # four parts: three words separated by spaces then a descriptive
    # phrase that can contain spaces.
    #
    # The first part gives the type we expect (e.g. float or int) and
    # this will be testing in ProcessBlock.
    #
    # The second sets the limits of the value:
    #  any allows any number (negative or positive)
    #  -   allows any negative number but not zero.
    #  -0  allows any negative number or zero
    #  0+  allows any positive number or zero
    #  +   allows any positive number but not zero.
    #
    # The third is the dictionary keyword in UScustomary.py to use
    # to convert inputs in US customary units into SI units.  A
    # special setting is "roughness", which tells ProcessBlock not
    # to convert the roughness/friction factor if it is negative
    # (-ve values are friction factors, +ve values are roughness heights).
    #
    # The fourth entry is a descriptive phrase that is used in error
    # messages 2223 to 2226 if the value is out of range.
                                # type  range  key       description
    valid_settings = {"#name": ("float    +   area       an area",
                                "float    +   dist1      a perimeter",
                                "float   any roughness   a roughness height/friction factor",
                                "QAstr")}
    # We make a list of entries that we must have.  In the case of
    # the sectypes there are none.
    requireds = []
    # We make a dictionary of the optional keywords that can be entered
    # for each entry in valid_settings.  The first is an entry in the
    # critical velocity calculation, the second sets a gradient for
    # the sectype.
    optionals = {"#name": {"height": "float + dist1 a height",
                           "gradient": "float any dist1 a gradient",
                          },
                }
    duplicates = ()
    settings = (valid_settings, requireds, optionals, duplicates)
    block_name = "sectypes"
    # Now call ProcessBlock.  It returns None if an error occurred.  It
    # returns the updated sectypes block (we don't need the index to the
    # line of the next block here).
    result = ProcessBlock(line_triples, tr_index, settings_dict,
                          block_name, sectypes_dict, settings,
                          log)
    if result is None:
        return(None)
    else:
        (discard, sectypes_dict) = result

    # Check that none of the sectypes had the name "same" or the name
    # "sectypes".
    # "Same" is a special sectype name used mid-tunnel that means
    # "use the same as the previous sectype".  It is used in sectype
    # changes to force the program to place a boundary in the tunnel
    # without the user having to figure out what section type to use.
    # "Sectypes" is a special sectype name used in the generation of
    # cloned tunnels: the "tunnelclones" blocks use it to generate
    # multiple tunnels.
    for key in sectypes_dict:
        if key in ("same", "sectypes"):
            tr_index = sectypes_dict[key][-1]
            line_number, line_data, line_text = line_triples[tr_index]
            # Get the key the user set for the error message (it may have
            # capital letters in it).
            users_key = line_data.split()[0]
            err = ('> Came across a faulty line of input in \n'
                   '> "' + file_name + '".\n'
                   '> The line defined a sectype named "' + users_key + '".\n'
                   '> This is not allowed because that word is\n'
                   '> reserved for an internally-named sectype.\n'
                   '> Please edit the file to rename the sectype.'
                  )
            gen.WriteError(2421, err, log)
            gen.ErrorOnLine(line_number, line_text, log, False)
            return(None)
        elif units == "si":
            # The name is OK and we are using SI units.  Check for very
            # high roughness heights and fault if it looks like they thing
            # that the roughness heights are in millimetres instead of
            # metres.
            details = sectypes_dict[key]


    return(sectypes_dict)


def ProcessTrType(line_triples, tr_index, settings_dict, trtypes_dict, log):
    '''Process a "begin traintype...end traintype" block and add its
    values into a dictionary of settings.

        Parameters:
            line_triples [(int, str, str)]  List of lines in the file.
            tr_index        int             Where to start reading the
                                            traintype block.
            settings_dict   {}              The entries in the settings block.
            trtypes_dict    {}              The traintypes, as a dictionary
                                            (input).
            log             handle          The handle of the logfile.

        Returns:
            tr_dict         {}              The train types, as a
                                            dictionary (updated).

        Errors:
            Aborts with 2601 if the length of a train is over 15 km.
    '''
    file_name = settings_dict["file_name"]
    debug1 = settings_dict["debug1"]
    units = settings_dict["units"]
    max_roughness = settings_dict["max_roughness"]

    # We make a dictionary of valid settings.
    valid_settings = {"geometry": ("float    +   dist1      a length",
                                   "float    +   area       an area",
                                   "float    +   dist1      a perimeter",
                                   "float   any roughness   a roughness/friction factor",
                                   "QAstr"),
                      "ends_cc1":  ("float   0+   null   a contraction coefficient",
                                    "QAstr"),
                      "ends_aero": ("float   0+   null   an annulus k-factor",
                                    "float   0+   area   a train blockage ratio",
                                    "QAstr"),
                      "ends_zeta": ("float   0+   null   zeta in the annulus",
                                    "float    +   area   an annulus area",
                                    "QAstr"),
                      "ends_ses":  ("float    0+   null   a nose drag coefficient",
                                    "float    0+   area   a bogie drag area",
                                    "QAstr"),
                     }
    # A description of the four types of end losses:
    #
    #  * "ends_Cc1" means contraction coefficient of a flow loss in the
    #    annulus at the nose.  This was used in early British Rail work
    #    (e.g. Gawthorpe and Pope 1976).  This probably originated in
    #    the 1960s (the most likely candidates are Hara in Japan or
    #    Earnshaw and Peacock in Britain).
    #    There is no correction for blockage ratio.  In the full-scale
    #    experiments that this type of flow loss was applied to, the
    #    contraction coefficient for each train-tunnel calculation was
    #    set to match the experimental results.
    #    It should be used for comparison with early full-scale test
    #    data rather than for design work.
    #    The tail loss coefficient is the Borda-Carnot expansion loss
    #    from the annulus area to the open tunnel area.
    #    We add the number 1 to the end of it in case we want to make
    #    a 2nd version with rules that take a reference train blockage
    #    ratio and adjust the loss according to the actual train
    #    blockage ratio, like in Gaillard's thesis or Gawthorpe's 1978
    #    paper.
    #
    #  * "ends_aero" is the end losses used in the Mott MacDonald
    #    Aero program.  At the nose, a loss coefficient and a train
    #    blockage ratio (train area / reference tunnel area) are given.
    #    The program calculates the reference tunnel area from the
    #    blockage ratio and the train area.
    #    The loss coefficient is applied to the dynamic pressure in the
    #    annulus behind the nose.
    #    If the blockage ratio is zero, the loss coefficient is treated
    #    as a constant regardless of the tunnel area the train is in.
    #    If it is not zero, that loss coefficient is adjusted by
    #      (reference tunnel area / actual tunnel area)^2
    #    The tail loss coefficient is the Borda-Carnot expansion loss
    #    from the annulus area to the open tunnel area and placed in
    #    the open tunnel area.
    #
    #  * "ends_zeta" means that at the nose, a loss coefficient is applied
    #    to the annulus air velocity and at the tail the Borda-Carnot
    #    expansion loss is used.  I don't know who first used this
    #    (might even have been Tollmien in 1927), but a useful summary
    #    is Vardy's two-part IMechE paper (1996).
    #    The pressure loss factor (zeta) and the annulus area it applies
    #    at are needed as inputs.  These are converted into a
    #    The tail loss coefficient is the Borda-Carnot expansion loss
    #    from the annulus area to the open tunnel area.
    #
    #  * "ends_SES" means a drag coefficient is set for the train nose
    #    area in the open air and adjusted using the rules in SES code
    #    OMEGA2.FOR.  The tail loss is a Borda-Carnot expansion loss
    #    from slightly less than the annulus area to the open tunnel
    #    area. (the smaller annulus area is a correction for the size
    #    of the boundary layers in the annulus.  The tail loss is based
    #    on work described by Hoerner for drag at the back of projectiles
    #    in the open air.  The rules used for calculating both are in
    #    OMEGA2.FOR).
    #



    # We make a list of entries that we must have.  In the case of the
    # trains we want the geometry and one of the three ways of defining
    # train end losses.  We can only require "geometry" in ProcessBlock,
    # we check for one of "ends_zeta", "ends_cc1" and "ends_ses" here"
    requireds = ["geometry",
                 ("ends_cc1", "ends_zeta", "ends_aero", "ends_ses")
                ]




    # We make a dictionary of the optional keywords that can be entered
    # for each entry in valid_settings.  The first three are entries in
    # the critical velocity calculation, the fourth sets a gradient for
    # the sectype.
    optionals = {}
    duplicates = ()
    settings = (valid_settings, requireds, optionals, duplicates)
    block_name = "traintype"
    # Now call ProcessBlock.  It returns None if an error occurred.  It
    # returns the updated sectypes block (we don't need the index to the
    # line of the next block here).
    result = ProcessBlock(line_triples, tr_index, settings_dict,
                          block_name, {}, settings, log)
    if result is None:
        return(None)
    else:
        (trtype_name, new_train_dict) = result

    # Get the train roughness or friction factor and (if a friction factor)
    # convert it into Fanning friction factor.
    frictiontype = settings_dict["frictiontype"]
    tr_rough = GetRoughness(new_train_dict["geometry"][3], frictiontype)
    if "ends_cc1" in new_train_dict:
        loss_type = 1
        loss = new_train_dict["ends_cc1"][0]
    elif "ends_zeta" in new_train_dict:
        loss_type = 2
        loss = new_train_dict["ends_zeta"][0]
    elif "ends_aero" in new_train_dict:
        loss_type = 3
        loss = new_train_dict["ends_aero"][0]
    elif "ends_ses" in new_train_dict:
        loss_type = 4
        loss = new_train_dict["ends_ses"][0]
    else:
        print('Need to add code in PROC ProcessTrType to handle a new type\n'
              'of train end loss')
        file_name = settings_dict["file_name"]
        gen.OopsIDidItAgain(log, file_name)
        return(None)
    # Store the train roughness/-c_f and end losses in a new entry.
    new_train_dict.__setitem__("lossprops", (tr_rough, loss_type, loss))

    return(trtype_name, new_train_dict)


def ProcessConstants(line_triples, tr_index, constants_dict,
                     settings_dict, log):
    '''Process a "begin constants...end constants" block and add its
    values into a dictionary of constants.  Each line has two mandatory
    entries (a name and a value).  The values are stored as words, not
    as numbers.  Constants may also be assigned lists of values, and these
    lists of values may contain the names of other constants that return
    numbers (not sublists: we don't want to go there).

        Parameters:
            line_triples [(int, str, str)]  List of lines in the file.  First
                                            entry is the line number in the file
                                            (starting at one, not zero).
                                            Second is the valid data on the line.
                                            Third is the entire line (including
                                            comments) also used in error messages.
            tr_index        int             Where to start reading the constants
                                            block.
            constants_dict  {}              The constants, as a dictionary (input).
            log             handle          The handle of the logfile.

        Returns:
           constants_dict   {}              The constants, as a dictionary (updated).

        Errors:
            Aborts with 2181 if the name of a constant started with "*".
            Aborts with 2182 if the name of a constant contained a comma.
            Aborts with 2183 if the name of a constant was a reserved
            word that is used for some other feature.
            Aborts with 2184 if the name of a constant was a number.
    '''
    debug1 = settings_dict["debug1"]
    file_name = settings_dict["file_name"]
    # We make the valid settings.  These can be any name and any floating
    # point number or Python list.  When we make a substitution we will
    # check the type and the range it is being substituted for.
    valid_settings = {"#name": ("QAstr",)}
    # We make a list of entries that we must have (none).
    requireds = []
    # We make a dictionary of the optional keywords (there are none) and
    # a list of the allowable duplicates (also none).
    optionals = {}
    duplicates = ()
    settings = (valid_settings, requireds, optionals, duplicates)
    # Create a list of names of constants that we can't allow.
    forbidden = ("duration",        # This is the duration of the run.
                 "range",           # These three are functions that
                 "startstepcount",  # generate lists and so can't be the
                 "startstopcount",  # names of constants too.
                 "up_ptl",          # The chainage of the up portal
                                    # in a traffic block.
                 "down_ptl",        # The chainage of the down portal
                                    # in a traffic block.
                )

    block_name = "constants"
    # Now call ProcessBlock.  It returns None if an error occurred.  It
    # returns the updated constants block.  We spoof the settings_dict
    # dictionary because we don't care about conversion to SI, if we
    # are converting it will happen after we substitute a constant for
    # its value.  Note that this changes a copy of settings_dict, which
    # we discard when we don't return the updated constants dictionary.
    settings_dict.__setitem__("units", "si")
    result = ProcessBlock(line_triples, tr_index, settings_dict,
                          block_name, constants_dict, settings, log)
    if result is None:
        return(None)
    else:
        (discard, constants_dict) = result
    if debug1:
        print("In constants", constants_dict)

    # Now check if any of the keys contained "*" and complain if
    # they did (keys starting with "*" will clash with the autoscale
    # instruction in axis extents, with *time in loops and with the
    # multipliers in list definitions).  Likewise, check for commas
    # anywhere in the name, as these clash with the structure of the
    #  "begin data...end data" blocks and the list of forbidden words.
    for key in constants_dict:
        tr_index = constants_dict[key][-1]
        (line_number, discard, line_text) = line_triples[tr_index]
        if key == "block_index":
            # No need to process this dictionary entry as a constant,
            # it's used to point to the start of the block and is
            # used in error messages.
            break
        elif "*" in key:
            err = ('> Came across a faulty line of input in \n'
                   '> "' + file_name + '".\n'
                   '> The line set a constant ("' + key + '").\n'
                   '> This contains a "*", which is not allowed\n'
                   '> because it fouls up the syntax for defining\n'
                   '> lists and the syntax of the "plots" block.\n'
                   '> Please edit the file to rename the constant.'
                  )
            gen.WriteError(2181, err, log)
            gen.ErrorOnLine(line_number, line_text, log, False)
            return(None)
        elif "," in key:
            err = ('> Came across a faulty line of input in \n'
                   '> "' + file_name + '".\n'
                   '> The line set a constant ("' + key + '").\n'
                   '> This contains a comma, which is not allowed\n'
                   '> (it fouls up the syntax of the "data" blocks,\n'
                   '> which use commas to separate the entries).\n'
                   '> Please edit the file to rename the constant.'
                  )
            gen.WriteError(2182, err, log)
            gen.ErrorOnLine(line_number, line_text, log, False)
            return(None)
        elif key in forbidden:
            err = ('> Came across a faulty line of input in \n'
                   '> "' + file_name + '".\n'
                   '> The line set a constant with the name "'
                     + key + '".\n'
                   '> This is a reserved word that cannot be used\n'
                   '> as the name of one of your constants because\n'
                   '> it is the name of an internal constant.\n'
                   '> Please edit the file to rename the constant\n'
                   '> to something else.'
                  )
            gen.WriteError(2183, err, log)
            gen.ErrorOnLine(line_number, line_text, log, False)
            return(None)
        # Now check to see if the key is a number.
        try:
            discard = float(key)
        except ValueError:
            pass
        else:
            # The name of the constant can be mistaken for a number.
            # it might be "2", it might be "inf".
            tr_index = constants_dict[key][-1]
            (line_number, discard, line_text) = line_triples[tr_index]
            err = ('> Came across a faulty line of input in \n'
                   '> "' + file_name + '".\n'
                   '> The line set a constant named "' + key + '".\n'
                   '> The name of the constant was a number, which\n'
                   '> is not allowed.  It is way too confusing for\n'
                   '> users if you are allowed to define a constant\n'
                   '> named "' + key + '" and give it the value '
                   + str(constants_dict[key][0]) + ' (which is\n'
                   '> what you just tried to do).\n'
                   '>\n'
                   '> If you you wrote a file specifically to see what\n'
                   '> would happen if you did this, then I doff my hat\n'
                   '> to you.  Full marks for sneakiness!\n'
                   '> Now, please edit the file to rename your constant\n'
                   "> so that its name can't be construed as a number."
                  )
            gen.WriteError(2184, err, log)
            gen.ErrorOnLine(line_number, line_text, log, False)
            return(None)

        # Check if the value returned was a number or a Python-style list.
        # Fault if it was neither.
        value = constants_dict[key]
        maybe_list = value[0]
        tr_index = value[-1]
        (line_number, line_data, line_text) = line_triples[tr_index]
        # maybe_list = line_data.split(maxsplit=1)[1].strip()
        try:
            discard = float(maybe_list)
        except (ValueError, TypeError):
            # Check if the contents of the line after the key take the
            # form of a Python list.  First we get all the text on the
            # line after the name of the constant, not just the first word.
            if debug1:
                print("Checking a constant as a list", maybe_list)
            result = CheckListAndRange(str(maybe_list), constants_dict,
                                       settings_dict, line_triples,
                                       line_number, line_text, False, log)
            if result is None:
                return(None)
            else:
                value_list = result

            new_dict_item = (value_list, "", constants_dict[key][-2],
                             constants_dict[key][-1])
            constants_dict.__setitem__(key, new_dict_item)
            if debug1:
                print("Constant", key, new_dict_item)
        else:
            # It's a number.  Check for number-related words that we
            # don't want the user to be able to set, like "NaN" and "inf".
            result = gen.CheckNanInf(maybe_list, file_name, line_number,
                     line_text, log)
            if result is None:
                return(None)
    return(constants_dict)


def CheckListAndRange(maybe_list, constants_dict, settings_dict, line_triples,
                      line_number, line_text, str_ok, log):
    '''Take the contents of a line and figure out if it matches the
    syntax of a Python list.  The list may contain numbers and the names
    of constants, or it may (optionally) contain a list of strings.
    The list may also have a multiplier after it, e.g. "[1,2] * 5" which
    is turned into [1, 2, 1, 2, 1, 2, 1, 2, 1, 2] (same as Python does).
    The multiplier may be the name of a constant, e.g. "[1,2] * jf_count"
    where jf_count is assigned to an integer in the constants block.
    Lists that are prepended by specific phrases can be treated as a
    special definition that generates a list from input similar to a
    Python "range()" function.
    First is a three-element range definition along the lines of
      "startstepcount(11050.0, 120., 7)".
    The "startstepcount" tells the program to generate a 7-element list
    starting at 11050 and increasing by 120 each time:
      [11050, 11170, 11290, 11410, 11530, 11650, 11770].
    Second is a three-element range definition along the lines of
      "startstepstop(12920.0, -120., 4)".
    The "startstepcount" tells the program to generate a 4-element list
    starting at 12920 and decreasing by 120 each time.  The list is
    sorted into ascending order after it has been generated:
      [12560, 12680, 12800, 12920].
    The list may have different component parts, e.g. [1,2] * 5 + [3]*12.
    When this happens, CheckListAndRange will be called recursively with
    each value of maybe_list being a part separated by a "+", then stitch
    the result together into one list.

        Parameters:
            maybe_list      str             A string that may be a list
            constants_dict  {} or str       A dictionary of constants without
                                            "#" before the name of the constant
                                            (this exists because when we call
                                            this routine from ProcessSettings
                                            its entries are not in settings_dict
                                            yet).
                                            Or a string "spoof_it".
            settings_dict   {}              Dictionary of the run settings.
            line_triples    [int, str, str] Data for all the lines.

            line_number     int             The line number the constant was
            line_text       str             The entire line, including comments.
            str_ok          Bool            If True, strings are accepted.
                                            If False, only numbers and the
                                            names of constants are accepted.
            log             handle          The handle of the logfile.

        Returns:
            my_list         []              A list of the values on the line,
                                            possibly expanded out.

        Errors:
            Aborts with 2381 if one of the entries linked by a "+" sign
            was not a list.
            Aborts with 2382 if there was a multiplier character that was
            not followed by a number or the name of a constant.
            Aborts with 2383 if a list started with a "[" but did not end
            with a "]".
            Aborts with 2384 if a list was empty.
            Aborts with 2385 if entries in a list were not separated by
            a comma.
            Aborts with 2386 if a range definition did not have "(" after
            the range keyword.
            Aborts with 2387 if a range definition had "(" after the keyword
            but did not have ")" at the end.
            Aborts with 2388 if entries in a range definition were not
            separated by a comma.
            Aborts with 2390 if a range definition used a counter of entries
            and the counter was less than one.
            Aborts with 2391 if a range definition used a step distance that
            was too close to zero (using math.isclose).
            Aborts with 2392 if a "range" range definition had a
            definition of step that did not allow us to get to "stop"
            from "start" in counts of "step" (such as a negative step
            when start is higher than stop.
            Aborts with 2393 if a range definition used start and stop
            and they were the same value (using math.isclose).
            Aborts with 2394 if part of an expression could not be reduced
            to a list definition, range definition, name of a constant
            or a number.
            Aborts with 2395 if a range definition was used and was
            used with a multiplier (multipliers can only be applied to
            lists).
            Aborts with 2396 if a constant that returned a number was
            used with a multiplier.
            Aborts with 2397 if a number was used with a multiplier.
    '''
    file_name = settings_dict["file_name"]

    # Check if this call occurs after the constants have been loaded
    # into settings_dict.
    constants_dict = SpoofConstants(constants_dict, settings_dict)

    # Define an empty list to hold the result.  This may be changed to
    # a floating point number lower down.
    my_result = []
    # First check for the presence of plus signs in the string.  These
    # mean that the list definition has more than one part.  An example
    # could be where we want to set a few jet fans at the two ends of a
    # non-incident tunnel and turn off all the banks in the middle, such
    # as   "[1, 2, 1, 1]  + [0] * 12 + [-1,0] * 3".
    # This has three parts: a four element list, a 12 element list and
    # a six element list.  We would process the three parts recursively
    # and add them together.
    if '+' in maybe_list:
        parts = maybe_list.split(sep = '+')
        for small_part in parts:
            result = CheckListAndRange(small_part, constants_dict,
                                       settings_dict, line_triples,
                                       line_number, line_text, str_ok, log)
            if result is None:
                return(None)
            else:
                # The small part we just processed should have returned a
                # list (you should not use "+" or "*" when setting a number).
                try:
                    my_result.extend(result)
                except TypeError:
                    err = ('> Came across a faulty line of input in \n'
                           '> "' + file_name + '".\n'
                           '> The line set an entry that had one or more\n'
                           '> lists in it separated by "+".  One of the\n'
                           '> entries was a number ('
                             + str(result) + ') instead of a list.\n'
                           '> Please edit the file to make the entry all\n'
                           '> lists, or one number, or one constant.'
                          )
                    gen.WriteError(2381, err, log)
                    gen.ErrorOnLine(line_number, line_text, log, False)
                    return(None)
    else:
        # If we get to here we have a list/range that does not have a + sign in it.
        # Check for five different valid types of entry:
        #   [1, 2, 3, 2, 1]    (simple list)
        #   [1, 0] * 5         (list with ten entries, set by a number multiplier)
        #   [1] * jf_count     (list whose length is set by the name of a constant).
        #   startstepcount(start, step, count)
        #   startstepstop(start, step, stop)
        # We split on the multiplication sign and strip out any whitespace.
        # print(maybe_list)
        try:
            parts = [part.strip() for part in maybe_list.lower().split(sep = "*")]
        except TypeError:
            raise()
        # Check for a multiplier sign in the text.
        if "*" in maybe_list:
            # Check if there is a text after the "*" (this can be empty if we
            # put something illegal like "[1, 2] *  +  [3, 1] in the input).
            if parts[1] == '':
                err = ('> Came across a faulty line of input in \n'
                       '> "' + file_name + '".\n'
                       '> The line set an entry that returns a list\n'
                       '> with a multiplier (*) in it. The multiplier\n'
                       '> was not followed by a number or the name of\n'
                       '> a constant.  The faulty term was "' + maybe_list + '".\n'
                       '> Please edit the file to add the multiplier.'
                      )
                gen.WriteError(2382, err, log)
                gen.ErrorOnLine(line_number, line_text, log, False)
                return(None)

            # If we get to here there is a number or a constant.  Check
            # that it is an integer number or a constant that returns an
            # integer, not a real number, a constant that returns a real
            # number or a constant that returns a list.
            result = CheckForConstant2(parts[1], constants_dict, settings_dict,
                          line_triples, line_number, line_text, True, log)
            if result is None:
                return(None)
            else:
                # We have a valid integer.
                mult = result
                # Check for "NaN" and "inf".
                result = gen.CheckNanInf(maybe_list, file_name, line_number,
                         line_text, log)
                if result is None:
                    return(None)

        # If we get to here we have taken the multiplier out of the phrase.
        # The first part of the phrase ought to be a python list, a number,
        # the name of a constant, or one of the two range expressions.
        candidate = parts[0]
        allow_mult = False

        if candidate[0] == "[":
            # It starts with a square bracket, so it probably is intended to
            # be a list.  Check if it ends in a square bracket and fault if
            # it does not.
            if candidate[-1] != "]":
                err = ('> Came across a faulty line of input in \n'
                       '> "' + file_name + '".\n'
                       '> The line set an entry that returns a list\n'
                       '> A list was started by a "[" but did not end\n'
                       '> in a "]".  It was "' + candidate + '".\n'
                       '> Please edit the file to add the "]" or\n'
                       '> delete the list.\n'
                      )
                gen.WriteError(2383, err, log)
                gen.ErrorOnLine(line_number, line_text, log, False)
                return(None)
            # Make a note that this entry is a list, which can have a
            # multiplier on it.  The other types of entry (numbers and
            # ranges) cannot have multipliers.
            allow_mult = True
            # If we get to here we have a list enclosed in square brackets.
            # Check if the entries consist of single words/numbers separated
            # by commas.  If they do, we can be sure it is a list.  First
            # turn the commas into " , " so that we can split on whitespace.
            modded_list = candidate.strip()[1:-1].replace(",", " , ")
            entries = modded_list.split()
            # Check for an empty list and fault.
            if len(entries) == 0:
                err = ('> Came across a faulty line of input in \n'
                       '> "' + file_name + '".\n'
                       '> The line had an entry that included an empty\n'
                       '> list.  Please edit the file to put at least\n'
                       '> one entry in it, or delete the empty list.\n'
                      )
                gen.WriteError(2384, err, log)
                gen.ErrorOnLine(line_number, line_text, log, False)
                return(None)
            for index, entry in enumerate(entries):
                # Check alternate entries for numbers/constants and commas.
                quot, rem = divmod(index, 2)
                if rem == 0:
                    # This is one of the entries.
                    if str_ok is False:
                        # The entry should be a number or the name of a
                        # constant that returns a number, not a list.
                        # We allow real numbers and integers.
                        result = CheckForConstant2(entry, constants_dict,
                                                   settings_dict, line_triples,
                                                   line_number, line_text,
                                                   False, log)
                        if result is None:
                            return(None)
                        else:
                            # We have a valid number.
                            my_result.append(result)
                    else:
                        # We just return the entry as it is.
                        my_result.append(entry)
                else:
                    # This should be a comma between entries.
                    if entry != ",":
                        err = ('> Came across a faulty line of input in \n'
                               '> "' + file_name + '".\n'
                               '> The line had a list as part of its entry.\n'
                               '> Entries in lists must be separated by\n'
                               '> commas, but the ' + gen.Enth(quot+1)
                                  + ' value was not\n'
                               '> separated from the ' + gen.Enth(quot+2)
                                  + ' value by a comma.\n'
                               '> Please edit the file to correct the list.\n'
                              )
                        gen.WriteError(2385, err, log)
                        gen.ErrorOnLine(line_number, line_text, log, False)
                        return(None)
        elif candidate in constants_dict:
            my_result = constants_dict[candidate][0]
            # Note that this constant could return a list or a number, both
            # are acceptable.
            constant = True
        elif ((candidate[:5].lower() == "range") or
              (candidate[:14].lower() == "startstopcount") or
              (candidate[:14].lower() == "startstepcount")):
            # It starts with the name of a range function.
            # We want three numbers inside brackets.  In the case of
            # "startstepcount" we want two floats and an integer, in the case
            # of "startstepstop" we want three floats.
            # First strip off the name of the range function.  We want to
            # be left with three numbers separated by commas inside ordinary
            # braces.
            # start stop step
            # start stop count
            # start step count
            # start stop step count
            if candidate[:5].lower() == "range":
                # This sets a start value, a stop value and a step value
                # (note that this layout is the same as the Python 'range()'
                # function).  It figures out the count of items that it
                # needs by itself, i.e. starts at <start>, goes towards
                # <stop> putting items at intervals of <step> until an
                # item would be put beyond <stop>.  Note that this command
                # may not have an item exactly at <stop>.
                remainder = candidate[5:].strip()
                range_type = candidate[:5]
                err_example = '"range (start location, stop location, step interval)"'
            elif candidate[:14].lower() == "startstepcount":
                # This sets a start value, step value and a count.  It figures
                # out the stop value by itself, i.e. spaces <count> entities
                # starting at <start> and placing them <step> apart.
                remainder = candidate[14:].strip()
                range_type = candidate[:14]
                err_example = '"startstepcount (start location, step interval, count of entries)"'
#            elif candidate[:14].lower() == "startstopcount":
            else:
                # This sets a start value, stop value and a count.  It figures
                # out the step value by itself, i.e. spaces <count> entities
                # between <start> and <stop> (inclusive), placing them far
                # equal distances apart.
                remainder = candidate[14:].strip()
                range_type = candidate[:14]
                err_example = '"startstopcount (start location, stop location, count of entries)"'

            if remainder[0] != "(":
                # It does not start with a bracket.  Complain.
                err = ('> Came across a faulty line of input in \n'
                       '> "' + file_name + '".\n'
                       '> The line set an entry that returns a range\n'
                       '> definition (' + range_type + ').  The keyword\n'
                       '> was not followed by a "(" to mark the start\n'
                       '> of three numbers that define the range.\n'
                       '> Please modify the line to match the following\n'
                       '> syntax:\n'
                       '>   ' + err_example
                      )
                gen.WriteError(2386, err, log)
                gen.ErrorOnLine(line_number, line_text, log, False)
                return(None)
            elif remainder[-1] != ")":
                # It does not end with a bracket.  Complain.
                err = ('> Came across a faulty line of input in \n'
                       '> "' + file_name + '".\n'
                       '> The line set an entry that returns a range\n'
                       '> definition (' + range_type + ').  It started\n'
                       '> with a "(" but did not end in a ")",\n'
                       '> instead it was "' + remainder[-1] + '".\n'
                       '> Please edit the file to add a ")" or\n'
                       '> delete the range definition.\n'
                      )
                gen.WriteError(2387, err, log)
                gen.ErrorOnLine(line_number, line_text, log, False)
                return(None)

            # If we get to here we have something enclosed in brackets.
            # Check that the entries consist of single words/numbers
            # separated by commas.  At the moment we don't care how many
            # entries there are, we'll check that later.
            modded_list = remainder.strip()[1:-1].replace(",", " , ")
            entries = modded_list.split()
            # Check that we have numbers separated by commas.
            for index, entry in enumerate(entries):
                # Check alternate entries for numbers/constants and commas.
                quot, rem = divmod(index, 2)
                if rem == 0:
                    # The entry should be a number or the name of a constant
                    # that returns a number, not a list.  We allow real
                    # numbers for start, stop and step and integers for
                    # count.
                    if (index == 5 and
                        range_type in ("startstopcount", "startstepcount")):
                        # This entry is the count of entries.
                        want_int = True
                    else:
                        # It can be a floating-point number.
                        want_int = False
                    result = CheckForConstant2(entry, constants_dict,
                                  settings_dict, line_triples, line_number,
                                  line_text, want_int, log)
                    if result is None:
                        return(None)
                    else:
                        # We have a valid number.
                        my_result.append(result)
                else:
                    # It should be a comma between entries.
                    if entry != ",":
                        err = ('> Came across a faulty line of input in \n'
                               '> "' + file_name + '".\n'
                               '> The line set an entry that returns a range\n'
                               '> definition (' + range_type + ').  Entries\n'
                               '> in range definitions must be separated by\n'
                               '> commas, but the ' + gen.Enth(quot+1)
                                  + ' value was not\n'
                               '> separated from the ' + gen.Enth(quot+2)
                                  + ' value by a comma.\n'
                               '> Please edit the file to correct this.\n'
                              )
                        gen.WriteError(2388, err, log)
                        gen.ErrorOnLine(line_number, line_text, log, False)
                        return(None)
            # Check how many entries there were.  We want exactly three.
            if len(my_result) != 3:
                if len(my_result) < 3:
                    err_text = "too few"
                else:
                    err_text = "too many"
                err = ('> Came across a faulty line of input in \n'
                       '> "' + file_name + '".\n'
                       '> The line set an entry that returns a range\n'
                       '> definition (' + range_type + ').  These require\n'
                       '> three numbers to define the range to use,\n'
                       '> but there were ' + err_text + ' entries ('
                         + str(len(my_result)) + ').  Please\n'
                       '> edit the file to put three entries in the\n'
                       '> range function.'
                      )
                gen.WriteError(2389, err, log)
                gen.ErrorOnLine(line_number, line_text, log, False)
                return(None)
            # If we get to here, we have a valid range function.  Get its
            # values and start checking the sanity of the values
            if range_type == "range":
                (start, stop, step) = my_result
            elif range_type == "startstopcount":
                (start, stop, count) = my_result
            elif range_type == "startstepcount":
                (start, step, count) = my_result

            # Note that we have now freed up 'my_result' and can use it
            # for the list generated by the range command.

            # Check that the counter is not zero or negative.
            if range_type in ("startstopcount", "startstepcount"):
                if count < 1:
                    err = ('> Came across a faulty line of input in \n'
                           '> "' + file_name + '".\n'
                           '> The line set an entry that returns a range\n'
                           '> definition (' + range_type + ').  These require\n'
                           '> a count of entries that is at least 1, but\n'
                           '> in this case it is ' + str(int(count)) + '.\n'
                           '> Please set the counter to 1 or above.'
                          )
                    gen.WriteError(2390, err, log)
                    gen.ErrorOnLine(line_number, line_text, log, False)
                    return(None)
            # Check that the step is not zero.
            if range_type in ("range", "startstepcount"):
                if math.isclose(step, 0.0, abs_tol = 1e-9):
                    err = ('> Came across a faulty line of input in \n'
                           '> "' + file_name + '".\n'
                           '> The line set an entry that returns a range\n'
                           '> definition (' + range_type + ').  These require\n'
                           '> a step distance to set how far apart the\n'
                           '> entities are.  In this case the step distance\n'
                           '> is zero.  Please set it to a non-zero value\n'
                           '> (note that the distances can be +ve or -ve).'
                          )
                    gen.WriteError(2391, err, log)
                    gen.ErrorOnLine(line_number, line_text, log, False)
                    return(None)
            # Check "range" ranges for number mismatches: we
            # can't let through a range that starts at 1000 and go to
            # 2000 in steps of -120.
            if range_type == "range":
                if (start < stop and step < 0) or (start > stop and step > 0) :
                    # We have a mismatch.  We can't get there from here,
                    # as it were.
                    if step < 0:
                        err_text = "increment"
                    else:
                        err_text = "decrement"
                    err = ('> Came across a faulty line of input in \n'
                           '> "' + file_name + '".\n'
                           '> The line set an entry that returns a range\n'
                           '> definition (range) that starts at\n'
                           '> ' + str(start) + ' and ends at ' + str(stop)
                              + ' but wants to get\n'
                           '> there in steps of ' + str(step) + '.\n'
                           '> Please change the step value so that the\n'
                           '> range function can ' + err_text + ' from '
                              + str(start) + '\n'
                           '> to ' + str(stop) + '.'
                          )
                    gen.WriteError(2392, err, log)
                    gen.ErrorOnLine(line_number, line_text, log, False)
                    return(None)
                else:
                    # It's a valid range.  Build it.  Note that this follows
                    # the numpy "range" syntax, where if you ask for the
                    # range(100, 200, 25) you get [100, 125, 150, 175]
                    # not [100, 125, 150, 175, 200] as you might expect.
                    # We're going to stick with that idiosyncracy, even
                    # though we might regret it.
                    my_result = list(np.arange(start, stop, step))
            elif range_type in ("range", "startstopcount"):
                if math.isclose(start, stop):
                    err = ('> Came across a faulty line of input in \n'
                           '> "' + file_name + '".\n'
                           '> The line set an entry that returns a range\n'
                           '> definition (' + range_type + ') that starts at\n'
                           '> ' + str(start) + ' and ends at ' + str(stop)
                              + '.  These are too\n'
                           '> close together to be valid.\n'
                           '> Please change the start or stop value so\n'
                           '> that the range function can work properly.'
                          )
                    gen.WriteError(2393, err, log)
                    gen.ErrorOnLine(line_number, line_text, log, False)
                    return(None)
                elif range_type == "range":
                    # We can use the entries the user gave directly.
                    my_result = list(np.arange(start, stop, step))
                else:
                    # It's a startstopcount entry.
                    # Figure out the spacing to go from start to stop
                    # with entries at the beginning and at the end,
                    # equally spaced.
                    step = (stop - start) / (count - 1)
                    # We need to add a little to the upper limit to
                    # ensure there is an entry at 'stop'.
                    my_result = list(np.arange(start, stop + 0.1 * step, step))
            else:
                # It must be "startstepcount".
                # Figure out where the last point is (the first is at 'start').
                stop = start + step * (count - 1)
                my_result = list(np.arange(start, stop + 0.1 * step, step))
        else:
            # It's not a list, a range definition or the name of a constant.
            # It must be a number.
            try:
                my_result = float(candidate)
            except ValueError:
                # It wasn't a number either.
                err = ('> Came across a faulty line of input in \n'
                       '> "' + file_name + '".\n'
                       '> The line had an entry that was not a list,\n'
                       '> the name of a constant, a valid number or\n'
                       '> a range definition.\n'
                       '> Instead it was "' + candidate + '".\n'
                       '> Please edit the file to correct it.\n'
                      )
                gen.WriteError(2394, err, log)
                gen.ErrorOnLine(line_number, line_text, log, False)
                return(None)
            else:
                constant = False
                # Check for "NaN" and "inf".
                result = gen.CheckNanInf(candidate, file_name, line_number,
                                     line_text, log)
                if result is None:
                    return(None)

        # Once we get to here we have one of the following in my_result:
        #   * a list of entries defined in the line we are processing
        #   * a list of entries defined in the constants block
        #   * a number defined in the line we are processing
        #   * a number defined in the constants block
        #   * a list of entries generated by a range definition.
        #
        # We check if we have a multiplier and a list and use the multiplier
        # on the list if we do.
        try:
            mult
        except UnboundLocalError:
            # We did not set a multiplier.  All is well, we don't need to do
            # anything else.
            pass
        else:
            # We only want a multiplier if we had a list.  We don't want it
            # in numbers or ranges.  Note that ranges return a list so you
            # can mix lists (with or without) multipliers with ranges (without
            # multipliers), e.g.
            #  [10090] * 1 + startstopcount(10210, 11350, 120) + [11360]
            # This is because the lists will mostly be used for setting
            # properties of things like cross-passages and jet fans, while
            # the ranges will mostly be used for setting the locations of
            # cross-passages and jet fans along a route.
            if allow_mult:
                my_result = my_result * mult
            elif type(my_result) is list:
                # We set a range definition.
                err = ('> Came across a faulty line of input in \n'
                       '> "' + file_name + '".\n'
                       '> The line had a range definition:\n'
                       '>   "'+ candidate
                         + '"\n'
                       '> and a multiplier (' + str(mult)
                         + ').  Multipliers can\n'
                       '> only be used in conjunction with lists\n'
                       '> or constants that return lists.\n'
                       '> Please edit the file to remove the\n'
                       '> multiplier or change the range to a list.'
                      )
                gen.WriteError(2395, err, log)
                gen.ErrorOnLine(line_number, line_text, log)
                return(None)
            else:
                # We set a number and a multiplier or a range and a
                # multiplier, which are not allowed.
                if constant:
                    err = ('> Came across a faulty line of input in \n'
                           '> "' + file_name + '".\n'
                           '> The line had a constant (' + candidate
                             + ') that\n'
                           '> returned a number (' + str(my_result)
                             + ') and a multiplier\n'
                           '> (' + str(mult)
                             + ').  Multipliers can only be used in\n'
                           '> conjunction with lists or constants that\n'
                           '> return lists.\n'
                           '> Please edit the file to correct the line or\n'
                           '> the definition of the constant.'
                          )
                    tr_index = constants_dict[candidate][-1]
                    oldline = line_triples[tr_index]
                    gen.WriteError(2396, err, log)
                    gen.ErrorOnTwoLines(oldline[0], oldline[2],
                                        line_number, line_text, log)
                    return(None)
                else:
                    err = ('> Came across a faulty line of input in \n'
                           '> "' + file_name + '".\n'
                           '> The line had a number (' + candidate
                             + ') and a multiplier\n'
                           '> (' + str(mult)
                             + ').  Multipliers can only be used in\n'
                           '> conjunction with lists or constants that\n'
                           '> return lists.\n'
                           '> Please edit the file to remove the\n'
                           '> constant or change the number to a list.'
                          )
                    gen.WriteError(2397, err, log)
                    gen.ErrorOnLine(line_number, line_text, log)
                    return(None)
    # Return the list of values or a single value.
    return(my_result)


def CheckForConstant2(word, constants_dict, settings_dict, line_triples,
                      line_number, line_text, integer, log):
    '''Check if a word is a number or the name of a constant that returns
    a number.  Fault if the word is not a number, not the name of a constant
    or the name of a constant that returns a list.
    Optionally fault if the number is not an integer.

        Parameters:
            word            str             A word that may be a number or
                                            the name of a constant.
            constants_dict  {} or str       A dictionary of constants without
                                            "#" before the name of the constant
                                            (this exists because when we call
                                            this routine from ProcessSettings
                                            its entries are not in settings_dict
                                            yet).
                                            Or a string "spoof_it".
            settings_dict   {}              Dictionary of the run settings.
            line_number     int             The line number the constant was
                                            used on.
            line_text       str             The entire line, including comments.
            line_triples    [int, str, str] Data for all the lines (in case we
                                            need to refer to a line defining a
                                            constant in an error message).
            integer         Bool            True if the number needs to be an
                                            integer, False if it does not.
            log             handle          The handle of the logfile.

        Returns:
            optionals_dict  {}              All the optional entries on the line.
            line_data       str             All the valid data on the line, but
                                            excluding optional entries.

        Errors:
            Aborts with 2363 if the word was the name of a constant that
            was assigned to a list.
            Aborts with 2364 if the word was not the name of a constant and
            not a number (because it is a list).
            Aborts with 2365 if the word was the name of a constant that
            was not an integer when an integer is required.
            Aborts with 2366 if the word was not an integer when an
            integer is required.
    '''
    file_name = settings_dict["file_name"]
    debug1 = settings_dict["debug1"]

    # Check if this call occurs after the constants have been loaded
    # into settings_dict.
    constants_dict = SpoofConstants(constants_dict, settings_dict)


    # The entry should be a number or the name of a constant
    # that returns a number, not a list.
    if word.lower() in constants_dict:
        # It is the name of a constant.  Check that
        # the value returned by that constant is a
        # number, not a list.
        try:
            word_sub = constants_dict[word.lower()][0]
            value = float(word_sub)
        except (TypeError, ValueError):
            err = ('> Came across a faulty line of input in \n'
                   '> "' + file_name + '".\n'
                   '> The line used a constant named "' + word + '"\n'
                   '> that was assigned to a list, not a number.\n'
                   '> In the context of this line of input, the\n'
                   '> constant must be assigned to a number but\n'
                   '> this returned "' + str(word_sub) + '" instead.\n'
                   '> Please edit the file to remove the name\n'
                   '> "' + word + '" from the line or change the\n'
                   '> definition of "' + word + '" to be a number.'
                  )
            tr_index = constants_dict[word][-1]
            oldline = line_triples[tr_index]
            gen.WriteError(2363, err, log)
            gen.ErrorOnTwoLines(oldline[0], oldline[2],
                                line_number, line_text, log)
            return(None)
    else:
        # It is not in the dictionary of constants.  Check if it is a number.
        try:
            value = float(word)
        except ValueError:
            if integer:
                text = "an integer"
            else:
                text = "a real number"
            # It is neither a number nor the name of a constant.
            err = ('> Came across a faulty line of input in \n'
                   '> "' + file_name + '".\n'
                   '> The line used a value that should have been\n'
                   '> ' + text + ' but which was "' + word + '".\n'
                   '> Please edit the file to correct the entry\n'
                   '> to a number or the name of a constant.'
                  )
            gen.WriteError(2364, err, log)
            gen.ErrorOnLine(line_number, line_text, log, False)
            return(None)
        else:
            # Set the substitute word to word (this is for consistency
            # with the variable set when it is a constant).
            word_sub = word
            # Check for "NaN" and "inf".
            result = gen.CheckNanInf(word_sub, file_name, line_number,
                     line_text, log)
            if result is None:
                return(None)

    # If we get to here we have a valid number, either directly or from
    # a constant.  Check if we need to fault if it is not an integer.
    if integer:
        try:
            value = int(word_sub)
        except ValueError:
            # It is not an integer when we want an integer.
            if word.lower() in constants_dict:
                err = ('> Came across a faulty line of input in \n'
                       '> "' + file_name + '".\n'
                       '> The line used a constant named "' + word + '"\n'
                       '> that was assigned to a real number, not\n'
                       '> an integer.  In the context of this line\n'
                       '> of input, the constant must return an\n'
                       '> integer, not "' + word_sub + '".\n'
                       '> Please edit the file to remove the name\n'
                       '> "' + word + '" from the line or change the\n'
                       '> the definition of the constant "' + word + '"\n'
                       '> to be an integer.'
                      )
                tr_index = constants_dict[word][-1]
                oldline = line_triples[tr_index]
                gen.WriteError(2365, err, log)
                gen.ErrorOnTwoLines(oldline[0], oldline[2],
                                    line_number, line_text, log)
            else:
                err = ('> Came across a faulty line of input in \n'
                       '> "' + file_name + '".\n'
                       '> The line used a list multiplier that should\n'
                       '> have been an integer but which was a real\n'
                       '> number (' + word_sub + '). Please edit the file to\n'
                       '> change the entry to an integer.'
                      )
                gen.WriteError(2366, err, log)
                gen.ErrorOnLine(line_number, line_text, log, False)
            return(None)
    return(value)


def SpoofConstants(constants_dict, settings_dict):
    '''Take a dictionary of constants and a dictionary of settings.  If the
    constants dictionary is actually a string ("spoof_it"), build the
    constants dictionary from the contents of settings_dict.

        Parameters:
            constants_dict  {} or str       A dictionary of constants without
                                            "#" before the name of the constant
                                            (this exists because when we call
                                            this routine from ProcessSettings
                                            its entries are not in settings_dict
                                            yet).
                                            Or a string "spoof_it".
            settings_dict   {}              Dictionary of the run settings.

        Returns:
            constants_dict  {}              The original or recreated
                                            dictionary of constants.
    '''
    if constants_dict == "spoof_it":
        constants_dict = {}
        for key in settings_dict:
            if key[0] == "#":
                constants_dict.__setitem__(key[1:], settings_dict[key])
    return(constants_dict)


def GetBegins(line_triples, begin_lines, block_name, min_entries, max_entries,
              file_name, debug1, log):
    '''Make a list of all the blocks (or sub-blocks in the current block)
    that have a particular block name, such as "tunnel".  Check the count
    of blocks against how many we expect (set by min_entries to max_entries,
    which could be zero to math.inf).  Finish when we encounter an "end"
    command that finishes the current block.

        Parameters:
            line_triples [(int, str, str)]  List of lines in the file.  First
                                            entry is the line number in the file
                                            (starting at one, not zero).
                                            Second is the valid data on the line.
                                            Third is the entire line (including
                                            comments) also used in error messages.
            begin_lines     [int]           List of which entries in line_triples
                                            are top-level "begin" blocks.
            block_name      str             The word we expect after "begin"
            min_entries     int             The minimum number of blocks we want
            max_entries     int             The maximum number of blocks we want
            file_name       str             The file name without the
                                            file path.
            debug1          bool            The debug Boolean set by the user.
            log             handle          The handle of the logfile.

        Returns:
            A list of the line numbers

        Errors:
            Aborts with 2061 if there were two blocks and one was wanted
            Aborts with 2062 if there were too many blocks
            Aborts with 2063 if there were too few blocks and one
            block was needed.
            Aborts with 2064 if there were too few blocks and more
            than one block was needed.
    '''
    # Make a list to hold the index numbers of the lines that match
    # the "begin" block we want.
    matches = []
    # Make a counter.  We increment it each time we encounter a "begin"
    # and decrement it each time we encounter an "end".  If it goes
    # negative we break out of the loop, because we've finished the
    # current block.  This lets us look for "begin graph" but only
    # on the current "page" block: the moment we hit "end page this
    # counter goes negative.
    end_counter = 0
    for entry in begin_lines:
        (line_number, line_data, line_text) = line_triples[entry]
        beginning = line_data.lower().split()
        if beginning[0] == "begin":
            end_counter += 1
            if block_name == "#name" or beginning[1] == block_name:
                matches.append(entry)
        elif beginning[0] == "end":
            end_counter -= 1
        if end_counter < 0:
            break
    if len(matches) > max_entries:
        # We have too many of this kind of block.  Complain
        # about the excess and give the line numbers of the
        # superfluous blocks.
        line_nos = []
        for tr_index in matches:
            line_nos.append(gen.Enth(line_triples[tr_index][0]))
        extras = len(line_nos) - 1
        if extras == 1:
            (line_number, line_data, line_text) = line_triples[matches[1]]
            err = ('> Found more than one "begin ' + block_name + '" blocks in\n'
                   '> "' + file_name + '".\n'
                   '> The ' + line_nos[0] + ' line started a "begin ' + block_name + '" block\n'
                   '> that was processed successfully, but a second\n'
                   '> "begin ' + block_name + '" block was encountered later.\n'
                   '> Please edit the file to remove the conflict.\n'
                   '> Either merge the blocks or remove one of them.'
                  )
            gen.WriteError(2061, err, log)
            gen.ErrorOnLine(line_number, line_text, log, False)
        else:
            err = ('> Found too many "begin ' + block_name + '" blocks in\n'
                   '> "' + file_name + '".\n'
                   '> The ' + line_nos[0] + ' line started a "begin files" block\n'
                   '> that was processed successfully, but more\n'
                   '> "begin ' + block_name + '" blocks were encountered on\n'
                   '> the following lines:\n'
                   + gen.FormatOnLines(line_nos[1:]) + '\n'
                   '> Please edit the file to remove the conflict.\n'
                   '> Either merge the blocks or remove all but '
                   + str(max_entries) + '.'
                  )
            gen.WriteError(2062, err, log)
        return(None)
    elif len(matches) < min_entries:
        # We have too few of this kind of block.  Complain
        # about it.
        if min_entries == 1:
            err = ('> Failed to find a "begin ' + block_name + '" block in\n'
                   '> the file "' + file_name + '".\n'
                   '> Please edit the file to add one.'
                  )
            gen.WriteError(2063, err, log)
        else:
            # Not sure if there are any instances where we'll need more than
            # one block, but you never know.  We test this by editing
            # the source.
            err = ('> Failed to find enough "begin ' + block_name + '" blocks in\n'
                   '> "' + file_name + '".\n'
                   '> Please edit the file to add at least ' + str(min_entries) + '.'
                  )
            gen.WriteError(2064, err, log)
        return(None)
    return(matches)


def ProcessSettings(line_triples, settings_dict, log):
    '''Find the "begin settings...end settings" block in the input file
    and turn its values into a dictionary of keys and values.  Most
    settings are optional but three are mandatory: we must have
    "frictiontype", "runtype" and "version".
    All the keys are converted to lower case, as are all values except
    those that are QA strings.

        Parameters:
            line_triples [(int, str, str)]  List of lines in the file.  First
                                            entry is the line number in the file
                                            (starting at one, not zero).
                                            Second is the valid data on the line.
                                            Third is the entire line (including
                                            comments) also used in error messages.
            settings_dict   {}              Dictionary of the run settings.
            log             handle          The handle of the logfile.

        Returns:
            settings_dict   {}              The updated settings dictionary.

        Errors:
            Aborts with 2121 if there was no setting for "version".
            Aborts with 2122 if there was no setting for "runtype".
            Aborts with 2123 if there was no setting for "frictiontype".
    '''
    debug1 = settings_dict["debug1"]
    file_name = settings_dict["file_name"]
    # Create a variable to track the current line in line_triples.
    tr_index = 0
    # Add the default values to the settings dictionary with an uppercase
    # first letter in the result, which we use to track duplicate entries.
    # After we finish we turn all the remaining uppercase values to lower
    # case and set the QA strings to "" if they still contain "#".
    settings_dict.__setitem__("version",("Not set",))
    settings_dict.__setitem__("runtype", ("Not set",))
    settings_dict.__setitem__("frictiontype", ("Not set",))
    settings_dict.__setitem__("frictionapprox", ("Colebrook",))
    settings_dict.__setitem__("units", ("SI",))
    settings_dict.__setitem__("qa1", ("No project number#",))
    settings_dict.__setitem__("qa2", ("No project name#",))
    settings_dict.__setitem__("qa3", ("No project text#",))
    settings_dict.__setitem__("p_atm", ("Not set",))
    settings_dict.__setitem__("rho_atm", ("Not set",))
    settings_dict.__setitem__("header", ("on",))
    settings_dict.__setitem__("footer", ("on",))
    settings_dict.__setitem__("plotnames", (".txt",))
    settings_dict.__setitem__("images", ("optional",))
    settings_dict.__setitem__("solver", ("moc2",))
    settings_dict.__setitem__("autokeys", ("off",))

    # Set a value for the scaling factor of text in keys on graphs.  The
    # default is 65% of default text size, because most long autokeys
    # seem to fit inside the graph frame at that scaling factor.
    settings_dict.__setitem__("keytextscale", 0.65)

    # We set a value for the maximum roughness height in SI input files,
    # 0.5 metres.  This is to catch users who set roughness heights in mm
    # instead of metres.  0.5 metres is a good limit, but can be exceeded
    # in real calculations: I have had vent ducts with super-T beams
    # in the roof where we ended up using a roughness height of 1.8 m
    # in SES to get the correct friction factor for the duct.
    settings_dict.__setitem__("max_roughness", (0.5,))

    # We set a value for the ratio of specific heats for air, gamma.
    # This is not set as a constant in case something turns up where
    # it needs to be changed.
    settings_dict.__setitem__("gamma", (1.4,))

    # Set a value that we add to the speed of sound when calculating
    # the minimum spacing of cells.  20 m/s seems like a good figure
    # to use for the homentropic method of characteristics solver
    # activated by "moc2".
    settings_dict.__setitem__("max_vel", (20.0,))

    # Set a rise time.  This is used to ramp up the velocity value at
    # portals that have a fixed velocity or volume flow.  Imposing the
    # fixed velocity value in one timestep can lead to stability problems,
    # so we ramp up the fixed velocity over the first 2 seconds to avoid
    # all but the worst of them (2 seconds seem to work well enough).
    settings_dict.__setitem__("rise_time", (2.0,))

    # Set a count of decimal places that we round times to before trying
    # to compare them.  Times generated by different ways (set by the
    # user, generated by numpy.arange) may be expected to be the same
    # but actually differ by a few picoseconds (10^-12 seconds) due to
    # the inexactness of floating-point arithmetic.  If we round to
    # eight decimal places, we'll probably end up with the matches we
    # want to return True returning True.
    settings_dict.__setitem__("time_accuracy", (8,))

    # Set the standard acceleration of gravity in m/s (set at the 3rd
    # General Conference on Weights and Measures, 1901).  Note that
    # this is not used in the conversion of pressures from SI units to
    # inches of water or inches of mercury.
    settings_dict.__setitem__("g", (9.80665,))

    # Set the minimum annulus area we can accept, 0.1 m^2.  This
    # should be fine except where someone uses Hobyah to model a
    # small-scale test rig, so we give them the option to change it.
    settings_dict.__setitem__("min_area", (0.1,))

    # Require that the count of jet fans in banks of jet fans be
    # an integer.
    settings_dict.__setitem__("jetfancounts", ("integers",))




    # We make a dictionary of valid settings that identifies what
    # entries and how many of them we expect after each setting.
    # We have four types: any integer, any real number, any one
    # of a list of entries and any string (this isn't as mad as it
    # sounds; it is useful for QA data).
    # In the cases where we have only one value we make a one-element
    # tuple.
                                        # type  range  conversion   description
    valid_settings = {"version":        ("int   any      null      a version number",),
                      "runtype":        (("calc", "plot"),),
                      "frictiontype":   (("fanning", "darcy", "atkinson"),),
                      "frictionapprox": (("colebrook", "colebrook-white",
                                          "moody", "ses"),),
                      "units":          (("si", "us"),),
                      "qa1":            ("QAstr",), # The project number
                      "qa2":            ("QAstr",), # The project name
                      "qa3":            ("QAstr",), # The project description
                      # Outside air pressure (Pa) and density (kg/m3).  We
                      # don't convert in ProcessBlock in case the definition
                      # of the units appears after the pressure or density.
                      "p_atm":          ("float + press2 outside air pressure",),
                      "rho_atm":        ("float + dens1  outside air density",),
                      "aero_step":      ("float + null   a timestep",),
                      "aero_time":      ("float + null   a runtime",),
                      "footer":         (("off", "on"),),
                      "header":         (("off", "on", "underfoot"),),
                       # Some users may want the curve filenames to be .csv.
                       # rather than .txt.  They can use "plotnames .csv".
                      "plotnames":   ((".txt", "txt", ".csv", "csv"),),
                      # If an image file cannot be found, the program will
                      # complain ("images optional" is the default).  For
                      # stuff that will end up in reports it is better to
                      # set "images required" so that a fault is called.
                      # When developing an input file it is often handy to
                      # not process the images at all, especially when gnuplot
                      # has to rotate them (this is slow).  The "hidden"
                      # option tells the program to show placeholders for all
                      # the images.
                      "images":         (("optional", "required", "hidden"),),
                      "keytextscale":   ("float + null   a key text scale "
                                                            "factor (0-1)",),
                      # This one can be changed by users in MoC calculations
                      # that glitch out because the air velocities are high.
                      "max_vel":        ("float 0+ null  an air velocity to"
                                                         " add to c",),
                      # These next four control figures in the method of
                      # characteristics calculation, ratio of specific heats
                      # and air velocity to add when calculating the grid
                      # size.  They are only really of use when debugging.
                      "gamma":          ("float + null   a ratio c_p / c_v",),
                      "rise_time":      ("float 0+ null   a rise time",),
                      # No need to change any of the following, but we might
                      # as well have them available for change for completeness.
                      "time_accuracy":  ("int 0+ null   a rise time",),
                      "g": ("float + accel   gravitational acceleration",),
                      "autokeys": (("on", "off"),),
                      "solver":   (("moc2",),),
                      "min_area":  ("float + null   a minimum annulus area",),
                      "jetfancounts": (("integers", "nonintegers"),),
                      # To do: solver and max_roughness.
                     }

    block_name = "settings"
    # Spoof the list of required settings, dictionary of optional entries
    # and the list of duplicates.  We will handle the required settings here
    # with specific error messages.
    settings = (valid_settings, [], {}, [])
    # Now call ProcessBlock.  It returns None if an error occurred.  It
    # returns a tuple of the updated settings block and the index to the
    # line of the next block if everything went OK.
    # Note that we pass settings_dict twice b
    result = ProcessBlock(line_triples, tr_index, settings_dict,
                          block_name, settings_dict, settings, log)
    if result is None:
        return(None)
    else:
        (discard, settings_dict) = result

    # Figure out the line number of the start of the settings block, in
    # case we need it in error messages.
    blockstart = gen.Enth(line_triples[tr_index][0])

    # Check if the three mandatory entries have been set and fault if
    # they haven't.
    if settings_dict["version"] == ("Not set",):
        err = ('> There is no setting for input file version number in\n'
               '> the file "' + file_name + '".\n'
               '> Please add "version 1" (without double quotes) to\n'
               '> the settings block (it starts on the ' + blockstart
               + ' line).'
              )
        gen.WriteError(2121, err, log)
        return(None)
    if settings_dict["runtype"] == ("Not set",):
        err = ('> There is no setting for the run type in the file\n'
               '> "' + file_name + '".\n'
               '> Please add "runtype plot" or "runtype calc" (without\n'
               '> double quotes) to the settings block (it starts on\n'
               '> the ' + blockstart + ' line).'
              )
        gen.WriteError(2122, err, log)
        return(None)
    if settings_dict["frictiontype"] == ("Not set",):
        err = ('> There is no setting for friction type in the file\n'
               '> "' + file_name + '".\n'
               '> Please add "frictiontype Darcy", "frictiontype Fanning"\n'
               '> or "frictiontype Atkinson" (without double quotes) to\n'
               '> the settings block (it starts on the '
                 + blockstart + ' line).\n'
               '> You must make a choice.\n'
               '> If you are unsure which to use (or were unaware that\n'
               '> there is more than one type of friction factor) then\n'
               '> please read "friction-rant.pdf"'" in Hobyah's\n"
               '> documentation folder.'
              )
        gen.WriteError(2123, err, log)
        return(None)

    # If the runtype is "calc", check that a timestep and runtime have
    # been set, complain if they haven't and check that they are sane.
    if settings_dict["runtype"][0] == "calc":
        if "aero_step" not in settings_dict:
            err = ('> There is no setting for the aero timestep in the\n'
                   '> file "' + file_name + '".\n'
                   '> In files that run a calculation an aero timestep\n'
                   '> must be set.  Something like "aero_step 0.05" to\n'
                   '> set a timestep in seconds.  Please add one to the\n'
                   '> "settings" block (it starts on the '
                   + blockstart + ' line).\n'
                   '> Alternatively, change the runtype to "plot" in\n'
                   '> the "settings" block so that the calculation is\n'
                   '> skipped.'
                  )
            gen.WriteError(2124, err, log)
            return(None)
        step_data = settings_dict["aero_step"]
        dt = step_data[0]
        # Check that the timestep is sane.  With computers being
        # so fast, we'll set an upper limit of 0.25 seconds, which
        # is probably about five times larger than a typical
        # timestep in the 2020s.
        if dt > 0.25:
            line_number, discard, line_text = line_triples[step_data[-1]]
            err = ('> The aero timestep in the file "' + file_name + '"\n'
                   '> is too high (over 0.25 seconds).\n'
                   '> Please set a value less than that (a typical\n'
                   '> range is 0.005 to 0.1 seconds).'
                  )
            gen.WriteError(2125, err, log)
            gen.ErrorOnLine(line_number, line_text, log, False)
            return(None)

        # Now check for a sane runtime.
        if "aero_time" not in settings_dict:
            err = ('> There is no setting for the aero runtime in the\n'
                   '> file "' + file_name + '".\n'
                   '> In files that run a calculation an aero runtime\n'
                   '> must be set.  Something like "aero_time 700" to\n'
                   '> set the run duration in seconds.  Please add one\n'
                   '> to the "settings" block (it starts on the '
                   + blockstart + ' line).\n'
                   '> Alternatively, change the runtype to "plot" in\n'
                   '> the "settings" block so that the calculation is\n'
                   '> skipped.'
                  )
            gen.WriteError(2126, err, log)
            return(None)
        else:
            # Check that the run time is not less than the aero timestep.
            # If it is, round it up so it is equal to it.  This avoids
            # a crash when using np.arange() in ProcessCalc.
            runtime_data = list(settings_dict["aero_time"])
            runtime = runtime_data[0]
            if runtime < dt:
                runtime_data[0] = dt
                settings_dict.__setitem__("aero_time", tuple(runtime_data))
                gen.WriteOut("Rounded up the runtime to one timestep.", log)
                print("Rounded up the runtime to one timestep.")
    # The ProcessBlock routine returns a list in its dictionary values,
    # because with most blocks we have multiple values.  The settings block
    # is set up such that the arguments all have one value, not two, so
    # it is better to not have them as lists.  We run through all the
    # entries and convert the lists to the first entry.  If the first entry
    # is a string we convert it to lower case.  We do this in the plots
    # block too, so we do it in a subroutine.
    settings_dict = FlattenSettings(settings_dict)

    # Check for entries in US units.  If the user set new values, convert
    # them to SI units.  Note that if we have the default it has already
    # been changed from "('Not set',)" to "not set" (flattening).
    if settings_dict["units"] == "us":
        pressure = settings_dict["p_atm"]
        density = settings_dict["rho_atm"]
        if pressure != "not set":
            # The user did set it, so convert it to SI
            SI_press = USc.ConvertToSI("press2", pressure, debug1, log)
            settings_dict.__setitem__("p_atm", SI_press[0])

        if density != "not set":
            SI_density = USc.ConvertToSI("dens1", density, debug1, log)
            settings_dict.__setitem__("rho_atm", SI_density[0])

    # Check for unset pressure and density and set them to the defaults in
    # SI units if they were not set.
    if settings_dict["p_atm"] == "not set":
        settings_dict.__setitem__("p_atm", 101325.0)
    if settings_dict["rho_atm"] == "not set":
        settings_dict.__setitem__("rho_atm", 1.2)



    # Check the entry for plotnames, turn "txt" into ".txt" and
    # turn "csv" into ".csv".
    if "." not in settings_dict["plotnames"]:
        settings_dict.__setitem__("plotnames", "." + settings_dict["plotnames"])

    # Check the QA entries and clear the ones that weren't set.  We
    # can tell the ones that weren't set because the default entries
    # contain '#', which is impossible to set in the input file.
    if "#" in settings_dict["qa1"]:
        settings_dict.__setitem__("qa1", "No project number")
    if "#" in settings_dict["qa2"]:
        settings_dict.__setitem__("qa2", "No project name")
    if "#" in settings_dict["qa3"]:
        settings_dict.__setitem__("qa3", "No project description")
    # Write the settings to the log file.
    gen.LogBlock(settings_dict, block_name, debug1, log)

    return(settings_dict)


def FlattenSettings(settings_dict):
    '''Take an updated dictionary of run settings, go through each
    of its values.  If the value is a tuple, we've updated the setting
    since we last flattened it.  Redefine the value to be the first
    entry in the tuple.  This turns the likes of

      {"plotunits": ["si", {}, 32]}

    into

      {"plotunits": "si"}

    i.e. we've thrown away the optional arguments (which we don't need
    in settings) and the index in line_triples (which we also don't need).

        Parameters:
            settings_dict   {}              Dictionary of the run settings.

        Returns:
            settings_dict   {}              Altered dictionary.
    '''
    for key in settings_dict:
        value = settings_dict[key]
        if type(value) is tuple:
            setting_value = value[0]
            # Some settings are numbers, some are strings.
            if type(setting_value) is str:
                # Some string settings are QA strings that we want
                # to retain the case of.  All others are converted to
                # lower case.
                if key not in ("qa1", "qa2", "qa3"):
                    settings_dict.__setitem__(key, setting_value.lower())
                else:
                    settings_dict.__setitem__(key, setting_value)
            else:
                # It's just a number or a list
                settings_dict.__setitem__(key, setting_value)
    # print("Flattened the settings")
    # for key in settings_dict:
    #     value = settings_dict[key]
    #     print(key, ":", value)
    return(settings_dict)


def ProcessCsvData(line_triples, tr_index, settings_dict, used_nicknames, log):
    '''Take a line in the "begin csv...end csv" block in the input file
    and turn its values into a dictionary of keys and values.  Each line
    of entry is a one-word nickname and the rest of the line is
    the names of a .csv file (including the ".csv").

    The keys are the nicknames converted to lower case, the values
    they yield are the name of the input file (with its extension
    so we can distinguish what type of file it is).

        Parameters:
            line_triples [(int, str, str)]  List of lines in the file.  First
                                            entry is the line number in the file
                                            (starting at one, not zero).
                                            Second is the v alid data on the line.
                                            Third is the entire line (including
                                            comments) also used in error messages.
            tr_index        int             Where to start reading the files block.
            settings_dict   {}              Dictionary of the run settings.
            used_nicknames  (,)             A tuple of nicknames that have
                                            already been used for data blocks.
            log             handle          The handle of the logfile.

        Returns:
            csv_dict        {}              A dictionary with the contents of
                                            the .csv files stored in it.

        Errors:
            Aborts with 2261 if line of entry didn't have a nickname and a
            file name separated by one or more spaces.
            Aborts with 2262 if a file name didn't end in ".csv" or ".txt"
            Aborts with 2263 if a nickname was a reserved word.
            Aborts with 2264 if a nickname was a duplicate.
            Aborts with 2265 if a nickname is the nickname of a "data"
            block.
            Aborts with 2266 if a .csv file name consisted of nothing
            but ".csv".
            Aborts with 2267 if a .csv file doesn't exist.
            Aborts with 2268 if a .csv file exists but is locked.
    '''
    debug1 = settings_dict["debug1"]
    file_name = settings_dict["file_name"]
    dir_name = settings_dict["dir_name"]
    reserved = settings_dict["reserved"]

    # Create a list of nicknames so we can catch duplicates and an
    # empty dictionary to hold the data.
    nicknames = []
    csv_dict = {}

    # Iterate over the lines after the one containing "begin csv"
    # until we reach the line with "end csv" in it.
    block_start = tr_index
    while tr_index < len(line_triples):
        tr_index += 1
        (line_number, line_data, line_text) = line_triples[tr_index]
        if line_data.split() == ["end", "csv"]:
            # We are at the end of the block
            break

        # There should be a custom key as the first word on the line
        # and the rest of the line is a file name.
        words = line_data.split(maxsplit = 1)

        # Complain if there is only one word on the line.
        if len(words) == 1:
            err = ('> The "csv" block in "' + file_name + '"\n'
                   '> has been told to read a nickname and a\n'
                   '> file name, but one of the entries had\n'
                   '> nothing but the nickname "' + words[0] + '".\n'
                   '> Please add a file name after the nickname.\n'
                   '> If the nickname is actually a file name\n'
                   '> then please add a nickname before it.'
                  )
            gen.WriteError(2261, err, log)
            gen.ErrorOnLine(line_number, line_text, log, False)
            return(None)
        else:
            (csv_nickname, csv_full_name) = words

        # Now check if there is ".csv" at the end of the line.
        if csv_full_name[-4:].lower() not in (".csv", ".txt"):
            err = ('> There is an invalid entry in a "begin csv"\n'
                   '> block in "' + file_name + '".\n'
                   '> The file "' + csv_full_name + '" has an\n'
                   '> incorrect extension or no extension.  The\n'
                   '> names of CSV files must end in ".csv" or ".txt".'
                  )
            gen.WriteError(2262, err, log)
            gen.ErrorOnLine(line_number, line_text, log, False)
            return(None)


        # Check for reserved words in the nickname.
        if csv_nickname in reserved:
            err = ('> The "csv" block in "' + file_name + '"\n'
                   '> has been told to read a nickname and a\n'
                   '> file name.  One of the nicknames was\n'
                   '> "' + csv_nickname + '", which is not allowed because\n'
                   '> it is a word reserved for other uses.\n'
                   '> Please choose a different word.'
                  )
            gen.WriteError(2263, err, log)
            gen.ErrorOnLine(line_number, line_text, log, False)
            return(None)

        # If we get to here, we probably have a valid file name.  We
        # might have an invalid one with a path (e.g. "/Users/me/.csv")
        # so we'll check for it later.

        # Check if we have any duplicate nicknames (we don't check for
        # duplicate file names).
        if csv_nickname.lower() in nicknames:
            # Figure out where the earlier definition was.
            offset = len(nicknames) - nicknames.index(csv_nickname.lower())
            oldline = line_triples[tr_index - offset]
            err = ('> The "csv" block in "' + file_name + '"\n'
                   '> has duplicate nicknames: "' + csv_nickname + '"\n'
                   '> appears twice (note that nicknames are not\n'
                   '> case sensitive).  Please rename one of them.'
                  )
            gen.WriteError(2264, err, log)
            gen.ErrorOnTwoLines(oldline[0], oldline[1],
                                line_number, line_text, log)
            return(None)
        elif csv_nickname.lower() in used_nicknames:
            # The nickname of this .csv file has already been used as the
            # nickname of a "begin data...end data" block.
            err = ('> Came across a nickname for a .csv file that\n'
                   '> has already been used as the nickname for a\n'
                   '> "begin data...end data" block.  The duplicate\n'
                   '> nickname is "' + csv_nickname + '" in the file\n'
                   '> "' + file_name + '".\n'
                   '> Please change one nickname or the other.'
                  )
            gen.WriteError(2265, err, log)
            gen.ErrorOnLine(line_number, line_text, log, False)
            return(None)
        else:
            nicknames.append(csv_nickname.lower())

        # Get the file name and path.  If only a file name is given the
        # path will be the same as the path to the input file's folder.
        (csv_file_name, csv_dir_name, file_stem,
               file_ext) = gen.GetFileData(csv_full_name, "", debug1, dir_name)
        if csv_file_name.lower() == ".csv":
            err = ('> Came across a file name that consisted of\n'
                   '> nothing but the extension ".csv" in the\n'
                   '> "csv" block of "' + file_name + '".\n'
                   '> Please give it a name.'
                  )
            gen.WriteError(2266, err, log)
            gen.ErrorOnLine(line_number, line_text, log, False)
            return(None)


        # Check that the .csv file exists and can be read.
        try:
            handle = open(csv_dir_name + csv_file_name, "r", encoding="UTF-8")
        except FileNotFoundError:
            # The .csv file doesn't exist.  Complain and return.
            err = ('> One of the .csv files in the "csv" block\n'
                   '> of "' + file_name + '"' + " doesn't exist.\n"
                   '> The missing file is "' + csv_file_name + '".\n'
                   '> Please edit the file name to correct it or\n'
                   '> remove it from the list of files.'
                  )
            gen.WriteError(2267, err, log)
            gen.ErrorOnLine(line_number, line_text, log, False)
            return(None)
        except PermissionError:
            # The .csv file exists but we don't have permission
            # to read it.  Complain and return.
            err = ('> One of the .csv files in the "csv" block\n'
                   '> of "' + file_name + '"' + " exists but.\n"
                   "> but you don't have permission to read it.\n"
                   '> The locked file is "' + csv_file_name + '".\n'
                   '> Please remove it from the list of files.'
                  )
            gen.WriteError(2268, err, log)
            gen.ErrorOnLine(line_number, line_text, log, False)
            return(None)
        # Read the contents of the .csv file into a list of lines.  We
        # spoof it as a "begin data...end data" block.
        try:
            csv_lines = handle.readlines()
        except UnicodeDecodeError:
            # These often turn up when odd characters are pasted from
            # Word or Excel.
            pass
        except:
            print("CSV decode error", csv_file_name)
        # Now get the contents as line triples.  Spoof the contents
        # of the .csv file as a "begin data" block.
        csv_triples = [[0, "begin data " + csv_nickname, "begin data " + csv_nickname]]
        for index, line in enumerate(csv_lines, start = 1):
            if "#" in line:
                # Split the data from the comment
                (data, discard) = line.split("#", maxsplit = 1)
            else:
                data = line
            # See if this line that has data on it (some lines
            # will be comments only)
            short_data = data.lstrip().rstrip()
            if short_data != "":
                # It has data on it.  Add the line to the triples.
                csv_triples.append([index, short_data, line.lstrip().rstrip()])

        csv_triples.append([index + 1, "end data", "end data"])
        spoof_settings = settings_dict.copy()
        spoof_settings.__setitem__("file_name", csv_file_name)
        result = ProcessUserData(csv_triples, 0, True, spoof_settings, log)
        if result is None:
            return(None)
        else:
            (block_name_low, data_dict) = result
            # Add the name of the csv file to the data dictionary, under
            # a key that can't be used as a header nickname.
            data_dict.__setitem__("#name", csv_file_name)
            csv_dict.__setitem__("csv_" + block_name_low, data_dict)

            # Replace the index of where the block started in line_triples.
            data_dict.__setitem__("#tr_index", block_start)
    return(csv_dict)


def ProcessUserData(line_triples, tr_index, csv_file, settings_dict, log):
    '''Take a "begin data...end data" block in the input file and
    turn its values into lists of data and keys to access them by.

    The key is the word after the words "begin data".  The values are
    a list of lists, all the numbers in the lists of lines containing
    something along the lines of:

    begin data
       0,  0,  2,   1
      20,  5,  3,   4
      30,  5,  3, 5.2
      40,   ,  7,   4
      50,   ,  8
      60,   ,  4
    end data

    Values are separated by commas, same as a .csv file.  The program checks
    for columns ending before other columns (like the 2nd column in the example
    above, where two commas in sequence mean a missing entry)

    All entries must be numbers or the names of constants defined in the file's
    constants block.  Optionally it may start with a line of entries that give
    one-word names to each column:

    begin data   data_name
      time,   psd_area,  psd_zeta
         0,       0.5,     2.4
       1.5,      12.4,   9.2
      28.5,   12.4,       9.2
      30.0,      0.5,      2.4
    end data

    The names of columns must not be the names of constants and cannot contain
    commas or spaces (use dashes or underscores instead, like the above example).
    Names are not case-sensitive.

    The columns can be referenced by numbers (starting at column 1, not at
    column zero) as well as the names.  The column numbers will have to be
    used if no names are given.
    Spaces between entries in the .csv file are ignored.

    This routine may also be used to process .csv files.

        Parameters:
            line_triples [(int, str, str)]  List of lines in the file.  First
                                            entry is the line number in the file
                                            (starting at one, not zero).
                                            Second is the valid data on the line.
                                            Third is the entire line (including
                                            comments) also used in error messages.
            tr_index        int             Where to start reading the block block.
            csv_file        Bool            False if the block is from a Hobyah
                                            file, True if it is from a .csv file.
            settings_dict   {}              Dictionary of the run settings.
            log             handle          The handle of the logfile.

        Returns:
            block_name_low  str             Name to assign this block of data to.
            data_list       {}              Dictionary of curve data, accessed by
                                            a given name.

        Errors:
            Aborts with 2241-2247 if the first line was a mixture of column
            titles, numbers, names of constants and/or blank entries.  The seven
            messages cover every combination.
            Aborts with 2248 if the name of a column contained a space.
            Aborts with 2249 if the name of a column was a duplicate.
            Aborts with 2250 if a line of data is longer than the header.
            Aborts with 2251 if a line of data contains an entry that is
            not a number or a constant.
    '''
    debug1 = settings_dict["debug1"]
    file_name = settings_dict["file_name"]

    if csv_file:
        # csv files don't have constants.
        constants = []
    else:
        # Get a list of the constants, in case any of them appear.
        constants = [const_key for const_key in settings_dict if '#' in const_key]


    # Get the name of this datasource from the "begin data <something>" line.
    # We know this line has exactly three entries because if it didn't it,
    # would have triggered error 2030 in the syntax checker.
    (line_number, line_data, line_text) = line_triples[tr_index]
    block_name = line_data.split()[2]
    block_name_low = line_data.lower().split()[2]
    block_start = tr_index
    if debug1:
        print("Processing data block", block_name)

    # Check the next line to see if it has names on it.
    tr_index += 1
    (line_number, line_data, line_text) = line_triples[tr_index]

    # Get the words the next line into a list with leading and trailing
    # whitespace removed.
    first_line = [word.lstrip().rstrip() for word in line_data.split(sep=',')]

    # Check if all the words are numbers and/or the names of constants and
    # raise a very detailed set of error messages depending on what we get.
    # Ideally we want them to be all numbers (in which case there are no
    # names), a mix of numbers and the names of constants (ditto) or a
    # set of words that aren't the names of constants, numbers, or blank.
    # We set three False Booleans.  We set the first one True when we find
    # a number.  We set the second one True if we find a word that is not
    # the name of a constant.  We set the third one True if we find a blank
    # entry (to commas together or separated by nothing but whitespace).
    # These are used to pick out mixtures of column names, numbers and
    # constant names and issue an appropriate error message.  We also build
    # a list of all the constant names we find for use in the error message,
    # as that is useful for someone who accidentally tries to give a column
    # the same name as a constant.
    found_number = False
    found_other = False
    found_blank = False
    const_names = []
    for entry in first_line:
        try:
            number = float(entry)
            found_number = True
        except ValueError:
            # It's not a number, check if it is a constant.
            const_key = entry.lower()
            if '#' + const_key in constants:
                const_names.append(const_key)
            elif entry == '':
                # It was a blank entry, which is a bit weird on the first line
                # but we'll allow it through as a non-number.
                found_blank = True
            else:
                # It's not a number or the name of a constant or a blank field.
                # It must be the name of a column header.
                found_other = True

    header_enth = gen.Enth(tr_index)
    if not found_other:
        # All the entries were either numbers, blanks or the names of constants.
        # So all the entries on the line can be reduced to numbers and
        # this line is not line of headers.  Build a list of headers
        # as numbers starting at 1.  Leave the index unchanged so that
        # we reprocess this line as numbers.
        headers = [str(entry + 1) for entry in range(len(first_line))]
    else:
        # We did find entries that were not numbers or the names of constants.
        # Check if we found a number or the name of a constant.  If we did
        # we complain.
        if found_blank and found_number and const_names != []:
            # We had the kitchen sink here.  Some names of column headers,
            # some numbers, some constants and some blank entries.  Complain.
            if csv_file:
                err = ('> The .csv file "' + file_name + '" started\n'
                       '> with a mixture of column names, blank entries\n'
                       '> and numbers.  Please edit the .csv file to\n'
                       '> either remove the column names or remove the\n'
                       '> numbers and blanks.'
                      )
            else:
                err = ('> The "' + block_name + '" datasource in "' + file_name + '"\n'
                       '> started with a mixture of column names, blank\n'
                       '> entries, numbers and the names of constants.\n'
                       '> Please edit the file to either remove the\n'
                       '> column names or remove the numbers, blanks\n'
                       "> and names of constants.  For what it's worth\n"
                       '> the following named constants were used:\n'
                       + gen.FormatOnLines(const_names)
                      )
            gen.WriteError(2241, err, log)
            gen.ErrorOnLine(line_number, line_text, log, False)
            return(None)
        elif found_number and const_names != []:
            # We had names of column headers, some numbers and some constants.
            # Complain.
            if csv_file:
                err = ('> The .csv file "' + file_name + '" started\n'
                       '> with a mixture of column names and numbers.\n'
                       '> Please edit the .csv file to either remove the\n'
                       '> column names or remove the numbers.'
                      )
            else:
                err = ('> The "' + block_name + '" datasource in "' + file_name + '"\n'
                       '> started with a mixture of column names, numbers\n'
                       '> and the names of constants.  Please edit the\n'
                       '> file to either remove the column names or\n'
                       '> remove the numbers and names of constants.  For\n'
                       "> what it's worth the following named constants\n"
                       '> were used:\n'
                       + gen.FormatOnLines(const_names)
                      )
            gen.WriteError(2242, err, log)
            gen.ErrorOnLine(line_number, line_text, log, False)
            return(None)
        elif found_blank and found_number:
            # We had names of column headers, some numbers and some blanks.
            # Complain.
            if csv_file:
                err = ('> The .csv file "' + file_name + '" started\n'
                       '> with a mixture of column names, numbers\n'
                       '> and blank entries.  Please edit the file to\n'
                       '> either remove the column names or remove the\n'
                       '> numbers and blank entries.\n'
                      )
            else:
                err = ('> The "' + block_name + '" datasource in "' + file_name + '"\n'
                       '> started with a mixture of column names, numbers\n'
                       '> and blank entries.  Please edit the file to\n'
                       '> either remove the column names or remove the\n'
                       '> numbers and blank entries.\n'
                      )
            gen.WriteError(2243, err, log)
            gen.ErrorOnLine(line_number, line_text, log, False)
            return(None)
        elif found_blank and const_names != []:
            # We had names of column headers, some blank entries and some
            # constants.  Complain.
            if csv_file:
                err = ('> The .csv file "' + file_name + '" started\n'
                       '> with a mixture of column names and blank\n'
                       '> entries.  Please edit the file to either\n'
                       '> remove the column names or remove entries.\n'
                      )
            else:
                err = ('> The "' + block_name + '" datasource in "' + file_name + '"\n'
                       '> started with a mixture of column names, blanks\n'
                       '> and the names of constants.  Please edit the\n'
                       '> file to either remove the column names or\n'
                       '> remove the blanks and names of constants.  For\n'
                       "> what it's worth the following named constants\n"
                       '> were used:\n'
                       + gen.FormatOnLines(const_names)
                      )
            gen.WriteError(2244, err, log)
            gen.ErrorOnLine(line_number, line_text, log, False)
            return(None)
        elif const_names != []:
            # We had names of column headers and some constants.  This
            # is the most likely case, where someone adds a name that
            # is already used as a constant.  Complain.
            err = ('> The "' + block_name + '" datasource in "' + file_name + '"\n'
                   '> started with a mixture of column names and the\n'
                   '> names of constants.  Please edit the file to\n'
                   '> either remove the column names or remove the\n'
                   "> names of constants.  For what it's worth the\n"
                   '> following named constants were used:\n'
                   + gen.FormatOnLines(const_names)
                  )
            gen.WriteError(2245, err, log)
            gen.ErrorOnLine(line_number, line_text, log, False)
            return(None)
        elif found_number:
            # We had names of column headers and some numbers.
            # Complain.
            if csv_file:
                err = ('> The .csv file "' + file_name + '" started\n'
                       '> with a mixture of column names and numbers\n'
                       '> Please edit the file to either remove the\n'
                       '> column names or remove the numbers.'
                      )
            else:
                err = ('> The "' + block_name + '" datasource in "' + file_name + '"\n'
                       '> started with a mixture of column names and\n'
                       '> numbers.  Please edit the file to either\n'
                       '> remove the column names or remove the numbers.'
                      )
            gen.WriteError(2246, err, log)
            gen.ErrorOnLine(line_number, line_text, log, False)
            return(None)

        elif found_blank:
            # We had names of column headers and some blanks.
            # Complain.
            if csv_file:
                err = ('> The .csv file "' + file_name + '" started\n'
                       '> with a mixture of column names and blank\n'
                       '> entries.  Please edit the file to fill in\n'
                       '> the blank entries, because if one column is\n'
                       '> given a name they all must be given a name.'
                      )
            else:
                err = ('> The "' + block_name + '" datasource in "' + file_name + '"\n'
                       '> started with a mixture of column names and\n'
                       '> blank entries.  Please edit the file to fill\n'
                       '> in the blank entries, because if one column\n'
                       '> is given a name they all must be given a name.'
                      )
            gen.WriteError(2247, err, log)
            gen.ErrorOnLine(line_number, line_text, log, False)
            return(None)
        tr_index += 1

        # If we get to here then all the entries were unique names of column
        # headers.  There were no numbers, blanks of names of constants.
        # Check that the names of constants are all single words.
        headers = []
        for index, word in enumerate(first_line):
            if ' ' in word:
                spaces = word.count(' ')
                if spaces == 1:
                    err_text1 = "a space.\n"
                    err_text2 = "it, "
                else:
                    err_text1 = "spaces.\n"
                    err_text2 = "them, "
                if csv_file:
                    err = ('> The .csv file "' + file_name + '"\n'
                           '> had a column name that contained ' + err_text1 +
                           '> Please edit the name "' + word + '" to remove\n'
                           '> ' + err_text2 + 'as column names cannot contain spaces.\n'
                          )
                else:
                    err = ('> The "' + block_name + '" datasource in "' + file_name + '"\n'
                           '> had a column name that contained ' + err_text1 +
                           '> Please edit the name "' + word + '" to remove\n'
                           '> ' + err_text2 + 'as column names cannot contain spaces.\n'
                          )
                err = err + ('> Remember that column names must be separated\n'
                             '> by commas (just like the numbers are).')
                gen.WriteError(2248, err, log)
                gen.ErrorOnLine(line_number, line_text, log, False)
                return(None)
            elif word in headers:
                firstenth = gen.Enth(headers.index(word) + 1)
                nextenth = gen.Enth(index + 1)
                if csv_file:
                    err = ('> The .csv file "' + file_name + '"\n'
                           '> had a column name that was a duplicate ("' + word + '"),\n'
                           '> it appears at the top of the ' + firstenth + ' and '
                           + nextenth + ' columns.'
                          )
                else:
                    err = ('> The "' + block_name + '" datasource in "' + file_name + '"\n'
                           '> had a column name that was a duplicate ("' + word + '"),\n'
                           '> it appears at the top of the ' + firstenth + ' and '
                           + nextenth + ' columns.'
                          )
                gen.WriteError(2249, err, log)
                gen.ErrorOnLine(line_number, line_text, log, False)
                return(None)
            headers.append(word)


    # Process the rest of the lines.  First make a list to hold the data and
    # get the count of words in the header, which is the maximum count of
    # columns allowed.
    header_size = len(headers)
    entries = []

    # for (line_number, line_data, line_text) in line_triples[tr_index:]:
    #     print(tr_index, line_data.split())
    #     if line_data.split() == ["end", "data"]:
    #         print("ended block1")
    #         break
    #     # if line_data.split() == ["end", "data"]: break

    for (line_number, line_data, line_text) in line_triples[tr_index:]:
    # while True:
    #     (line_number, line_data, line_text) == line_triples[tr_index]
        if line_data.split() == ["end", "data"]:
            # We've reached the end of the data block.
            break
        else:
            # Get the line number of the next line.  We use this to look
            # for breaks in lines of data, which we turn into something
            # that gnuplot treats as a line break.
            next_line_num = line_triples[tr_index + 1][0]

        words = [word.lower().lstrip().rstrip() for word in line_data.split(sep=',')]
        if len(words) > header_size:
            if csv_file:
                err = ('> The .csv file "' + file_name + '" had a\n'
                       '> line of entry that contained more entries\n'
                       '> (' + str(len(words)) + ' entries) than the first line ('
                         + str(len(headers)) + ' entries).\n'
                       '> The first line of a .csv file sets its maximum\n'
                       '> width.  Please edit the file to either shorten\n'
                       '> this line or lengthen the header (the ' + header_enth + ' line).'
                      )
            else:
                err = ('> The "' + block_name + '" datasource in "' + file_name + '"\n'
                       '> had a line of entry that contained more entries\n'
                       '> (' + str(len(words)) + ' entries) than the first line ('
                         + str(len(headers)) + ' entries).\n'
                       '> The first line of a data block sets its maximum\n'
                       '> width.  Please edit the file to either shorten\n'
                       '> this line or lengthen the header (the ' + header_enth + ' line).'
                      )
            gen.WriteError(2250, err, log)
            gen.ErrorOnLine(line_number, line_text, log, False)
            return(None)

        else:
            # Now process all the words in the line.
            line = []
            for index, word in enumerate(words):
                # Check for a blank entry first.
                if word == '':
                    # Put in a blank entry as a few spaces so that we pad out
                    # the numbers.
                    value = "     "
                else:
                    try:
                        value = float(word)
                    except ValueError:
                        # Check if it is the name of a constant.
                        try:
                            value = settings_dict['#' + word]
                        except KeyError:
                            # It's not a blank, a number or a constant.  Complain.
                            if csv_file:
                                err = ('> The .csv file "' + file_name + '" had an\n'
                                       '> entry that was not a blank entry or a number.\n'
                                       '> It was "' + word + '" instead.  Please edit\n'
                                       '> the file to replace it with something more\n'
                                       '> appropriate.\n'
                                      )
                            else:
                                err = ('> The "' + block_name + '" datasource in "' + file_name + '"\n'
                                       '> had an entry that was not a blank entry, a\n'
                                       '> number or the name of a constant.  It was\n'
                                       '> "' + word + '" instead.  Please edit the file\n'
                                       '> to replace it with something more appropriate.\n'
                                      )
                            err = err + ('> Keep in mind that numbers must be separated\n'
                                         '> by commas.')
                            gen.WriteError(2251, err, log)
                            gen.ErrorOnLine(line_number, line_text, log, False)
                            return(None)
                line.append(value)
            # Once we get to here we have processed all the words on the line.
            # Check if we need to pad it out with blanks so that the zip function
            # works.
            if len(line) < header_size:
                line.extend(["     "] * (header_size - len(line)))
        tr_index += 1
        entries.append(line)

        # Now check if there was one or more blank lines after this line.
        # If there was, put in two blank lines for (this causes gnuplot
        # to break the curve.
        if (next_line_num - line_number) > 1:
            entries.append(["     "] * header_size)
            entries.append(["     "] * header_size)

    # When we get to here we've read all the data into a list of sub-lists.
    # Each sub-list contains a row of data.  We want to turn this into a list
    # of sub-lists in which each sub-list is a column of data.  We may not be
    # able to use zip here.  We can if the data is rectangular but we can't
    # if it is not.
    value_list = list(zip(*entries))

    # Put the columns of data into a dictionary under their keys.
    data_dict = {}
    for index, header in enumerate(headers):
        data_dict.__setitem__(header.lower(), value_list[index])

    # Add the index of where the block started in line_triples.  We
    # can use this to point to the start of the block in error messages
    # that use the data in the blocks.
    data_dict.__setitem__("#tr_index", block_start)
    return(block_name_low, data_dict)


def ProcessPlotFiles(line_triples, tr_index, settings_dict, log):
    '''Take "begin files...end files" block in the input file
    and turn its values into a dictionary of keys and values.  Each
    entry is the name of an input file for Hobyah or ses plus either
     * nothing at all (a default nickname will be used)
     * a one-word nickname chosen by the user
     * instructions on how to make a nickname from the elements
       of the file name.

    The keys are the nicknames converted to lower case, the values
    they yield are the name of the input file (with its extension
    so we can distinguish what type of file it is) and the contents
    of the binary file loaded by the appropriate class instance.

        Parameters:
            line_triples [(int, str, str)]  List of lines in the file.  First
                                            entry is the line number in the file
                                            (starting at one, not zero).
                                            Second is the valid data on the line.
                                            Third is the entire line (including
                                            comments) also used in error messages.
            tr_index        int             Where to start reading the files block.
            settings_dict   {}              Dictionary of the run settings.
            log             handle          The handle of the logfile.

        Returns:
            files_dict      {}              Dictionary of plot files and their
                                            nicknames.

        Errors:
            Aborts with 2141 if the type of files block is invalid.
            Aborts with 2142 if a file name didn't end in ".hbn" or ".sbn".
            Aborts with 2143 if a file name had too few syllables in it.
            Aborts with 2144 if a nickname generated from syllables had
            spaces in it.
            Aborts with 2145 if a nickname was given but there was no
            file name.
            Aborts with 2146 if the nickname was a reserved word, such as
            "calc", "title", "xlabel" etc.
            Aborts with 2147 if there was a duplicate nickname.
            Aborts with 2148 if a file name consisted only of ".hbn"
            or ".sbn".
            Aborts with 2149 if the binary file associated with the file
            doesn't exist.
            Aborts with 2150 if the user doesn't have permission to read
            the binary file.
    '''
    debug1 = settings_dict["debug1"]
    file_name = settings_dict["file_name"]
    dir_name = settings_dict["dir_name"]
    reserved = settings_dict["reserved"]
    # Figure out what type of list of files this is.  There are three
    # variants:
    #   begin files numbered
    #       Each line has only the file name on it and the nicknames
    #       given to them are "file1", "file2", "file3", etc.
    #
    #   begin files nickname
    #       Each line has a one-word nickname followed by the file name
    #
    #   begin files 2syllables
    #       Each line has only the file name on it and the nicknames
    #       are generated from the second and final syllables in the
    #       file name.
    (line_number, line_data, line_text) = line_triples[tr_index]
    begin_text = line_data.lower().split()
    # The syntax checker has already checked that it has exactly 3 entries.
    block_type = begin_text[2]

    if block_type not in ("numbered", "nicknames", "2syllables"):
        err = ('> The "begin files" block in "' + file_name + '"\n'
               '> has been given the type "' + block_type + '".\n'
               '> Only "numbered", "nicknames" or "2syllables" are\n'
               '> permitted here.'
              )
        gen.WriteError(2141, err, log)
        gen.ErrorOnLine(line_number, line_text, log, False)
        return(None)

    # Usually we would make a call to ProcessBlock, but it isn't
    # suitable for the list of plot files.  We do it here instead.
    files_dict = {}

    if block_type == "numbered":
        # Start the counter.
        file_num = 1

    # Iterate over the lines after the one containing "begin files"
    # until we reach the line with "end files" in it.
    while tr_index < len(line_triples):
        tr_index += 1
        (line_number, line_data, line_text) = line_triples[tr_index]
        if line_data.split() == ["end", "files"]:
            # We are at the end of the block
            break

        # Now check if there is ".hbn" or ".sbn" at the end.
        if line_data[-4:].lower() not in (".hbn", ".sbn"):
            err = ('> There is an invalid entry in a "begin files"\n'
                   '> block in "' + file_name + '".\n'
                   '> The file "' + line_data + '" in the\n'
                   '> "files" block has an incorrect extension or\n'
                   '> no extension.  Filenames must end in ".hbn"\n'
                   '> or ".sbn" (signifying a Hobyah binary file or\n'
                   '> an SESconv binary file respectively).'
                  )
            gen.WriteError(2142, err, log)
            gen.ErrorOnLine(line_number, line_text, log, False)
            return(None)
        else:
            file_data = line_data

        # If we get to here, we probably have a valid file name.  We
        # might have an invalid one with a path (e.g. "/Users/me/.hbn")
        # so we'll check for it later.
        if block_type == "numbered":
            nickname = "file" + str(file_num)
            file_num += 1
        elif block_type == "2syllables":
            # Complain if the name has too few syllables in it.
            # Syllables are delineated by dashes.
            syllables = file_data[:-4].split(sep = "-")
            if len(syllables) < 3:
                err = ('> The "files" block in "' + file_name + '"\n'
                       '> has been told to generate nicknames from\n'
                       '> two syllables in the file name but there\n'
                       '> are not enough syllables available in the\n'
                       '> file named "' + line_data + '".\n'
                       '> Please either give the file name three\n'
                       '> or more syllables separated by dashes (a\n'
                       '> name like "WGT-012-fire-12080ab.txt" has\n'
                       '> four syllables ("WGT", "012", "fire" and\n'
                       '> "12080ab").'
                      )
                gen.WriteError(2143, err, log)
                gen.ErrorOnLine(line_number, line_text, log, False)
                return(None)
            else:
                nickname = syllables[1] + '-' + syllables[-1]

            # Disallow any nicknames names that have spaces in them, as
            # this mucks up the two-syllable key.
            if " " in nickname:
                err = ('> The "files" block in "' + file_name + '"\n'
                       '> has been told to generate one-word nicknames\n'
                       '> from two syllables in the file name  The\n'
                       '> syllables in the nickname "' + nickname + '"\n'
                       '> (generated from "' + line_data + '"\n'
                       '> has spaces in it, which is not allowed.\n'
                       '> Please rename the file or use a different\n'
                       '> naming scheme.'
                      )
                gen.WriteError(2144, err, log)
                gen.ErrorOnLine(line_number, line_text, log, False)
                return(None)
        else:
            # There should be a custom key as the first word on the line.
            words = file_data.split(maxsplit = 1)

            # Complain if there is only one word on the line.
            if len(words) == 1:
                err = ('> The "files" block in "' + file_name + '"\n'
                       '> has been told to read a nickname and a\n'
                       '> file name, but one of the entries had\n'
                       '> nothing but the nickname, "' + file_data + '".\n'
                       '> Please add a file name or choose another\n'
                       '> way of generating nicknames.'
                      )
                gen.WriteError(2145, err, log)
                gen.ErrorOnLine(line_number, line_text, log, False)
                return(None)
            else:
                # It's safe to overwrite the first word in file_data, we
                # don't need all of it any more.
                (nickname, file_data) = words
            if nickname in reserved:
                err = ('> The "files" block in "' + file_name + '"\n'
                       '> has been told to read a nickname and a\n'
                       '> file name.  One of the nicknames was\n'
                       '> "' + nickname + '", which is not allowed because\n'
                       '> it is a word reserved for other uses.\n'
                       '> Please choose a different word.'
                      )
                gen.WriteError(2146, err, log)
                gen.ErrorOnLine(line_number, line_text, log, False)
                return(None)
        # Check if we have any duplicates.
        if nickname in files_dict:
            err = ('> The "files" block in "' + file_name + '"\n'
                   '> has duplicate nicknames: "' + nickname + '"\n'
                   '> appears twice (note that nicknames are not\n'
                   '> case sensitive).  Please rename one of them.'
                  )
            gen.WriteError(2147, err, log)
            gen.ErrorOnLine(line_number, line_text, log, False)
            return(None)

        # We call the generic routine GetFileData because it is
        # set up to use the current working directory if there is
        # no path.  We give no default extension.
        (bin_file_name, pf_dir_name, file_stem,
               file_ext) = gen.GetFileData(file_data, "", debug1, dir_name)
        if bin_file_name in (".hbn", ".sbn"):
            err = ('> Came across a file name that consisted of\n'
                   '> nothing but the extension "' + bin_file_name
                     + '" in the\n'
                   '> "files" block of "' + file_name + '".\n'
                   '> Please give it a name.'
                  )
            gen.WriteError(2148, err, log)
            gen.ErrorOnLine(line_number, line_text, log, False)
            return(None)
        else:
            full_name = pf_dir_name + bin_file_name

        try:
            bin_handle = open(full_name, "rb")
        except FileNotFoundError:
            # The binary file doesn't exist.  Complain and return.
            if bin_file_name[-4:] == ".hbn":
                source_name = bin_file_name[:-4] + '.txt'
                process_text = '"\n'
            else:
                source_name = bin_file_name[:-4] + '.ses'
                process_text = '",\n> process it with SESconv.py to '  \
                               'create a binary\n'
            err = ('> One of the binary output files in the "files"\n'
                   '> block of "' + file_name + '"' + " doesn't exist\n"
                   '> in the same folder as the input file.\n'
                   '> The missing file is "' + bin_file_name + '".\n'
                   '> Please either run "' + source_name
                      + process_text +
                   '> and copy the binary to the correct folder.\n'
                   '> Alternatively, remove it from the list of files.\n'
                  )
            gen.WriteError(2149, err, log)
            gen.ErrorOnLine(line_number, line_text, log, False)
            return(None)
        except PermissionError:
            # The binary file exists but we don't have permission
            # to read it.  Complain and return.
            err = ('> One of the binary output files in the "files"\n'
                   '> block of "' + file_name + '"' " exists\n"
                   "> but you don't have permission to read it.\n"
                   '> The locked file is "' + bin_file_name + '".\n'
                   '> Please either change its permissions or remove\n'
                   '> it from the list of files and remove any\n'
                   '> references to it in your plots.'
                  )
            gen.WriteError(2150, err, log)
            gen.ErrorOnLine(line_number, line_text, log, False)
            return(None)
        else:
            # The binary file exists and we have permission to read
            # it.  We try to read its contents, assign the path and
            # name of the binary file to the nickname and close the
            # file.
            if bin_file_name[-4:] == ".hbn":
                contents = clHobyah.Hobyahdata(pf_dir_name, bin_file_name,
                                               log, line_number, line_text)
            else:
                # It must have the extension ".sbn" (if it didn't then
                # error 2142 would have been raised).
                contents = clSES.SESdata(pf_dir_name, bin_file_name,
                                         log, line_number, line_text)
            if contents.success is None:
                # It was not a valid Hobyah file or valid SES binary
                # (most likely the problem is that it was an old binary
                # file version).  The relevant module has already
                # issued an error message, so return.
                return(None)
            # Put the binary file path/name and the contents into
            # files_dict.
            files_dict.__setitem__(nickname, (full_name, contents))
            bin_handle.close()

    # Write the files and their nicknames to the log file for the
    # record.
    gen.LogBlock(files_dict, "files", debug1, log)
    return(files_dict)


def RaiseTooFew(req_words, file_name, optionals_dict, line_number, line_text, log):
    '''Raise faults 2102 or 2103, too few words on the line.  This routine
     exists because it is called by PROC ProcessBlock more than once, and
     because it may need to show a list of the words consumed by optional
     entries.

        Parameters:
            req_words       int             The minimum count of words
                                            after this keyword.
            file_name       str             The file name without the
                                            file path.
            optionals_dict  {}              The optional entries on the line.
            line_number     int             The line number in the input file
            line_text       str             All the contents of the line in
                                            the input file.
            log             handle          The handle of the logfile.

        Returns:
            None

        Errors:
            Aborts with 2102 if a line in the settings block had too few
            words on it and there were no optional entries.
            Aborts with 2103 if a line in the settings block had too few
            words on it and there were optional entries.
    '''
    # We have too few mandatory entries on this line.
    err = ('> Came across a faulty setting in "' + file_name + '".\n'
           '> The setting has too few entries in it\n'
           '> (expected ' + str(req_words+1) + ' words).'
          )
    if optionals_dict == {}:
        gen.WriteError(2102, err, log)
        gen.ErrorOnLine(line_number, line_text, log, False)
    else:
        # One of the optional entries may have consumed a keyword,
        # add a list of the optional entries and use a different
        # number
        err = err + gen.ListTheKeys(optionals_dict)
        gen.WriteError(2103, err, log)
        gen.ErrorOnLine(line_number, line_text, log, False)
    return(None)


def RaiseDudSpec(dud_entry, block_name, file_name, log):
    '''Raise fault 1202, dud hardcoded specification.  This routine exists
    because it is called by PROC ProcessBlock more than once and by
    PROC CheckRangeAndSI once.

        Parameters:
            dud_entry       str             The specification.
            block_name      str             Name of the type of block
                                            that contains the dud spec.
            file_name       str             The file name without the
                                            file path.
            log             handle          The handle of the logfile.

        Returns:
            None

        Errors:
            Aborts with 1202, one of the hardcoded entry
            specifications was wrong.
    '''
    err = ('> Found an invalid definition in the source\n'
           '> code.  It was\n'
           '>   "' + str(dud_entry) + '".\n'
           '> instead of a tuple of valid words or a string\n'
           '> starting with "int", "float" or "#name".\n'
           '> The block being processed at the time was\n'
           '> "' + block_name + '".\n'
           '>'
           )
    gen.WriteError(1202, err, log)
    gen.OopsIDidItAgain(log, file_name)
    return(None)


def ProcessBlock(line_triples, tr_index, settings_dict,
                 block_name, block_dict, block_settings, log):
    '''Read a block of a given name and process its entries
    according to the dictionary of settings, faulting if there
    is a mismatch (e.g. a float where an integer should be,
    too few entries etc.)
    Turn its values into a dictionary of keys and a list of values
    and put them to the dictionary.

    All the keys are converted to lower case, as are all values except
    those that are QA strings.  Return None in the case of an error
    and return a tuple of the settings and the index of the line
    in the input file after "end <block_name>".

    This routine is used for blocks where entries can be duplicates,
    like jet fans at different chainages.  The dictionary keys are
    created from the object type (area change, resistance, adit)
    and its location (a check is made to prevent two things being
    at the same location).
    The routine also handles the definition of multiple entries from
    one line of input, for doing things like defining multiple adits
    at fixed spacings (most road tunnels in Australia have the cross-
    passages at 120 m intervals).

        Parameters:
            line_triples [(int, str, str)]  List of lines in the file.  First
                                            entry is the line number in the file
                                            (starting at one, not zero).
                                            Second is the valid data on the line.
                                            Third is the entire line (including
                                            comments) also used in error messages.
            tr_index        int             The index in line_triples of the
                                            line containing "begin <block_name>".
            settings_dict   {}              The entries in the settings block.
            block_name      str             The word we expect after "begin"
            block_dict      dict            A dictionary of entries which this
                                            routine updates or adds to.
            block_settings  dict            A list with four entries (two lists,
                                            two dictionaries) of the behaviour
                                            expected.
            log             handle          The handle of the logfile.

        Returns:
            dict_key        str             If a block is named, this is the
                                            name of it.
            block_dict      {}              A dictionary of the entries in the
                                            block.  The default values are used
                                            if no entry was read in the block.

        Errors:
            Aborts with 2101 the first word was not a valid keyword.
            Aborts with 2102 if a line in the settings block had too few
            words on it and there were no optional entries (this error
            is raised in function RaiseTooFew).
            Aborts with 2103 if a line in the settings block had too few
            words on it and there were optional entries (this error is
            raised in function RaiseTooFew).
            Aborts with 2104 if an entry was not one of a range of valid
            strings.
            Aborts with 2105 if a line in the settings block had too many
            words on it.
            Aborts with 2106 if an entry that must appear only once is a
            duplicate.
            Aborts with 2107 if an entry had an optional entry but no
            optional entries were allowed for this entry.
            Aborts with 2108 if an entry had an invalid optional keyword.
            Aborts with 2109 if an entry had a valid optional keyword but
            the value assigned to it was not a valid word.
            Aborts with 2110 if a unique required entity (e.g. the
            definition of where the exit portal is in a tunnel) was absent.
            Aborts with 2111 if one of a list of mutually exclusive (but
            required) entities (e.g. the definition of how to calculate
            train nose and tail loss) was absent.
            Aborts with 2112 if more than one entry in a list of mutually
            exclusive entities appeared in a block.
            Aborts with 2113 if the name of a tunnel or route contained
            @, ' or ".
    '''
    debug1 = settings_dict["debug1"]
    file_name = settings_dict["file_name"]

    # Check if we need to convert to SI units.
    if block_name != "settings":
        toSI = settings_dict["units"] == "us"
    else:
        # We are reading the settings block.  It is possible that an
        # input file could have a pressure set in IWG or density set
        # in lb/ft^3 before setting the units to US customary units.
        # So we don't convert to SI here when reading the settings,
        # we convert in PROC ProcessSettings.
        toSI = False

    # Create a list that we use to track duplicates and another used
    # to track entities that are too close together.
    used = []
    chainages = []
    # Break out the settings into its components.
    #
    # Valid_settings is a dictionary that defines what keywords are
    # allowed and what arguments each keyword can take.
    #
    # requireds is a list that defines what keywords must be present.
    #
    # optionals is a dictionary that defines what optional arguments
    # each keyword can take.
    #
    # duplicates is a list of which keywords can appear more than
    # once.  For these, the dictionary key is not the keyword but the
    # keyword and the chainage.
    (valid_settings, requireds, optionals, duplicates) = block_settings

    # First line of the block, the begin line.  Get the name of
    # the entity (tunnel, route) which will be used as the key
    # for the entity.  We don't need to check if the name exists,
    # we already checked in the syntax checker.  We do put it in
    # a try block because some blocks that are processed here do
    # not have a name (e.g. begin plots).
    (line_number, line_data, line_text) = line_triples[tr_index]
    all_words = line_data.lower().split()
    try:
        entity_name = all_words[2]
        UC_name = line_data.split()[2]
    except IndexError:
        entity_name = 'unnamed'
        UC_name = entity_name
    # A quick sanity check that gets triggered all the time during
    # development work.
    if all_words[:2] != ["begin", block_name]:
        print("> Oops, fouled up in ProcessBlock with:\n> ", line_text,
              '\n> when expecting "' + block_name + '".')
        # raise()
        gen.OopsIDidItAgain(log, file_name)
        return(None)

    # Check that the names of tunnels and routes do not have any forbidden
    # characters in them.  Note that we ignore leading dashes in tunnel names,
    # as there is a more informative error message for them in ProcessTunnel.
    if block_name in ("tunnel", "route"):
        if "'" in entity_name:
            complaint = " contains a single\n> quote ('), which is"
            dud = True
        elif '"' in entity_name:
            complaint = ' contains a double \n> quote ("), which is'
            dud = True
        elif "@" in entity_name:
            complaint = " contains an @,\n> which is"
            dud = True
        else:
            dud = False
        if dud:
            err = ('> In the file named "' + file_name + '"\n'
                   '> the name of ' + block_name + ' "' + entity_name
                     + '" is\n'
                   '> not valid because it' + complaint
                     + ' a reserved symbol.\n'
                   '> Please edit the file to remove it.'
                  )
            gen.WriteError(2113, err, log)
            gen.ErrorOnLine(line_number, line_text, log, False)
            return(None)
    # Store the index of the line that starts the block, in case we
    # need it for error messages about absent entries.
    tr_store = tr_index

    # Iterate over the contents of the file, from the line after the
    # line that contains "begin <block_name>" to the line that contains
    # "end <block_name>" (we know it exists because the file has passed
    # the begin...end syntax check).
    if debug1:
        print("List in ProcessBlock:\n", line_triples)
    while tr_index < len(line_triples):
        tr_index += 1
        (line_number, line_data, line_text) = line_triples[tr_index]
        # Get the optional entries on the line into a dictionary.
        result = gen.GetOptionals(line_number, line_data, line_text,
                 file_name, debug1, log)
        if result is None:
            return(None)
        else:
            (line_data, optionals_dict) = result
        # Now split the remainder of the line up.  We know we still have
        # data on the line.
        all_words = line_data.lower().split()
        if all_words == ["end", block_name]:
            # We are at the end of the block
            break
        # If we got to here all is OK.  Split the contents of the line into
        # the key and a list of values.
        keyword = all_words[0]
        words = all_words[1:]

        # Now check if the first word is a valid key and fault if it is not.
        process_this = True
        if keyword not in valid_settings:
            # We have a random word.  There are two valid random word
            # definitions in the valid settings, "#skip" and "#name".
            # "#skip" means we don't want to process unrecognised lines
            # because we are skipping over everything except names in
            # valid settings.  "#name" means that we allow any word and
            # we do process the line.
            if "#skip" in valid_settings:
                # Set a Boolean that we will use to decide whether to
                # process the contents of this line.  We use it as the
                # test in an infinite while loop.
                process_this = False
            elif "#name" in valid_settings:
                # We do allow random names.  Set an entry for the random
                # word in valid_settings and optionals (if necessary).
                valid_settings.__setitem__(keyword, valid_settings["#name"])
                if "#name" in optionals:
                    optionals.__setitem__(keyword, optionals["#name"])
            else:
                # Get the keys in the settings dictionary into a list so
                # that we can have them pretty-printed.
                err = ('> Came across an unrecognised keyword in "' + file_name + '".\n'
                       '> The keyword is "' + keyword + '".  Valid keywords\n'
                       '> are as follows:\n'
                       + gen.FormatOnLines(valid_settings.keys())
                      )
                gen.WriteError(2101, err, log)
                gen.ErrorOnLine(line_number, line_text, log, False)
                return(None)

        if process_this:
            # Figure out the minimum number of words we expect, accounting
            # for descriptions (which are sentences).
            # We may increase this number when we encounter certain keywords
            # that require a number, like the keyword "portal", which needs
            # a number (wind pressure) after it.
            req_words = len(valid_settings[keyword])


            # Now start checking the keys and values.
            if debug1 and toSI:
                # Input values are in US customary units.  The best way I've
                # found of ensuring that I don't foul up the conversions is to
                # write a message to the logfile giving the original contents
                # of a line in US customary units if the command-line argument
                # "-debug1" is set), state the conversion factors and units
                # for each number in the logfile (in USc.ConvertToSI),
                # then write the dictionary in SI units to the logfile.
                log.write('Read a line in US units: "' + line_data + '"\n')

            # Now run through the entries checking if the values are
            # consistent with what we expect (e.g. where we expect an
            # integer we don't have a float).  We have to do this in
            # four steps: integers, floats, QA strings and one of a
            # list of allowable words (strings).
            # First we check numbers (in CheckRangeAndSI).  That checks
            # the number type, range, and if necessary converts it to SI.
            # If we are expecting a QA string we consume the rest of the
            # line because QA strings can have spaces in them.
            # Lastly, we check if the value was in the list of valid one-
            # word strings for this entry.
            #
            # But first we create a list to hold the entries.  If all goes
            # well the first set of entries will be the mandatory entries
            # defined in valid_settings for such an entry.  Its penultimate
            # entry will be the dictionary of optional entries on the line,
            # with the keys set to lower case.  The last entry is the current
            # value of tr_index, so that we can find the source line if we
            # need it for error messages.
            entries = []
            # Create a counter to point to the current word in words.  We can't
            # use enumerate(words) because we may need to increment it directly.
            w_index = 0
            # Create a second counter to where we are in the expected settings.
            s_index = 0
            while True:
                # Get the type of entry we expect for this: integer, float,
                # word or QA string.
                if s_index == len(valid_settings[keyword]):
                    # We have reached the end of the definition.
                    break
                else:
                    expected = valid_settings[keyword][s_index]

                # Now see if we have an entry for it on the line.
                try:
                    word = words[w_index]
                except IndexError:
                    if expected == "QAstr":
                        # We are reading a line of entry that is allowed to
                        # have an optional description at the end of the line
                        # but does not have one.  Set the optional description
                        # to a blank string and stop reading entries.
                        entries.append('')
                        break
                    else:
                        # There were too few entries on the line.  Complain
                        # with error 2102 or 2103.
                        RaiseTooFew(req_words, file_name, optionals_dict,
                                        line_number, line_text, log)
                        return(None)

                # If we get to here we have a valid word and a valid
                # description.
                place = gen.Enth(w_index + 1)

                # Now check for numbers, which may be integer or float.
                if (expected[:3] == "int" or
                    expected[:5] == "float" or
                    expected[:6] == "*float"):
                    # Build a couple of lines of error text for CheckRangeAndSI.
                    # We do this here because we're talking about keywords and
                    # CheckRangeAndSI is also called by the routine that
                    # processes optional arguments, which sends it a different
                    # pair of error lines.
                    err_lines = ('> The ' + place + ' entry for keyword "'
                                + keyword + '" was "' + word + '"')

                    result = CheckRangeAndSI(word, expected, toSI, err_lines,
                                             line_number, line_text,
                                             settings_dict, log)
                    if result is None:
                        return(None)
                    else:
                        entries.append(result)
                elif expected == "#name":
                    # This word can be any word.  It is the name of something
                    # else, such as the name of a sectype in a tunnel.  We
                    # don't check it here, we check it later in whichever
                    # routine called ProcessBlock.
                    entries.append(word.strip())
                elif expected == "QAstr":
                    # This setting can be anything it wants to be, as it is a
                    # QA string - a project number, project description or
                    # the description of a tunnel.
                    # It is always the last entry in a line of input and
                    # consumes all the rest of the line.
                    words = line_data.split(maxsplit = req_words)[1:]
                    if len(words) > w_index:
                        # We do have some comments after the required entries,
                        # which are the last in the list.
                        entries.append(words[-1])
                        # Now break, because we've finished this line.  This
                        # is the second point at which we could break out of the
                        # line.
                        break
                elif type(expected) is tuple:
                    # We want one of a list of words (e.g. ["portal", "node"]).
                    # There is a complication, though.  Some words stand alone
                    # ("frictiontype" does).  Others (such as "portal") require
                    # a number after them (in the case of "portal" the number is
                    # a gauge wind pressure).  A third type requires a name
                    # after them ("node" does, to identify the name of the node).
                    # This block checks those words and if necessary, processes
                    # the number that follows and updates the counters named
                    # req_words and w_index.
                    found = False
                    for entry in expected:
                        exp_entries = entry.split(maxsplit = 1)
                        if exp_entries[0].lower() == word:
                            found = True
                            break
                    if not found:
                        if len(expected) == 0:
                            # The user probably put in the second step of a
                            # two-step process without doing the first step.
                            # Give an informative error message, this is an
                            # obscure error.
                            text1 = ('> There were no valid entries for this, meaning\n'
                                     "> that you probably used an entity that is\n"
                                     '> defined in two steps (traffic, jet fans,\n'
                                     '> jet fans etc.) and only did the 2nd step.')
                        else:
                            text1 = '> Valid entries for this are: \n'  \
                                      + gen.FormatOnLines(expected)

                        err = ('> Came across an unrecognised entry in "' + file_name + '".\n'
                               '> The ' +gen.Enth(w_index + 1) + ' entry for keyword "'
                                  + keyword + '" was "' + word + '".\n'
                                  + text1
                              )
                        gen.WriteError(2104, err, log)
                        gen.ErrorOnLine(line_number, line_text, log, False)
                        return(None)
                    else:
                        entries.append(word)


                    if len(exp_entries) > 1:
                        # We are expecting this keyword to consume another
                        # word, but we don't know what it is yet (#name, int
                        # or float).  Update  the list of words we have and
                        # the counters.
                        w_index += 1
                        words = line_data.split(maxsplit = req_words)[1:]
                        place = gen.Enth(w_index + 1)
                        try:
                            word = words[w_index]
                        except IndexError:
                            # There were too few entries on the line.  Complain.
                            RaiseTooFew(req_words, file_name, optionals_dict,
                                            line_number, line_text, log)
                            return(None)


                        if exp_entries[1] == "#name":
                            # We have a valid word that needs a name after
                            # it, and we have a valid entry.  Add it and
                            # carry on with the next word.
                            entries.append(word.strip())
                        elif (exp_entries[1][:3] == "int" or
                              exp_entries[1][:5] == "float" or
                              exp_entries[1][:6] == "*float"):
                            # We have a valid word that needs a number after
                            # it.  The remainder of the text on the line ought
                            # to be a number format text in a form that suits
                            # CheckRangeAndSI.  We will check its correctness
                            # there.
                            # We're expecting a number and sub_expected is a
                            # number specifier.
                            sub_expected = exp_entries[1]
                            err_lines = ('> The ' + place + ' entry for keyword "'
                                        + keyword + '" was "' + word + '"')

                            result = CheckRangeAndSI(word, sub_expected, toSI,
                                                     err_lines,
                                                     line_number, line_text,
                                                     settings_dict, log)
                            if result is None:
                                return(None)
                            else:
                                entries.append(result)
                        else:
                            # The sub-entry specifier didn't start with "#name",
                            # "int" or "float".  Complain.
                            RaiseDudSpec(expected, block_name + "2", file_name, log)
                            return(None)
                else:
                    # We mis-spelled a specification, it wasn't "#name",
                    # "int", "float" or a list of valid words.  Complain.
                    RaiseDudSpec(expected, block_name + "1", file_name, log)
                    return(None)

                # Now increment w_index and s_index and go round the loop again.
                w_index += 1
                s_index += 1


            # Now check for too many required entries.  The counter 'req_words'
            # may have been updated while reading the entries: some types of
            # entry have a word and a value (e.g. portal) while others don't
            # (e.g. node).
            if len(words) > req_words:
                err = ('> Came across a faulty setting in "' + file_name + '".\n'
                       '> The setting has too many entries in it\n'
                       '> (expected ' + str(req_words + 1) + ' words).'
                      )
                gen.WriteError(2105, err, log)
                gen.ErrorOnLine(line_number, line_text, log, False)
                return(None)


            # # Check for the absence of optional descriptions and add an
            # # empty string if it is absent.
            # if (valid_settings[keyword][-1] == "QAstr" and
            #     len(entries) < len(valid_settings[keyword]) ):
            #         entries.append('')

            # Get the keyword and check if it is duplicable.  If it isn't,
            # use the first keyword as the dictionary key.
            # If it is, get the first word (which must always be a chainage,
            # I think) and combine them to form the dictionary key.
            if keyword not in duplicates:
                dict_key = keyword
                # Check for duplicate entries and complain if we find one.
                if dict_key in used:
                    err = ('> Came across a duplicate keyword in "' + file_name + '".\n'
                           '> The keyword "' + keyword + '" has been used already in\n'
                           '> this "' + block_name + '" block and you must have\n'
                           '> only one of them per block.'
                          )
                    gen.WriteError(2106, err, log)
                    gen.ErrorOnLine(line_number, line_text, log, False)
                    return(None)
            else:
                # Construct a dictionary key with the keyword, '#' and the
                # tr_index of this line.  The '#' symbol is so that it can't
                # be spoofed and the addition of tr_index makes it unique.
                dict_key = keyword + '#' + str(tr_index)

            # Add the keyword to the list of keys already used, for the error
            # message above.
            used.append(keyword)

            # Add the chainage to the list, so we can check for duplicates later.
            chainages.append(entries[0])

            # Now that we know what kind of line of input this is, check the
            # form of the optional entries.
            # Define the optional entries allowed in each keyword
            if keyword not in optionals:
                count_keys = len(optionals_dict)
                if  count_keys != 0:
                    # There is an optional entry set in a keyword
                    # where none are allowed.
                    err = ('> Came across an invalid optional entry in\n'
                           '> "' + file_name + '".\n'
                           '> The keyword "' + keyword + '" has no valid\n'
                           "> optional entries, but you've set "
                          )
                    if count_keys == 1:
                        err = err + 'one.\n> Please remove it.'
                    else:
                        err = err + str(count_keys) + '.\n> Please remove them.'
                    gen.WriteError(2107, err, log)
                    gen.ErrorOnLine(line_number, line_text, log, False)
                    return(None)
            else:
                # This keyword can have optional entries.  Get the dictionary
                # of optional entries permitted for this keyword, e.g.
                # {"zeta_bf": ("float 0+ null a k-factor",),
                #  "zeta_fb": ("float 0+ null a k-factor",),
                #  "height": ("float any dist1 a height",), }
                allowables = optionals[keyword]
                # Check that the keys of all optional entries are valid.  Sometimes
                # we want to ignore all optional entries (so we can process them
                # later elsewhere, in which case "#any" will be in the list of
                # allowable entries.
                for opt_key in optionals_dict:
                    if opt_key not in allowables:
                        # The optional entry is wrong (mis-spelled or not
                        # valid for this line of entry).

                        err = ('> Came across an invalid optional entry in\n'
                               '> "' + file_name + '".\n'
                               '> The keyword "' + keyword + '" cannot use the\n'
                               '> optional entry "' + opt_key + '", ')
                        # Now finish the message depending on how many keywords
                        # are allowed.
                        allowed_keys = list(allowables.keys())
                        if len(allowed_keys) == 1:
                            err = err + ('there is one\n'
                                         '> valid optional entry for this keyword,\n'
                                         '> "' + allowed_keys[0] + '".'
                                        )
                        else:
                            err = err + ('the only valid\n'
                                         '> optional entries for this keyword are:\n'
                                         + gen.FormatOnLines(allowables)
                                        )
                        gen.WriteError(2108, err, log)
                        gen.ErrorOnLine(line_number, line_text, log, False)
                        return(None)

                    # Now check the value in the option and if necessary convert
                    # to SI.
                    expected = allowables[opt_key]
                    opt_word = optionals_dict[opt_key]
                    if expected[:3] == "int" or expected[:5] == "float":
                        # Build a couple of lines of error text for CheckRangeAndSI.
                        # We do this here because we're talking about optional keys
                        # and values. CheckRangeAndSI is also called when we
                        # process required arguments, which sends it a different
                        # pair of error lines.
                        err_lines = ('> The optional entry "' + opt_key
                                        + '" was assigned\n'
                                     '> the value "' + opt_word + '"')
                        result = CheckRangeAndSI(opt_word, expected, toSI, err_lines,
                                                 line_number, line_text,
                                                 settings_dict, log)
                        if result is None:
                            return(None)
                        elif toSI:
                            # We converted the number from US units to SI, reset
                            # the value.
                            optionals_dict.__setitem__(opt_key, result)
                    elif type(expected) is tuple and opt_word.lower() not in expected:
                        # We are expecting one of a list of allowable words.
                        err = ('> Came across an invalid optional entry in\n'
                               '> "' + file_name + '".\n'
                               '> The optional entry "' + opt_key + '" was assigned\n'
                               '> the value "' + opt_word + '".  '

                               )
                        # Now finish the message depending on how many values
                        # are permitted.  We don't actually have options with only
                        # one entry at the moment but I'm sure one will appear.
                        if len(expected) == 1:
                            err = err + ('There is one\n'
                                         '> valid optional entry for this keyword,\n'
                                         '> "' + expected[0] + '".'
                                        )
                        else:
                            err = err + ('The only valid\n'
                                         '> optional entries for this keyword are:\n'
                                         + gen.FormatOnLines(expected)
                                        )
                        gen.WriteError(2109, err, log)
                        gen.ErrorOnLine(line_number, line_text, log, False)
                        return(None)

            # All is well.  Add the dictionary of optional keywords to the
            # entries, convert it to a tuple and set it in the dictionary.
            entries.extend([optionals_dict, tr_index])
            block_dict.__setitem__(dict_key, tuple(entries))
            if block_name in ("sectypes",):
                # Write the block_dict to the log file (this is for blocks with
                # many independent entries).  We spoof a dictionary holding
                # only the last entry.
                gen.LogBlock({dict_key: entries}, block_name[:-1], debug1, log)
    # Now check that all required entries are present.
    # Make a list of keywords in block_dict that are in tuples containing
    # alternative options.
    found = []
    for poss_key in requireds:
        # Check for all the words we must have.  First we check if
        # the key being tested is a tuple of mutually exclusive words,
        # like a tuple of the four keywords that set train nose and tail
        # loss factors in different ways.  If the entry isn't a tuple
        # we assume it is a string, the name of a required keyword.
        if type(poss_key) is tuple:
            # 'poss_key' is not a key, it is a tuple of mutually exclusive
            # keys.
            found = []
            for candidate in poss_key:
                # Get a list of all the sub-keys that we have and check
                # them.
                for key in block_dict:
                    if key[:len(candidate)] == candidate:
                        found.append(key)
            if found == []:
                # All of the keys were absent.  Complain.
                # Get the line number and line text of the start of the
                # block.
                (start_number, discard, start_text) = line_triples[tr_store]

                if len(poss_key) == 1:
                    # This was a tuple with one entry (which is a bit odd
                    # but is not disallowed).  We raise this error in a
                    # subroutine because we need to be able to raise it
                    # from another place in this block.
                    Raise2110(block_name, file_name, poss_key[0],
                              start_number, start_text, log)
                    return(None)
                elif len(poss_key) == 2:
                    text1 = " two "
                elif len(poss_key) == 3:
                    text1 = " three "
                elif len(poss_key) == 4:
                    text1 = " four "
                else:
                    text1 = " many "
                err = ('> Came across a ' + block_name + ' definition\n'
                       '> in "' + file_name + '"\n'
                       '> that lacked one of' + text1 + 'required (but\n'
                       '> mutually exclusive) entries:\n'
                       + gen.FormatOnLines(poss_key, lastword = "or") + '\n'
                       '> Please add a line to define one of them.\n'
                       '> The faulty block of input started at the '
                         + gen.Enth(start_number) + ' line:\n'
                        '>   ' + start_text
                      )
                gen.WriteError(2111, err, log)
                # gen.ErrorOnLine(start_number, start_text, log, False)
                return(None)
            elif len(found) > 1:
                # There was more than one entry (and we want only one).
                # Figure out how many lines we need to print in the error
                # message (we can print up to four).
                tr1_index = block_dict[found[0]][-1]
                tr2_index = block_dict[found[1]][-1]
                if len(found) > 2:
                    tr3_index = block_dict[found[2]][-1]
                else:
                    # Spoof the third line with the second.
                    tr3_index = tr2_index
                if len(found) > 3:
                    tr4_index = block_dict[found[3]][-1]
                else:
                    # Spoof the fourth line with the second.
                    tr4_index = tr2_index
                line1_num, discard, line1_text = line_triples[tr1_index]
                line2_num, discard, line2_text = line_triples[tr2_index]
                line3_num, discard, line3_text = line_triples[tr3_index]
                line4_num, discard, line4_text = line_triples[tr4_index]
                err = ('> Came across a ' + block_name + ' definition\n'
                       '> in "' + file_name + '" that\n'
                       '> had too many mutually exclusive entries.\n'
                       '> The following entries are mutually exclusive:\n'
                       + gen.FormatOnLines(poss_key) + '\n'
                       '> Please remove all but one of them.'
                      )
                gen.WriteError(2112, err, log)
                gen.ErrorOnManyLines(line1_num, line1_text,
                                     line2_num, line2_text,
                                     line3_num, line3_text,
                                     line4_num, line4_text,
                                      log, False)
                return(None)

        else:
            # Get a list of all the sub-keys that we have and check
            # them.
            found = []
            for key in block_dict:
                if key[:len(poss_key)] == poss_key:
                    found.append(key)
            if found == []:
                # This key is required and it is not present.
                (start_number, discard, start_text) = line_triples[tr_store]
                Raise2110(block_name, file_name, poss_key,
                          start_number, start_text, log)
                return(None)
    # Store the name of the block and the value of tr_index it starts
    # at.  This is handy for later error messages.
    block_dict.__setitem__("block_index", (entity_name, UC_name, tr_store))


    if block_name in ("tunnel",):
        # Write the block_dict to the log file (this is for blocks with
        # entries that are related, so they all appear in the log file
        # together).
        gen.LogBlock(block_dict, block_name + " " + entity_name, debug1, log)
    return(entity_name, block_dict)


def Raise2110(block_name, file_name, key, line_number, line_text, log):
    '''Raise error 2110, complaining about a required entry that is
    absent.  This is in its own procedure because it is called from
    two places in PROC ProcessBlock.

        Parameters:
            block_name      str             Name of the block currently
                                            being processed.
            file_name       str             The file name, for error messages.
            key             str             A required keyword that is
                                            not present.
            line_number     int             The line number, for error messages.
            line_text       str             The entire line, including comments.
            log             handle          The handle of the logfile.

        Returns:
            None            None            A failure condition.

        Errors:
            Aborts with 2110 if a required keyword is not present.
    '''
    err = ('> Came across a ' + block_name + ' definition\n'
           '> in "' + file_name + '"\n'
           '> that lacked a required entry, "' + key + '".\n'
           '> Please add a line to define it.'
          )
    gen.WriteError(2110, err, log)
    gen.ErrorOnLine(line_number, line_text, log, False)
    return(None)


def CheckForConstant(word, form, settings_dict):
    '''Take the name of something that we expect to be either a number
    or the name of a constant.  If it is not the name of a constant
    return the number.  If it is the name of a constant and the Boolean
    'form' is False, return the value of the constant.  If it is the
    name of a constant and 'form' is True return the details of the
    constant (the line number it was defined on, the value of the
    constant and the text of the line that defined it).

        Parameters:
            word            str             String of the number we want to check
            form            bool            If False, return the value only.  If
                                            True, return a tuple of the value
                                            and the details of which line in the
                                            file the constant came from.
            settings_dict   {}              The entries in the settings block.

        Returns:
            word            str or []       The original word (str), the word
                                            assigned to the constant (str) or
                                            the tuple of (line number, word,
                                            and line text).
    '''
    debug1 = settings_dict["debug1"]
    if debug1:
        print("Entry to CheckForConstant", word, form, type(word))

    # First check to see if the word can be turned into a floating-point
    # number.  If it can it is definitely not a constant because it would
    # have triggered error 2183.
    try:
        discard = float(word)
    except:
        # It can't be turned into a number so it might be the name of
        # a constant.
        pass
    else:
        # It can be turned into a number so it cannot be a constant.
        return(word)

    # First get all the names of constants.
    names_consts = GetConstantNames(settings_dict)[0]

    # Now check if the value of the key is the name of a constant.
    candidate = '#' + word.lower()
    if debug1:
        print("Checking for a match to a constant:", candidate, "\n",
              names_consts, candidate in names_consts)
    if candidate in names_consts:
        # We have a match.  Return one value or three depending on
        # the value of 'form'.
        # (const_number, value, const_text) = settings_dict[candidate]
        # const_number and const_text are the line number and line
        # text of the line where the constant was defined, in case
        # we need them for an error message.  'value' is the value
        # assigned to the name of the constant in the constants block.
        if form is True:
            word = settings_dict[candidate]
            if debug1:
                print("Substituting constant 1:", word, type(word[1]))
        else:
            word = settings_dict[candidate][0]
            if debug1:
                print("Substituting constant 2:", word, type(word))

    # Return the original word, the number that may have overwritten it,
    # or the tuple detailing the constant and where it came from.  The
    # above is ugly code but it works.
    return(word)


def GetConstantNames(settings_dict):
    '''Take the settings dictionary and pull out all the names of
    constants.  Return a list of keys (the names of constants with
    '#' prepended) and a list of the names (no '#' here).

        Parameters:
            settings_dict   str             A dictionary of random stuff,
                                            including (at the time this
                                            routine is called) the contents
                                            of the constants dictionary.
        Returns:
            keys            []              A list of the names of entries
                                            in the "constants" block, with
                                            a "#" prepended to them.
            names           []              A list of the names of entries
                                            in the "constants" block.
    '''
    keys = []
    names = []
    for key, result in settings_dict.items():
        # All constants have a '#' as the first letter of the key
        # and return a list (so that the line they were defined on
        # can be used in error messages).
        if key[0] == '#' and type(result) is list:
            keys.append(key)
            names.append(key[1:])
    return(keys, names)


def CheckRangeAndSI(word, expected, toSI, err_lines, line_number, line_text,
                    settings_dict, log):
    '''Take a number and a string that defines what its allowable range is
    and what key to use to convert it to SI units.  Check it lies in that
    range and convert it to SI if necessary.

        Parameters:
            word            str             String of the number we want to check
            expected        str             A string detailing the type of
                                            number we expect, the allowable
                                            range and the key to use to convert
                                            it to SI.
            toSI            bool            If True convert from US units to
                                            SI units.
            err_lines       str             Two lines of error text message.
                                            They calls to here from ProcessBlock
                                            and GetOptionals are two different
                                            lines.
            line_number     int             The line number in the input file
            line_text       str             All the contents of the line in
                                            the input file.
            settings_dict   {}              The entries in the settings block.
            log             handle          The handle of the logfile.

        Returns:
            number         int or float     The checked and possibly-converted-
                                            to-SI number,

        Errors:
            Aborts with 1201 if a number specifier didn't have
            four entries (the code is broken).
            Aborts with 1202 if the specified number type wasn't "int"
            or "float" (in a sub-block), (the code is broken).
            Aborts with 1203 if the rule for range checking was
            mis-spelled (the code is broken).
            Aborts with 2221 if the constant was named "*time" (a fake
            constant only used when plotting loops).  This one is likely
            to appear a lot when a user copies plot commands from a loop
            block into a page block.
            Aborts with 2222 if an integer was expected and the result
            was not an integer.
            Aborts with 2223 if a floating-point number was expected
            and the result was not a float.
            Aborts with 2224-2227 if the number is out of the declared
            range (see below).
    '''
    debug1 = settings_dict["debug1"]
    file_name = settings_dict["file_name"]
    # First we get the range check rules.  The string 'expected' takes
    # the form of three words and a description or one word (#name):
    #
    #    "#name"
    #    "int      0+    dist1    a chainage"
    #    "float    0+    dist1    a chainage"
    #    "*float   0+    dist1    a chainage"
    #
    # "#name" is any word, intended for the names of joins.  When a number
    # is expected, the first word can be "float" or "int", defining the type.
    # There is a special instance of "float" that permits the number to
    # be prepended by an asterisk, this is to autoscale axis extents in
    # gnuplot xrange, yrange etc. commands.
    #
    # The second word sets the rules for range checks.  It can be
    # one of the following:
    #    "-"   must be a negative number, not zero or positive
    #    "-0"  must be a negative number or zero
    #    "0+"  must be a positive number or zero
    #    "+"   must be a positive number, not zero or negative
    #    "any" can be any number (i.e. no check)
    #
    # The third word is a dictionary key from UScustomary.py or the
    #  word "roughness", which means "only convert to SI if it is
    # a positive roughness height, not a negative friction factor."
    #
    # The fourth entry is a description that we'll use in the error
    #  message, something like "an area" or "a sectype height".
    words = expected.split(maxsplit = 3)
    if len(words) != 4:
        # We'll likely run into this one a lot while developing
        # the code.
        err = ('> Found too few entries in a number specifier in\n'
               '> CheckRangeAndSI (there should be four).  It\n'
               '> occurred while processing the file\n'
               '> "' + file_name + '".\n'
               + err_lines +
               '\n> The faulty descriptor was "' + expected + '".\n'
               '>'
               )
        gen.WriteError(1201, err, log)
        gen.OopsIDidItAgain(log, file_name)
        return(None)
    else:
        (num_type, rules, convert_key, descrip) = words

    if debug1:
        print("In CheckRangeAndSI", words, word)
    result = CheckForConstant(word, True, settings_dict)

    if type(result) is str:
        # This word was probably not the name of a constant.  But check
        # if it is the name of a constant that only exists inside looped
        # plots and give a useful error message to the user if it is.
        if word.lower() == "*time":
            # Looks like someone copied the plot commands for a loop and
            # pasted them into a page.
            err = ('> Came across ' + descrip + ' that was not\n'
                   '> a valid constant in "' + file_name + '".\n'
                   '> The constant named "*time" is a special one that\n'
                   '> can only be used inside timeloops.  You probably\n'
                   '> copied and pasted the graph from a loop definition\n'
                   '> into a page definition.  Please change all the\n'
                   '> instances of "*time" to a number or to the name\n'
                   '> of a constant in the "constants" block.'
                  )
            gen.WriteError(2221, err, log)
            gen.ErrorOnLine(line_number, line_text, log, False)
            return(None)
        else:
            # The word was a valid number.  We keep it as a string.
            constant = False
            # Spoof the values to avoid raising errors
            const_number = "unused constant line number"
            const_text = "unused constant line text"
    else:
        # It was the name of a constant and we have substituted for it.
        # Overwrite the original definition of 'word' with the value
        # (also still a string) associated with the constant.
        constant = True
        (word, const_text, const_number) = result


    # Set the autoscale flag to False.  We may set it True if we are
    # processing axis extents or intervals ("*float" num_type).
    autoscale = False

    if num_type == "int":
        # We expect this entry to be an integer value.
        try:
            number = int(str(word))
        except ValueError:
            err = ('> Came across ' + descrip + ' that was not\n'
                   '> an integer in "' + file_name + '".\n'
                   + err_lines
                  )
            if not constant:
                # This was a fault on one line.
                err = err + '.\n> The only valid entries there are integers.'
                gen.WriteError(2222, err, log)
                gen.ErrorOnLine(line_number, line_text, log, False)
            else:
                # The fault was in two lines, extend the error
                # message to include the line setting the constant.
                err = err + (',\n'
                       '> referring to a constant of that name which\n'
                       '> was not an integer. The only valid entries\n'
                       '> there are integers or integer constants.'
                      )
                gen.WriteError(2222, err, log)
                gen.ErrorOnTwoLines(const_number, const_text,
                                    line_number, line_text, log, False)
            return(None)
    elif num_type in ("float", "*float"):
        # We expect this entry to be a real number value.  If the number
        # is preceded by a "*", we return the "*" regardless (it means
        # "tell gnuplot to autoscale this axis extent or axis interval").
        if num_type == "*float" and word[0] == "*":
            # We are autoscaling an axis extent.  Remove the asterisk
            # and set the autoscale flag.
            autoscale = True
            # If there was no number in the word for the next block to
            # read, spoof one that can be ignored.
            if word == "*":
                word = "42"
            else:
                word = word[1:]
        try:
            number = float(word)
        except (TypeError, ValueError):
            err = ('> Came across ' + descrip + ' that was not\n'
                   '> a real number in "' + file_name + '".\n'
                   + err_lines
                  )
            if not constant:
                if autoscale:
                    err = (err + '\n> The only valid entries there are real numbers,\n'
                                 '> real numbers preceded by "*" and constants.\n'
                                 '> There is not a constant named "' + word + '".')
                else:
                    err = (err + '\n> The only valid entries there are real numbers\n'
                                 '> and constants, and there is not a constant named\n'
                                 '> "' + word + '".')
                # Add a list of the names of the constants that have
                # been defined, if any have been.
                const_names = GetConstantNames(settings_dict)[1]
                if len(const_names) != 0:
                    err = err + '\n> The following constants are defined:\n'\
                              + gen.FormatOnLines(const_names)
                gen.WriteError(2223, err, log)
                gen.ErrorOnLine(line_number, line_text, log, False)
            else:
                # The fault was in two lines, extend the error
                # message.
                err = err + (',\n'
                       '> referring to a constant of that name which\n'
                       '> was not a number. The only valid entries\n'
                       '> there are real numbers or constants (there\n'
                       '> are no constants defined).'
                      )
                # Add a list of the names of the constants that have
                # been defined.
                const_names = GetConstantNames(settings_dict)[1]
                if len(const_names) != 0:
                    err = err + '\n> The following constants are defined:\n'\
                              + gen.FormatOnLines(const_names)
                gen.WriteError(2223, err, log)
                gen.ErrorOnTwoLines(const_number, const_text,
                                    line_number, line_text, log, False)
            return(None)
    else:
        # The sub-entry specifier had "int", "float" or "name"
        # mis-spelled.  We can't actually get here at the moment
        # but it's worth having it, as future code edits may
        # allow a path to it.  "If" blocks without an "else" in
        # them can cause very, very confusing errors during
        # development.  We raise a dud spec without giving a
        # block name.
        print("Raising dud spec", word, const_text, const_number)
        RaiseDudSpec(expected, "unknown", file_name, log)
        return(None)

    # Check for weird stuff.  The code above will accept the words
    # "NaN" (not-a-number) and "inf" (infinity) as valid integers/floats
    result = gen.CheckNanInf(word, file_name, line_number, line_text, log)
    if result is None:
        return(None)

    # If we get to here it is a suitable number.  Check the range.
    if rules == "-" and number >= 0:
        err = ('> Came across ' + descrip + ' that should\n'
               '> have been negative but was not, in \n'
               '> "' + file_name + '"\n'
               + err_lines
              )
        if not constant:
            err = err + '.\n'   \
                      '> The only valid entries there are negative numbers.'
            gen.WriteError(2224, err, log)
            gen.ErrorOnLine(line_number, line_text, log, False)
        else:
            # The fault was in two lines, extend the error
            # message.
            err = err + ',\n'   \
                      '> referring to a constant of that name which\n'   \
                      '> was not negative.'

            gen.WriteError(2224, err, log)
            gen.ErrorOnTwoLines(line_number, line_text,
                                const_number, const_text,
                                log, False)
        return(None)
    elif rules == "-0" and number > 0:
        err = ('> Came across ' + descrip + ' that should\n'
               '> have been negative or zero but was not, in \n'
               '> "' + file_name + '".\n'
               + err_lines
              )
        if not constant:
            err = (err + '.\n'
                   '> The only valid entries there are negative numbers\n'
                   '> or zero.'
                  )
            gen.WriteError(2225, err, log)
            gen.ErrorOnLine(line_number, line_text, log, False)
        else:
            # The fault was in two lines, extend the error
            # message.
            err = (err + ',\n'
                   '> referring to a constant of that name which\n'
                   '> was positive.'
                  )
            gen.WriteError(2225, err, log)
            gen.ErrorOnTwoLines(const_number, const_text,
                                line_number, line_text, log, False)
        return(None)
    elif rules == "0+" and number < 0:
        err = ('> Came across ' + descrip + ' that should\n'
               '> have been positive or zero but was not, in \n'
               '> "' + file_name + '".\n'
               + err_lines
              )
        if not constant:
            err = (err + '.\n'
                   '> The only valid entries there are positive numbers\n'
                   '> or zero.'
                  )
            gen.WriteError(2226, err, log)
            gen.ErrorOnLine(line_number, line_text, log, False)
        else:
            # The fault was in two lines, extend the error
            # message.
            err = (err + ',\n'
                   '> referring to a constant of that name which\n'
                   '> was negative.'
                  )
            gen.WriteError(2226, err, log)
            gen.ErrorOnTwoLines(const_number, const_text,
                                line_number, line_text, log, False)
        return(None)
    elif rules == "+" and number <= 0:
        err = ('> Came across ' + descrip + ' that should\n'
               '> have been positive but was not, in \n'
               '> "' + file_name + '".\n'
               + err_lines
              )
        if not constant:
            err = (err + '.\n'
                   '> The only valid entries there are positive numbers\n'
                   '> (not negative numbers or zero).'
                  )
            gen.WriteError(2227, err, log)
            gen.ErrorOnLine(line_number, line_text, log, False)
        else:
            # The fault was in two lines, extend the error
            # message.
            err = (err + ',\n'
                   '> referring to a constant of that name which\n'
                   '> was not positive.'
                  )
            gen.WriteError(2227, err, log)
            gen.ErrorOnTwoLines(const_number, const_text,
                                line_number, line_text, log, False)
        return(None)
    elif rules not in ("-", "-0", "0+", "+", "any"):
        # We'll likely run into this one occasionally while developing
        # the code.
        err = ('> Found an invalid range testing rule in CheckRangeAndSI\n'
               '> while processing "' + file_name + '".\n'
               + err_lines +
               '\n> The faulty descriptor was "' + expected + '".\n'
               '>'
               )
        gen.WriteError(1203, err, log)
        gen.OopsIDidItAgain(log, file_name)
        return(None)

    # If we get to here the range matches.  Check if we need to convert
    # to SI.
    if toSI and convert_key != "ignore":
        if convert_key == "roughness":
            # A special for roughness height/friction factor.  We only
            # convert roughness heights (from feet to metres), we leave
            # dimensionless friction factors (negative numbers) unchanged.
            if number >= 0.0:
                # It's safe to not check for a returned None.
                result = USc.ConvertToSI("dist1", number, debug1, log)
                number = result[0]
        else:
            result = USc.ConvertToSI(convert_key, number, debug1, log)
            if result is None:
                # We fouled up the key in the source code.  An error message
                # has already been issued, but add a second one asking for
                # a bug report with the file that triggered it.
                gen.OopsIDidItAgain(log, file_name)
            else:
                # Overwrite the US value with the SI value
                number = result[0]
    # Now check if we need to return an autoscale instruction instead of
    # the  number.
    if autoscale:
        number = "*"
    return(number)


def ProcessFile( arguments ):
    '''
    Take a file name and a file index and process the file.  We do a few
    checks first and if we pass these, we open a logfile (the file's
    namestem plus ".log") in a subfolder and start writing stuff about
    the run to it.  The parameters are in a tuple because this routine
    may be called by the "multiprocessing" library.

        Parameters:
            file_string     str             String of text with the file name
                                            and (optionally) the file path.
            file_num        int             This files place in the list of
                                            files fed to the program (starting
                                            at 1, not at zero).
            file_count      int             The count of files.
            settings_dict   {}              Dictionary of the run settings.

        Returns:
            nothing

        Errors:
            Aborts with 2001 the name of the input file didn't end in ".txt".
            Aborts with 2002 if you don't have permission to open the input file.
            Aborts with 2003 if the input file doesn't exist.
            Aborts with 2004 if you don't have permission to open the logfile.
            Aborts with 2005 if you don't have permission to write to the logfile.
            Aborts with 2006 if the numpy or pandas library is not on this computer
            (and writes a complaint the logfile).
            Aborts with 2007 if you want to write an SES input file but don't
            have a .hbn file to base it on.
            Aborts with 2008 if you want to write an SES input file but don't
            have permission to read the .hbn file.
    '''
    (file_string, file_num, file_count, settings_dict) = arguments
    debug1 = settings_dict["debug1"]
    show_errors = settings_dict["show_errors"]
    script_name = settings_dict["script_name"]
    when_who = settings_dict["when_who"]

    # Get the file name, the directory it is in, the file stem and
    # the file extension.
    (file_name, dir_name,
        file_stem, file_ext) = gen.GetFileData(file_string, ".txt", debug1)

    if show_errors:
        print('')
    else:
        print("\n> Processing file " + str(file_num) + " of "
              + str(file_count) + ', "' + file_name + '".\n>')

    settings_dict.__setitem__("file_name", file_name)
    settings_dict.__setitem__("dir_name", dir_name)
    settings_dict.__setitem__("file_stem", file_stem)
    settings_dict.__setitem__("file_ext", file_ext)

    # Ensure the file extension is .txt.
    if file_ext.lower() != ".txt":
        # The file_name doesn't end with ".txt" so it is not a
        # Hobyah file.  Put out a message about it.
        print('> *Error* type 2001 ******************************\n'
              '> Skipping "' + file_name + '", because it\n'
              "> doesn't end with"' the extension ".txt".')
        gen.PauseIfLast(file_num, file_count)
        # Whether or not we paused, we return to main here
        return()
    # Filter out all the text files that end in "_ses.txt", as these
    # are files written by SESconv.py, not Hobyah input files.
    if file_stem.lower()[-4:] == "_ses":
        # The file_name doesn't end with ".txt" so it is not a
        # Hobyah file.  Put out a message about it.
        print('> Skipping "' + file_name + '", because it\n'
              '> is an SESconv.py output file.')
        gen.PauseIfLast(file_num, file_count)
        return()

    # If we get to here, the file name did end in .txt.
    # Check if the file exists.  If it does, check that we have
    # permission to read it.  Fail if the file doesn't exist
    # or if we don't have access.
    if os.access(dir_name + file_name, os.F_OK):
        # The file exists.
        try:
            inp = open(dir_name + file_name, 'r', encoding='utf-8')
        except PermissionError:
            print('> *Error* type 2002 ******************************\n'
                  '> Skipping "' + file_name + '", because you\n'
                  "> do not have permission to read it.")
            gen.PauseIfLast(file_num, file_count)
            return()
        else:
            # Load the lines in the file into a list of strings.
            file_contents = inp.readlines()
            inp.close()
    else:
        print('> *Error* type 2003 ******************************\n'
              '> Skipping "' + file_name + '" in folder\n'
              '> "' + dir_name + '", because it\n'
              "> doesn't exist.")
        gen.PauseIfLast(file_num, file_count)
        return()

    # Create a logfile to hold observations and debug entries.
    # We create a subfolder of ancillary files to hold the logfiles
    # (among other things), so they don't clutter up the main folder.
    # First check if the folder exists and create it if it doesn't.
#
#    Can't use pathlib in Python 3.5 on my Mac, alas - this next line
#    doesn't work for me.
#    pathlib.Path.mkdir(dir_name + "ancillaries", exist_ok = True)
#
    if not os.access(dir_name + "ancillaries", os.F_OK):
        try:
            os.mkdir(dir_name + "ancillaries")
        except PermissionError:
            print('> *Error* type 2004 ******************************\n'
                  '> Skipping "' + file_name + '", because it\n'
                  "> is in a folder where you do not have permission\n"
                  '> to create the required "ancillaries" subfolder.')
            gen.PauseIfLast(file_num, file_count)
            return()

    # Now try to create the logfile.
    log_name = dir_name + "ancillaries/" + file_stem + ".log"
    try:
        log = open(log_name, 'w', encoding='utf-8')
    except PermissionError:
        print('> *Error* type 2005 ******************************\n'
              '> Skipping "' + file_name + '", because you\n'
              "> do not have permission to write to its logfile.")
        gen.PauseIfLast(file_num, file_count)
        return()
    else:
        # We have permission to create/write to the logfile.  Write
        # some traceability data to it.
        log.write('Processing "' + dir_name + file_name + '"\n'
                  '  using ' + script_name +
                  ', run at ' + when_who + '.\n')

    # Try to import the pandas and numpy libraries.  If they aren't
    # installed on this machine then write a message to the screen
    # and to the logfile.
    try:
        package_name = "numpy"
        import numpy as np
        package_name = "pandas"
        import pandas as pd
    except ModuleNotFoundError:
        err = ("> Ugh, can't process this run because Python's\n"
               '> ' + package_name + ' library is not installed.\n'
               '> If you are fortunate enough to have an IT\n'
               '> department, please ask them to install it for\n'
               '> you then try again.  If you do not have an IT\n'
               '> department and have to do it for yourself, good\n'
               '> luck!\n'
               '> Note that there are other programs you may\n'
               '> want to install: gnuplot and ImageMagick.\n'
               '> See section 1.8 of the Hobyah User Guide for\n'
               '> more details.'
              )
        gen.WriteError(2006, err, log)
        return(None)

    # Check the file for valid begin <block>...end <block> syntax.   If we
    # have a problem the routine will return None.  If all is well, it
    # will return a list holding the lines of formal comment at the top of
    # the file and all the lines between "begin settings" and "end plots".
    # Everything after the "end plots" is ignored (so we can store blocks
    # of unused input there).

    # Some block types do not need a name after the noun, such as "begin plots".
    # Some do, such as "begin tunnel 101".  We make a list of those that do
    # not need a name for the check.  We keep these as lists so that they
    # are not confused with the tuples returned by ProcessBlock.
    named = ["tunnel", "route", "data", "files", "fanchar", "traintype",
             "schedule", "tunnelclones", "trafficsteady", "gradients",
             # For the entries in the next line we allow a name so that
             # the "ignore" keyword is treated as a name and is processed
             # correctly.
             "page", "timeloop", "filesloop", "graph", "image",
            ]
    unnamed = ["settings", "testblock", "constants", "plotcontrol",
               "sectypes", "gradients", "elevations", "heights",
               "tunnels", # Routes have lists of tunnels in them
               "speedlimits", "lanes", "radii", "sectors", "coasting",
               "regenfractions",
               "sub_testblock", "traffictypes", "jetfantypes",
               "plots", "verbatim", "csv", "image",
               "page", "timeloop", "filesloop", "graph", "image",
               "nicknames", "exclude", "sesdata"
              ]
    duplicables = ["testblock", "constants", "sectypes",
                   "page", "graph", "timeloop", "filesloop",
                   "verbatim", "data", "image", "fanchar",
                   "schedule", "tunnelclones",
                   "trafficsteady", "sesdata"
                  ]
    # The code below gets a list of the blocks that cannot be duplicated.
    # [name for name in named+unnamed if name not in duplicables]
    # Make dictionaries of which block names can contain other blocks.
    # If a block does not appear in this dictionary then other blocks
    # cannot be nested within blocks of that type.
    nestables = {"route": ("tunnels", "elevations", "gradients",
                           "schedule", "speedlimits", "lanes",
                           "radii", "sectors", "coasting",
                           "regenfractions",),
                 "plots": ("page", "timeloop", "filesloop"),
                 "page": ("graph", "image"),
                 "timeloop": ("graph", "image", "verbatim"),
                 "filesloop": ("nicknames", "exclude", "graph",
                               "image", "verbatim"),
                 "graph": ("verbatim", "sub_testblock"),
                 "testblock": ("sub_testblock",),
                 "image": ("verbatim",),
                }
    # Finally, a few words are reserved and can't be used for things
    # like file nicknames or route names.  Add them to the settings
    # dictionary.
    reserved = ["calc", "begin", "end",
                "title", "xlabel", "ylabel", "x2label", "y2label",
                "xrange", "yrange", "x2range", "y2range",
                "margins", "verbatim", "data", "allroutes",
               ]
    # Now add those lists to the settings dictionary so we don't
    # have to bother passing them.
    settings_dict.__setitem__("named", named)
    settings_dict.__setitem__("unnamed", unnamed)
    settings_dict.__setitem__("duplicables", duplicables)
    settings_dict.__setitem__("reserved", reserved)


    result = syntax.CheckSyntax(file_contents, file_name, unnamed, named,
                                duplicables, nestables, "Hobyah", log, debug1)

    if result is None:
        # The begin...end syntax was not valid.  The routine
        # has already issued an appropriate error message.
        # Return back to main() to process the next file.
        log.close()
        gen.PauseIfLast(file_num, file_count)
        return()
    else:
        # If we get to here then every instance of "begin <foo>" was
        # closed by a matching "end <foo>" at the same level of nesting.
        # Every instance of <foo> was correctly named and inside a block
        # that allowed <foo> blocks as sub-blocks.  We can process the
        # contents of the file without having to check for correct
        # begin...end syntax.

        (comments, line_triples, begin_lines) = result
        # Comments is the comments at the top of the file.  We append
        # these to the plot files for QA purposes.
        #
        # Line_triples is a list of sub-lists.  Each sub-list has three
        # entries, each related to a line of valid input in the file.
        # The three entries are:
        #  * The line number in the file.  This is used in error
        #    messages.
        #  * The valid data on the line, with any comment removed.
        #  * The entire line, also used in error messages.
        #
        # begin_lines is an index of the entries in line_triples that
        # hold the start of a top-level begin...end block.
        if debug1:
            print("Top level blocks are as follows:")
            for entry in begin_lines:
                print("  ", entry, line_triples[entry][:2])

    settings_dict.__setitem__("file_comments", comments)
    # If we get to here we know that there are no duplicate names
    # in the blocks at each level, that all the blocks have matching
    # begin...end entries and are correctly nested.  We know that
    # all "begin" lines that require more than two entries have more
    # than two entries.  We know where each top-level block starts.

    # Now we can process the valid entries without stumbling over
    # input clashes in the blocks, irrespective of how nested they are.
    if debug1:
        print("Processing settings")
    result = ProcessSettings(line_triples, settings_dict, log)
    if result is None:
        # Something went wrong.  The routine we called has already
        # issued a suitable error message.  Go back to main().
        log.close()
        gen.PauseIfLast(file_num, file_count)
        return(None)
    else:
        # Add the run settings to the settings dictionary.
        for key in result:
            settings_dict.__setitem__(key, result[key])


    # We now have the settings.  We look for blocks defining constants.
    # Find where all the constants blocks begin (there can be any
    # number of them).
    # Note that we do this at this level so that we can use constants
    # in files used for plotting only as well as files used for
    # calculations.
    result = GetBegins(line_triples, begin_lines, "constants",
                       0, math.inf, file_name, debug1, log)
    if result is None:
        log.close()
        gen.PauseIfLast(file_num, file_count)
        return(None)
    constants_dict = {}
    if debug1:
        print("Processing constants", result, type(result))
    for tr_index in result:
        result = ProcessConstants(line_triples, tr_index, constants_dict,
                                  settings_dict, log)
        if result is None:
            log.close()
            gen.PauseIfLast(file_num, file_count)
            return(None)
        else:
            constants_dict = result
    # Now we add the constants to the settings dictionary.  When we
    # set the keys of the constants we prepend a '#' character to
    # the key so that we can avoid name conflicts with the dictionary
    # keys that were defined in the "settings" block.
    #
    # Note that we don't need to check for duplicates.  If there were
    # any duplicates, error 2106 would have already been raised.
    for key in constants_dict:
        value = constants_dict[key]
        new_key = "#" + key.lower()
        # Build a list of the line number, the value and the line text.
        (const_number, discard, const_text) = line_triples[value[-1]]
        constant = [value[0], const_text, const_number]
        settings_dict.__setitem__(new_key, constant)


    # Now we seek the blocks that contain the user's blocks of data.
    # These may be used for plotting or for setting time-variable data
    # such as the resistance of platform screen doors as they open
    # and close.
    #
    result = GetBegins(line_triples, begin_lines, "data",
                       0, math.inf, file_name, debug1, log)
    if result is None:
        log.close()
        gen.PauseIfLast(file_num, file_count)
        return(None)

    # Create a dictionary to hold the blocks of user data and the
    # .csv files.
    user_data_dict = {}

    # Process the user data blocks.
    for line_index in result:
        result = ProcessUserData(line_triples, line_index,
                                   False, settings_dict, log)
        if result is None:
            log.close()
            gen.PauseIfLast(file_num, file_count)
            return(None)
        else:
            (block_name, data_list) = result
            user_data_dict.__setitem__(block_name, data_list)
        if debug1:
            print(block_name, data_list)

    # Now look for a block of nicknames and associated names of .csv files
    # We only expect only one csv block.  If we find it we add their
    # contents to the user data dictionary (each .csv file is treated
    # as if it were a data block.
    result = GetBegins(line_triples, begin_lines, "csv",
                       0, 1, file_name, debug1, log)
    if result is None:
        log.close()
        gen.PauseIfLast(file_num, file_count)
        return(None)
    else:
        # Get a list of the names that have been used already so that we
        # can avoid duplicates.
        used_nicknames = tuple(user_data_dict.keys())
        # Process the csv block.
        for line_index in result:
            result = ProcessCsvData(line_triples, line_index, settings_dict,
                                    used_nicknames, log)
            if result is None:
                log.close()
                gen.PauseIfLast(file_num, file_count)
                return(None)
            else:
                # Add the dictionary of .csv data to the dictionary of user
                # data.
                user_data_dict.update(result)

    # Store the blocks of user data and .csv data in the settings dictionary.
    settings_dict.__setitem__("user_data", user_data_dict)

    if debug1:
        print("User data:", user_data_dict)

    # Now run the calculation.
    if (settings_dict["runtype"] == "calc" and
        settings_dict["nocalc"] is False):
            if debug1:
                print("Processing calculation")
            result = ProcessCalc(line_triples, begin_lines, settings_dict, log)
            if result is None:
                # Something went wrong.  Go back to main().
                log.close()
                gen.PauseIfLast(file_num, file_count)
                return(None)
    # If we get to here we either ran a calculation successfully
    # or we didn't run a calculation at all.


    # Now we seek the block that contains the names of any external
    # files we want to plot, the "begin files...end files" block.
    # This block may or may not exist: we may be plotting only
    # results from this calculation or user-specified data.
    result = GetBegins(line_triples, begin_lines, "files",
                       0, 1, file_name, debug1, log)
    if result is None:
        return(None)
    elif result == []:
        # We had no "begin files" block in this input file.  Spoof
        # the files dictionary.
        files_dict = {}
    else:
        # Process the files block.
        result = ProcessPlotFiles(line_triples, result[0],
                                  settings_dict, log)
        if result is None:
            log.close()
            gen.PauseIfLast(file_num, file_count)
            return(None)
        else:
            files_dict = result

    # We add the nickname "calc" to the dictionary of plottable files
    # to represent the results of the calculation we have just run.
    # We may have just run a calculation and written an output file,
    # or we may have already run one and have a binary file we can
    # use.  Or this file may only be for plotting and not have a
    # binary file at all.
    bin_name = file_stem + ".hbn"
    contents = clHobyah.Hobyahdata("", bin_name, log, False)
    # If there is a binary file associated with this Hobyah file
    # then 'contents' holds everything in the file.  If there is
    # not a binary file then it only contains one entry, which is
    # contents.success = None or contents.success = False (both
    # results may be returned by failures).
    if contents.success is True:
        files_dict.__setitem__("calc", (bin_name, contents))


    # Check if we need to generate any SES input files from the
    # Hobyah file.  A new SES input file based on the geometry of
    # the Hobyah run is generated by each "SESdata" block.
    SESfiles = {}
    result = GetBegins(line_triples, begin_lines, "sesdata",
                       0, math.inf, file_name, debug1, log)
    if result is None:
        log.close()
        gen.PauseIfLast(file_num, file_count)
        return(None)
    else:
        for line_index in result:
            result = ProcessSESData(line_triples, line_index, settings_dict,
                                    files_dict, log)
            if result is None:
                log.close()
                gen.PauseIfLast(file_num, file_count)
                return(None)
            # We don't need to store anything that returns from the
            # routine, the user can edit and adjust their SES input
            # file now that Hobyah has converted its geometry into
            # SES input.

    # Now find out where the plotting block is.
    result = GetBegins(line_triples, begin_lines, "plots",
                       1, 1, file_name, debug1, log)
    if result is None:
        log.close()
        gen.PauseIfLast(file_num, file_count)
        return(None)
    else:
        # We know there is only one so we don't need the loop here.
        tr_index = result[0]

    # We don't need to check for errors because we already checked
    # for the existence of one (and only one) plots block in the
    # syntax checker.
    result = ProcessPlots(line_triples, tr_index, settings_dict,
                          files_dict, log)
    if result is None:
        log.close()
        gen.PauseIfLast(file_num, file_count)
        return(None)

    # Now seek out the test block (the test block raises errors
    # relating to mis-spelling the number and range specifications).
    # We do this at the end of the run because most times a test block
    # is used in a file, it is to test that an error message is raised
    # correctly.

    if debug1:
        print("> Processing testblock")
    result = GetBegins(line_triples, begin_lines, "testblock",
                       0, 2, file_name, debug1, log)
    if type(result) is list and len(result) != 0:
        # We do have a testblock.  Process it (it will likely return
        # an error but we don't really care, that is its purpose).
        discard = ProcessTestBlock(line_triples, result[0],
                                   settings_dict, log)
        if len(result) == 2:
            # One of the files we are using to process test blocks has
            # two testblocks in it.  We use this to trip error 2064
            # by making another call to GetBegins looking for more
            # than one block.
            result = GetBegins(line_triples, begin_lines, "testblock",
                               3, 3, file_name, debug1, log)
    # We completed with no failures, return to main() and
    # process the next file.
    if show_errors:
        print('')
    else:
        print("> Finished processing file " + str(file_num) + ".")
    log.close()
    return()


def main():
    '''
    This is the main Hobyah loop.  It checks the python version, then
    uses the argparse module to process the command line arguments
    (options and file names).  It generates some QA data for the run
    then it calls a routine to process the files.  They can be processed
    in series or in parallel.

        Parameters:
            This is the main function.  It has no parameters.
    '''

    # First check the version of Python.  We need 3.7 or higher, fault
    # if we are running on something lower (unlikely these days, but you
    # never know).  If it is lower then the routine calls sys.exit.
    gen.CheckVersion()

    # Parse the command line arguments.
    parser = argparse.ArgumentParser(
        description = "Process a series of Hobyah input files and "
                      "run them.  It calculates airflow in a network "
                      "of tunnels and plots the results alongside "
                      "results from other Hobyah calculations and SES "
                      "(Subway Environment Simulation) calculations."
        )

    parser.add_argument('-debug1', action = "store_true",
                              help = 'A developer setting.  Turns on '
                                     'debugging messages')

    parser.add_argument('-nocalc', action = "store_true",
                              help = 'Prevent all the files from running '
                                     'calculations.  It just replots the '
                                     'figures')

    parser.add_argument('-nofortran', action = "store_true",
                              help = 'Tell Hobyah to use the Python routines '
                                     'to calculate, even if the Fortran ones '
                                     'are available')

    parser.add_argument('-serial', action = "store_true",
                              help = 'Process the runs in series, not '
                                     'in parallel. This is useful for '
                                     'fault-finding, because it means that '
                                     'when a failure occurs the name of '
                                     'the file that failed is close by '
                                     'in the transcript')

    parser.add_argument('-showerrors', action = "store_true",
                              help = "A developer setting.  Used for preventing"
                                     " certain runtime messages being printed"
                                     " to the console when running test files"
                                     " that write a transcript of errors for"
                                     " the manual")

    parser.add_argument('-dudbins', action = "store_true",
                              help = "A developer setting.  Used to tell the"
                                     " program to write new binary files "
                                     " that trigger certain error messages"
                                     " in classHobyah.py.")

    parser.add_argument('file_name', nargs = argparse.REMAINDER,
                              help = 'The names of one or more '
                                     'Hobyah input files (.txt files)')

    args_hobyah = parser.parse_args()

    # If we get here, we have at least one file to process.

    # Get some QA data before we start processing them.
    # First get name of this script (if it has one) and the last time it
    # was modified (in human-readable form).  This does not work if we
    # have imported Hobyah.py in a Terminal session, so we catch the
    # exception and set generic stuff.
    try:
        script_name = os.path.basename(__file__)
        script_data = pathlib.Path(__file__)
        script_since = datetime.datetime.fromtimestamp(script_data.stat().st_ctime)
        script_date = gen.TimePlusDate(script_since) # e.g. "08:31 on 1 Sep 2020"
    except NameError:
        # We are probably running in a Python session under Terminal
        # or inside an IDE.
        script_name = "No script"
        script_date = "No date/time"

    # Next get the user's name and a QA string (user, date of
    # the run and time of the run).
    user_name, when_who = gen.GetUserQA()


    # Print the program name and disclaimer.

    if args_hobyah.file_name == []:
        # There were no files.  Print the help text, pause if we
        # are running on Windows, then exit.
        parser.print_help()
        gen.PauseFail()

    file_count = len(args_hobyah.file_name)

    # Set the various values in a dictionary of settings.  Note that
    # we put these all in settings_dict (rather than in options_dict
    # as we do in SESconv.py because they are relevant to the calculation).
    settings_dict = {"debug1": args_hobyah.debug1,
                     "serial": args_hobyah.serial,
                     "nocalc": args_hobyah.nocalc,
                     "dudbins": args_hobyah.dudbins,
                     "file_count": file_count,
                     "script_name": script_name,
                     "script_date": script_date,
                     "show_errors": args_hobyah.showerrors,
                     "nofortran": args_hobyah.nofortran,
                     "user_name": user_name,
                     "when_who": when_who,
                    }

    if not args_hobyah.showerrors:
        # Print a blurb.
        print(#'Hobyah.py, ' + script_date.split(sep = 'on ')[1] + '\n'
              'Hobyah.py, 11 August 2024\n'
              'Copyright (C) 2020-2024 Ewan Bennett\n'
              'This is free software, released under the BSD 2-clause open\n'
              'source licence.  See licence.txt for copying conditions.\n\n'
              'This software is provided by the copyright holders and\n'
              'contributors "AS IS" and any express or implied warranties,\n'
              'including, but not limited to, the implied warranties of\n'
              'merchantability and fitness for a particular purpose are\n'
              'disclaimed.  In no event shall the copyright holder or\n'
              'contributors be liable for any direct, indirect, incidental,\n'
              'special, exemplary, or consequential damages (including,\n'
              'but not limited to, procurement of substitute goods or\n'
              'services; loss of use, data, or profits; or business\n'
              'interruption) however caused and on any theory of liability,\n'
              'whether in contract, strict liability, or tort (including\n'
              'negligence or otherwise) arising in any way out of the use\n'
              'of this software, even if advised of the possibility of such\n'
              'damage.')

    # Build a list of lists of arguments for ProcessFile.
    if file_count > 1:
        runargs = []
        for fileIndex, fileString in enumerate(args_hobyah.file_name):
            runargs.append((fileString, fileIndex + 1, file_count,
                            settings_dict.copy())
                          )
        if args_hobyah.serial:
            # The command-line option "-serial" has been set, so we process
            # the file(s) sequentially.  It is occasionally useful when
            # doing development.
            for args in runargs:
                ProcessFile(args)
        else:
            # Run them all in parallel, using as many cores as are available.
            corestouse = multiprocessing.cpu_count()
            my_pool = multiprocessing.Pool(processes = corestouse)
            my_pool.map(ProcessFile,runargs)
    else:
        # We only have one output file to process.  Best to not bother with
        # the overhead the multiprocessing library adds.
        ProcessFile( (args_hobyah.file_name[0], 1, 1, settings_dict) )
    return()



if __name__ == "__main__":
    main()

# This comment is here so I can play around with Excel sheet names, which
# have a limit of 31 characters.  Define them as a file number, a short
# description of the type of curve, the property and the place.
# |-Excel tab names <= 31 chars-|
# file42-tprof-deceltemp-tr105
# file42-fprof-elevation-rt4
# file42-trans-elevation-rt4
# file42-tprof-trainicon-rt4
# file42-tprof-speedlimits-rt45
# Maybe use the nickname of the source if it would not cause the tab
# name to go over 31 characters?  Or could this break Excel macros?
# "tprof" means "profile at one instant in time" (transient profile)
# "fprof" means "profile of something that doesn't vary with time,
# such as tunnel height.
